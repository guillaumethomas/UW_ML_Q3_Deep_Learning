{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Lesson 2: Neural Net Computations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In the previous assignment we used Keras to train a neural network. In this assignment you will build your own minimal neural net library. The basic structure is given to you; you will need to fill in details such as weight updating for backpropogation. Then you will test the network on learning the XOR function.\n",
    "\n",
    "Read through the class definitions below first to understand the basic architecture.\n",
    "\n",
    "Then you should add code as necessary where marked \"TODO\" in the code below and remove the NotImplementedError exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference \n",
    "\n",
    "- [Toward Data Science](https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76)\n",
    "- [Derivative Sigmoid](https://beckernick.github.io/sigmoid-derivative-neural-network/)\n",
    "- [Back Propagation](https://ml-cheatsheet.readthedocs.io/en/latest/backpropagation.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Define a Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNet():\n",
    "    \"\"\"Implements a basic feedforward neural network.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._layers = []  # An ordered list of layers. The first layer is the input; the final is the output.\n",
    "    \n",
    "    def _add_layer(self, layer):\n",
    "        if self._layers:\n",
    "            # Update pointers. We keep a doubly-linked-list of layers for convenience.\n",
    "            prev_layer = self._layers[-1]\n",
    "            prev_layer.set_next_layer(layer)\n",
    "            layer.set_prev_layer(prev_layer)\n",
    "            \n",
    "        self._layers.append(layer)\n",
    "    \n",
    "    def add_input_layer(self, size, **kwargs):\n",
    "        assert type(size).__name__ == 'int', ('Input layer requires integer size. Type was %s instead.' \n",
    "                                              % type(size).__name__)\n",
    "        layer = InputLayer(size=size, **kwargs)\n",
    "        self._add_layer(layer)\n",
    "\n",
    "    def add_dense_layer(self, size, **kwargs):\n",
    "        assert type(size).__name__ == 'int', ('Dense layer requires integer size. Type was %s instead.' \n",
    "                                              % type(size).__name__)\n",
    "        # Find the previous layer's size.\n",
    "        prev_size = self._layers[-1].size()\n",
    "        layer = DenseLayer(shape=(prev_size, size), **kwargs)\n",
    "        self._add_layer(layer)\n",
    "\n",
    "    def summary(self, verbose=False):\n",
    "        \"\"\"Prints a description of the model.\"\"\"\n",
    "        for i, layer in enumerate(self._layers):\n",
    "            print('%d: %s' % (i, str(layer)))\n",
    "            if verbose:\n",
    "                print('weights:', layer.get_weights())\n",
    "                if layer._use_bias:\n",
    "                    print('bias:', layer._bias)\n",
    "                print()\n",
    "\n",
    "        \n",
    "    def predict(self, x):\n",
    "        \"\"\"Given an input vector x, run it through the neural network and return the output vector.\"\"\"\n",
    "        assert isinstance(x, np.ndarray)\n",
    "        \n",
    "        # TODO\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "    def train_single_example(self, X_data, y_data, learning_rate=0.01):\n",
    "        \"\"\"Train on a single example. X_data and y_data must be numpy arrays.\"\"\"\n",
    "        \n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "\n",
    "        # Forward propagation.\n",
    "        \n",
    "        # TODO\n",
    "        \n",
    "        \n",
    "        # Backpropagation.\n",
    "        \n",
    "        # TODO\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "\n",
    "    def train(self, X_data, y_data, learning_rate, num_epochs, randomize=True, verbose=True, print_every_n=100):\n",
    "        \"\"\"Both X_data and y_data should be ndarrays. One example per row.\n",
    "        \n",
    "        This function takes the data and learning rate, and trains the network for num_epochs passes over the \n",
    "        complete data set. \n",
    "        \n",
    "        If randomize==True, the X_data and y_data should be randomized at the start of each epoch. Of course,\n",
    "        matching X,y pairs should have matching indices after randomization, to avoid scrambling the dataset.\n",
    "        (E.g., a set of indices should be randomized once and then applied to both X and y data.)\n",
    "        \n",
    "        If verbose==True, will print a status report every print_every_n epochs with these\n",
    "        results:\n",
    "        \n",
    "        * Results of running \"predict\" on each example in the training set\n",
    "        * MSE (mean squared error) on the dataset\n",
    "        * Accuracy on the dataset\n",
    "        \"\"\"\n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "        assert X_data.shape[0] == y_data.shape[0]\n",
    "\n",
    "        # TODO\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def compute_mean_squared_error(self, X_data, y_data):\n",
    "        \"\"\"Given input X_data and target y_data, compute and return the mean squared error.\"\"\"\n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "        assert X_data.shape[0] == y_data.shape[0]\n",
    "        \n",
    "        mse = 0\n",
    "        \n",
    "        # TODO\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "        return mse\n",
    "    \n",
    "    def compute_accuracy(self, X_data, y_data):\n",
    "        \"\"\"Given input X_data and target y_data, convert outputs to binary using a threshold of 0.5\n",
    "        and return the accuracy: # examples correct / total # examples.\"\"\"\n",
    "        assert isinstance(X_data, np.ndarray)\n",
    "        assert isinstance(y_data, np.ndarray)\n",
    "        assert X_data.shape[0] == y_data.shape[0]\n",
    "        \n",
    "        correct = 0\n",
    "        for i in range(len(X_data)):\n",
    "            outputs = self.predict(X_data[i])\n",
    "            outputs = outputs > 0.5\n",
    "            if outputs == y_data[i]:\n",
    "                correct += 1\n",
    "        acc = float(correct) / len(X_data)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Define activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Activation():  # Do not edit; update derived classes.\n",
    "    \"\"\"Base class that represents an activation function and knows how to take its own derivative.\"\"\"\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "    \n",
    "    def activate(x):\n",
    "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def derivative_given_y(self, y):\n",
    "        \"\"\"y is a scalar or a numpy array. \n",
    "        \n",
    "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityActivation(Activation):\n",
    "    \"\"\"Activation function that passes input through unchanged.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(name='Identity')\n",
    "    \n",
    "    def activate(self, x):\n",
    "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
    "        return x\n",
    "    \n",
    "    def derivative_given_y(self, y):\n",
    "        \"\"\"y is a scalar or a numpy array. \n",
    "        \n",
    "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
    "        return 1\n",
    "    \n",
    "    \n",
    "class SigmoidActivation(Activation):\n",
    "    \"\"\"Sigmoid activation function.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(name='Sigmoid')\n",
    "    \n",
    "    def activate(self, x):\n",
    "        \"\"\"x is a scalar or a numpy array. Returns the output y, the result of applying the function to input x.\"\"\"\n",
    "        # Done\n",
    "        def sigmoid(z):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        \n",
    "        if type(x).__name__ == 'ndarray': \n",
    "            return np.array(list(map(sigmoid, x)))\n",
    "        \n",
    "        return sigmoid(x)\n",
    "        \n",
    "        #raise NotImplementedError()\n",
    "\n",
    "    \n",
    "    def derivative_given_y(self, y):\n",
    "        \"\"\"y is a scalar or a numpy array. \n",
    "        \n",
    "        Returns the derivative d(f)/dx given the *activation* value y.\"\"\"\n",
    "        # Done\n",
    "        S = self.activate(y)\n",
    "        return S * (1 - S)\n",
    "        #raise NotImplementedError()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.5       ]\n",
      " [0.73105858 0.5       ]\n",
      " [0.5        0.73105858]\n",
      " [0.73105858 0.73105858]]\n",
      "0.7310585786300049\n",
      "[[0.25       0.25      ]\n",
      " [0.19661193 0.25      ]\n",
      " [0.25       0.19661193]\n",
      " [0.19661193 0.19661193]]\n",
      "0.19661193324148185\n"
     ]
    }
   ],
   "source": [
    "X_data = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "\n",
    "print(SigmoidActivation().activate(X_data))\n",
    "print(SigmoidActivation().activate(1))\n",
    "print(SigmoidActivation().derivative_given_y(X_data))\n",
    "print(SigmoidActivation().derivative_given_y(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Define a method to initialize neural net weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import uniform \n",
    "\n",
    "def WeightInitializer():\n",
    "    \"\"\"Function to return a random weight. for example, return a random float from -1 to 1.\"\"\"\n",
    "    #Done\n",
    "    #raise NotImplementedError()\n",
    "    return uniform(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Define a neural net Layer base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    \"\"\"Base class for NNet layers. DO NOT MODIFY THIS CLASS. Update derived classes instead.\n",
    "    \n",
    "    Conceptually, in this library a Layer consists at a high level of:\n",
    "      * a collection of weights (a 2D numpy array)\n",
    "      * the output nodes that come after the weights above\n",
    "      * the activation function that is applied to the summed signals in these output nodes\n",
    "      \n",
    "    So a Layer isn't just nodes -- it's weights as well as nodes.\n",
    "      \n",
    "    Specifically, to send signal forward through a 3-layer network, we start with an Input Layer that does\n",
    "    very little.  The outputs from the Input layer are simply the fed-in input data.  \n",
    "    \n",
    "    Then, the next layer will be a Dense layer that holds the weights from the Input layer to the first hidden\n",
    "    layer and stores the activation function to be used after doing a product of weights and Input-Layer\n",
    "    outputs.\n",
    "    \n",
    "    Finally, another Dense layer will hold the weights from the hidden to the output layer nodes, and stores\n",
    "    the activation function to be applied to the final output nodes.\n",
    "    \n",
    "    For a typical 1-hidden layer network, then, we would have 1 Input layer and 2 Dense layers.\n",
    "    \n",
    "    Each Layer also has funcitons to perform the forward-pass and backpropagation steps for the weights/nodes\n",
    "    associated with the layer.\n",
    "    \n",
    "    Finally, each Layer stores pointers to the pervious and next layers, for convenience when implementing\n",
    "    backprop.\n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self, shape, use_bias, activation_function=IdentityActivation, weight_initializer=None, name=''):\n",
    "        # These are the weights from the *previous* layer to the current layer.\n",
    "        self._weights = None\n",
    "        \n",
    "        # Tuple of (# inputs, # outputs) for Dense layers or just a scalar for an input layer.\n",
    "        assert type(shape).__name__ == 'int' or type(shape).__name__ == 'tuple', (\n",
    "            'shape must be scalar or a 2-element tuple')\n",
    "        if type(shape).__name__ == 'tuple':\n",
    "            assert len(shape)==2, 'shape must be 2-dimensional. Was %d instead' % len(shape)\n",
    "        self._shape = shape \n",
    "    \n",
    "        # True to use a bias node that inputs to each node in this layer; False otherwise.\n",
    "        self._use_bias = use_bias\n",
    "        \n",
    "        if use_bias:\n",
    "            bias_size = shape[-1] if len(shape) > 1 else shape\n",
    "            self._bias = np.zeros(bias_size)\n",
    "            if weight_initializer:\n",
    "                for i in range(bias_size):\n",
    "                    self._bias[i] = weight_initializer()\n",
    "        \n",
    "        # Activation function to be applied to each dot product of weights with inputs.\n",
    "        # Instantiate an object of this class.\n",
    "        self._activation_function = activation_function() if activation_function else None\n",
    "        \n",
    "        # Method used to initialize the weights in this Layer at creation time.\n",
    "        self._weight_initializer = weight_initializer\n",
    "        \n",
    "        # Layer name (optional)\n",
    "        self._name = name\n",
    "        \n",
    "        # Calculated output vector from the most recent feed_forward(inputs) call.\n",
    "        self._outputs = None\n",
    "        \n",
    "        # Doubly linked list pointers to neighbor layers.\n",
    "        self._prev_layer = None  # Previous layer is closer to (or is) the input layer.\n",
    "        self._next_layer = None  # Next layer is closer to (or is) the output layer.\n",
    "    \n",
    "    def set_prev_layer(self, layer):\n",
    "        \"\"\"Set pointer to the previous layer.\"\"\"\n",
    "        self._prev_layer = layer\n",
    "    \n",
    "    def set_next_layer(self, layer):\n",
    "        \"\"\"Set pointer to the next layer.\"\"\"\n",
    "        self._next_layer = layer\n",
    "    \n",
    "    def size(self):\n",
    "        \"\"\"Number of nodes in this layer.\"\"\"\n",
    "        if type(self._shape).__name__ == 'tuple':\n",
    "            return self._shape[-1]\n",
    "        else:\n",
    "            return self._shape\n",
    "        \n",
    "    def get_weights(self):\n",
    "        \"\"\"Return a numpy array of the weights for inputs to this layer.\"\"\"\n",
    "        return self._weights\n",
    "    \n",
    "    def get_bias(self):\n",
    "        \"\"\"Return a numpy array of the bias for nodes in this layer.\"\"\"\n",
    "        return self._bias\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        \"\"\"Feed the given inputs through the input weights and activation function, and set the outputs vector.\n",
    "        \n",
    "        Also returns the outputs vector for convenience.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def backpropagate(self, error, learning_rate):\n",
    "        \"\"\"Adjusts the weights coming into this layer based on the given output error vector.\n",
    "        \n",
    "        For the output layer, the \"error\" vector should be a list of output errors, y_k - t_k.\n",
    "        For a hidden layer, the \"error\" vector should be a list of the delta values from the following layer, such as delta_z_k\n",
    "        \n",
    "        Returns a list of the delta values for each node in this layer. These deltas can be used as the error\n",
    "        values when calling backpropagate on the previous layer.\"\"\"\n",
    "        raise NotimplementedError()\n",
    "        \n",
    "    def __str__(self):\n",
    "        activation_fxn_name = self._activation_function.name if self._activation_function else None\n",
    "        return '[%s] shape %s, use_bias=%s, activation=%s' % (self._name, self._shape, self._use_bias,\n",
    "                                                              activation_fxn_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Define InputLayer and DenseLayer base classes\n",
    "\n",
    "The DenseLayer class is where most of the computation happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "class InputLayer(Layer):\n",
    "    \"\"\"A neural network 1-dimensional input layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, size, name='Input'):\n",
    "        assert type(size).__name__ == 'int', 'Input size must be integer. Was %s instead' % type(size).__name__\n",
    "        super().__init__(shape=size, use_bias=False, name=name, activation_function=None)\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        assert len(inputs)==self._shape, 'Inputs must be of size %d; was %d instead' % (self._shape, len(inputs))\n",
    "        self._outputs = inputs\n",
    "        return self._outputs\n",
    "\n",
    "    def backpropagate(self, error, learning_rate):\n",
    "        return None  # Nothing to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer(Layer):\n",
    "    \"\"\"A neural network layer that is fully connected to the previous layer.\"\"\"\n",
    "    \n",
    "    def __init__(self, shape, use_bias=True, name='Dense', **kwargs):\n",
    "        super().__init__(shape=shape, use_bias=use_bias, name=name, **kwargs)\n",
    "        \n",
    "        self._weights = np.zeros(shape)\n",
    "        if self._weight_initializer:\n",
    "            for i in range(shape[0]):\n",
    "                for j in range(shape[1]):\n",
    "                    self._weights[i,j] = self._weight_initializer()\n",
    "    \n",
    "    def feed_forward(self, inputs):\n",
    "        \"\"\"Feed the given inputs through the input weights and activation function, and set the outputs vector.\n",
    "        \n",
    "        Also returns the outputs vector for convenience.\"\"\"\n",
    "        \n",
    "        assert len(inputs)==self._shape, 'Inputs must be of size %d; was %d instead' % (self._shape, len(inputs))\n",
    "        \n",
    "\n",
    "        output = np.dot(self.get_weights(), inputs) + self.get_bias()\n",
    "        output = self._activation_function.activate(output)\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        # Update output vector for later use, and return it.\n",
    "        self._outputs = output \n",
    "        return self._outputs\n",
    "        \n",
    "    def backpropagate(self, error, learning_rate):\n",
    "        \"\"\"Adjusts the weights coming into this layer based on the given output error vector.\n",
    "        \n",
    "        For the output layer, the \"error\" vector should be a list of output errors, y_k - t_k.\n",
    "        For a hidden layer, the \"error\" vector should be a list of the delta values from the following layer, such as delta_z_k\n",
    "        \n",
    "        Returns a list of the delta values for each node in this layer. These deltas can be used as the error\n",
    "        values when calling backpropagate on the previous layer.\"\"\"\n",
    "        assert isinstance(error, np.ndarray)\n",
    "        assert isinstance(self._prev_layer._outputs, np.ndarray)\n",
    "        assert isinstance(self._outputs, np.ndarray)  \n",
    "        \n",
    "        # Compute deltas. \n",
    "        deltas = None\n",
    "        # TODO\n",
    "        raise NotImplementedError()\n",
    "          \n",
    "        \n",
    "        # Compute gradient.\n",
    "        # TODO\n",
    "        \n",
    "        \n",
    "        # Adjust weights.\n",
    "        # TODO\n",
    "        self._weights = self.get_weights() - learning_rate * \n",
    "        \n",
    "        \n",
    "        # Adjust bias weights.\n",
    "        if self._use_bias:\n",
    "            # TODO\n",
    "            pass\n",
    "            \n",
    "        return deltas\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Train a neural net\n",
    "\n",
    "## Create a dataset for the XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 1]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "X_data = np.array([[0,0],[1,0],[0,1],[1,1]])\n",
    "y_data = np.array([[0,1,1,0]]).T\n",
    "print(X_data)\n",
    "print(y_data)\n",
    "print(type(X_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Create a neural network using the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [Input] shape 2, use_bias=False, activation=None\n",
      "1: [Dense] shape (2, 2), use_bias=True, activation=Sigmoid\n",
      "2: [Output] shape (2, 1), use_bias=True, activation=Sigmoid\n"
     ]
    }
   ],
   "source": [
    "nnet = NNet()\n",
    "nnet.add_input_layer(2)\n",
    "nnet.add_dense_layer(2, weight_initializer=WeightInitializer, activation_function=SigmoidActivation)\n",
    "nnet.add_dense_layer(1, weight_initializer=WeightInitializer, activation_function=SigmoidActivation, name='Output')\n",
    "nnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: [Input] shape 2, use_bias=False, activation=None\n",
      "weights: None\n",
      "\n",
      "1: [Dense] shape (2, 2), use_bias=True, activation=Sigmoid\n",
      "weights: [[ 0.75597926 -0.22474251]\n",
      " [-0.26396281 -0.03155437]]\n",
      "bias: [0.95909984 0.71383101]\n",
      "\n",
      "2: [Output] shape (2, 1), use_bias=True, activation=Sigmoid\n",
      "weights: [[ 0.76506915]\n",
      " [-0.26753383]]\n",
      "bias: [0.59856937]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nnet.summary(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Print the resuting neural net weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "nnet.summary(verbose=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
