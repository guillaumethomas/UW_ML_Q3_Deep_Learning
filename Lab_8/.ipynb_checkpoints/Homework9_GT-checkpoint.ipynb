{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 8\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "\n",
    "In the a previous homework assignments, you used two different dynamic programming algorithms and Monte Carlo reinforcement learning to solve a robot navigation problem by finding optimal paths to a goal in a simplified warehouse environment. Now you will use time differencing reinforcement learning to find optimal paths in the same environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration of the warehouse environment is illustrated in the figure below.\n",
    "\n",
    "<img src=\"GridWorldFactory.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **Grid World for Factory Navigation Example** </center>\n",
    "\n",
    "The goal is for the robot to deliver some material to position (state) 12, shown in blue. Since there is a goal state or **terminal state** this an **episodic task**. \n",
    "\n",
    "There are some barriers comprised of the states $\\{ 6, 7, 8 \\}$ and $\\{ 16, 17, 18 \\}$, shown with hash marks. In a real warehouse, these positions might be occupied by shelving or equipment. We do not want the robot to hit these barriers. Thus, we say that transitioning to these barrier states is **taboo**.\n",
    "\n",
    "As before, we do not want the robot to hit the edges of the grid world, which represent the outer walls of the warehouse. \n",
    "\n",
    "## Representation\n",
    "\n",
    "You are, no doubt, familiar with the representation for this problem by now.    \n",
    "\n",
    "As with many such problems, the starting place is creating the **representation**. In the cell below encode your representation for the possible action-state transitions. From each state there are 4 possible actions:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "There are a few special cases you need to consider:\n",
    "- Any action transitioning state off the grid or into a barrier should keep the state unchanged. \n",
    "- Any action in the goal state keeps the state unchanged. \n",
    "- Any transition within the taboo (barrier) states can keep the state unchanged. If you experiment, you will see that other encodings work as well since the value of a barrier states are always zero and there are no actions transitioning into these states. \n",
    "\n",
    "> **Hint:** It may help you create a pencil and paper sketch of the transitions, rewards, and probabilities or policy. This can help you to keep the bookkeeping correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy for latter\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "- [TD in RL](https://towardsdatascience.com/td-in-reinforcement-learning-the-easy-way-f92ecfa9f3ce)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to define the initial transition probabilities for the Markov process. Set the probabilities for each transition as a **uniform distribution** leading to random action by the robot. \n",
    "\n",
    "> **Note:** As these are just starting values, the exact values of the transition probabilities are not actually all that important in terms of solving the RL problem. Also, notice that it does not matter how the taboo state transitions are encoded. The point of the DP algorithm is to learn the transition policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = {0:{'u':0, 'd':5, 'l':0, 'r':1},\n",
    "          1:{'u':1, 'd':1, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':2, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':3, 'l':2, 'r':4},\n",
    "          4:{'u':4, 'd':9, 'l':3, 'r':4},\n",
    "          5:{'u':0, 'd':10, 'l':5, 'r':5},\n",
    "          6:{'u':6, 'd':6, 'l':6, 'r':6},\n",
    "          7:{'u':7, 'd':7, 'l':7, 'r':7},\n",
    "          8:{'u':8, 'd':8, 'l':8, 'r':8},\n",
    "          9:{'u':4, 'd':14, 'l':9, 'r':9},\n",
    "          10:{'u':5, 'd':15, 'l':10, 'r':11},\n",
    "          11:{'u':11, 'd':11, 'l':10, 'r':12},\n",
    "          12:{'u':12, 'd':12, 'l':12, 'r':12},\n",
    "          13:{'u':13, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':9, 'd':19, 'l':13, 'r':14},\n",
    "          15:{'u':10, 'd':20, 'l':15, 'r':15},\n",
    "          16:{'u':16, 'd':16, 'l':16, 'r':16},\n",
    "          17:{'u':17, 'd':17, 'l':17, 'r':17},\n",
    "          18:{'u':18, 'd':18, 'l':18, 'r':18},\n",
    "          19:{'u':14, 'd':24, 'l':19, 'r':19},\n",
    "          20:{'u':15, 'd':20, 'l':20, 'r':21},\n",
    "          21:{'u':21, 'd':21, 'l':20, 'r':22},\n",
    "          22:{'u':22, 'd':22, 'l':21, 'r':23},\n",
    "          23:{'u':23, 'd':23, 'l':22, 'r':24},\n",
    "          24:{'u':19, 'd':24, 'l':23, 'r':24}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot receives the following rewards:\n",
    "- 10 for entering position 0. \n",
    "- -1 for attempting to leave the grid. In other words, we penalize the robot for hitting the edges of the grid.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another. If we did not have this penalty, the robot could follow any random plan to the goal which did not hit the edges. \n",
    "\n",
    "This **reward structure is unknown to the MC RL agent**. The agent must **learn** the rewards by sampling the environment. \n",
    "\n",
    "In the code cell below encode your representation of this reward structure you will use in your simulated environment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards =  {0:{'u':-1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "          1:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          2:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          4:{'u':-1, 'd':-0.1, 'l':-0.1, 'r':-1},\n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          6:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          7:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          8:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "          11:{'u':-1, 'd':-1, 'l':-0.1, 'r':10},\n",
    "          12:{'u':10.0, 'd':10.0, 'l':10.0, 'r':10.0}, # 12:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0}\n",
    "          13:{'u':-1, 'd':-1, 'l':10, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1},\n",
    "          15:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          16:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          17:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          18:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          19:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          20:{'u':-0.1, 'd':-1, 'l':-1, 'r':-0.1},\n",
    "          21:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          22:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          23:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          24:{'u':-0.1, 'd':-1, 'l':-0.1, 'r':-1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find it useful to create a list of taboo states, which you can encode in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "taboos = [6, 7, 8, 16, 17, 18]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TD(0) Policy Evaluation\n",
    "\n",
    "With your representations defined, you can now create and test functions to perform TD(0) **policy evaluation**. \n",
    "\n",
    "As a first step you will need a function to find the rewards and next state given a state and an action. You are welcome to start with the `state_values` function from the TD/Q-learning notebook. However, keep in mind that you must modify this code to correctly treat the taboo states of the barrier. Specifically, taboo states should not be visited. \n",
    "\n",
    "Execute your code to test it for each possible action from state 11.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, array([-1. , -1. , -0.1, 10. ]), False)\n",
      "(11, array([-1. , -1. , -0.1, 10. ]), False)\n",
      "(12, array([10., 10., 10., 10.]), True)\n",
      "(10, array([-0.1, -0.1, -1. , -0.1]), False)\n"
     ]
    }
   ],
   "source": [
    "def action_lookup(index):\n",
    "    \"\"\"Helper function returns action given an index\"\"\"\n",
    "    action_dic = {0:'u', 1:'d', 2:'l', 3:'r'}\n",
    "    return action_dic[index]\n",
    "\n",
    "def index_lookup(action):\n",
    "    \"\"\"Helper function returns index given action\"\"\"\n",
    "    index_dic = {'u':0, 'd':1, 'l':2, 'r':3}\n",
    "    return index_dic[action]\n",
    "\n",
    "\n",
    "def next_state(state, action_index, neighbors = neighbors, action_lookup = action_lookup):\n",
    "    return(neighbors[state][action_lookup[action_index]])\n",
    "\n",
    "def simulate_environment(s, action, neighbors = neighbors, rewards = rewards, terminal = 12):\n",
    "    \"\"\"\n",
    "    Function simulates the environment for Q-learning.\n",
    "    returns s_prime and reward given s and action\n",
    "    \"\"\"\n",
    "    s_prime = neighbors[s][action]\n",
    "    reward_prime = np.array([rewards[s_prime][a] for a in rewards[0].keys()])\n",
    "    return (s_prime, reward_prime, is_terminal(s_prime, terminal))\n",
    "    \n",
    "\n",
    "def is_terminal(state, terminal = 12):\n",
    "    return state == terminal\n",
    "\n",
    "#adding a function to take care of the taboos\n",
    "def is_taboos(state, taboo = taboos):\n",
    "    return state in taboo\n",
    "\n",
    "## Test the function\n",
    "for a in ['u', 'd', 'r', 'l']:\n",
    "    print(simulate_environment(11, a,terminal=12))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the expected results here from 11 it can only to 12 or 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_policy  = {0:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      1:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      2:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      3:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      4:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      5:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      6:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      7:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      8:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      9:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      10:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      11:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                      13:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      14:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      15:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      16:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      17:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      18:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      19:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      20:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      21:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      22:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      23:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      24:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def start_episode(n_states, n_actions):\n",
    "    '''Function to find a random starting values for the episode\n",
    "    that is not the terminal state'''\n",
    "    state = nr.choice(range(n_states))\n",
    "    while(is_terminal(state) or is_taboos(state)):  ## Make sure not starting at the terminal state\n",
    "         state = nr.choice(range(n_states))\n",
    "    ## Now find a random starting action index\n",
    "    a_index = nr.choice(range(4), size = 1)[0]\n",
    "    s_prime, reward, terminal = simulate_environment(state, action_lookup(a_index))   \n",
    "    return state, a_index, reward[a_index] ## action_lookup(a_index), reward[a_index]\n",
    "\n",
    "## test the function to make sure never starting in terminal state\n",
    "test = [start_episode(25,4) for _ in range(25)]\n",
    "starts = [i[0] for i in test]\n",
    "True not in list(set([i in taboos for i in starts]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the algo cannot starts from the terminal state or a taboo state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('r', 1, array([-1. , -1. , -0.1, -0.1]), False)\n",
      "1 ('d', 1, array([-1. , -1. , -0.1, -0.1]), False)\n",
      "2 ('r', 3, array([-1. , -1. , -0.1, -0.1]), False)\n",
      "3 ('r', 4, array([-1. , -0.1, -0.1, -1. ]), False)\n",
      "4 ('u', 4, array([-1. , -0.1, -0.1, -1. ]), False)\n",
      "5 ('l', 5, array([-0.1, -0.1, -1. , -1. ]), False)\n",
      "6 ('d', 6, array([-1, -1, -1, -1]), False)\n",
      "7 ('l', 7, array([-1, -1, -1, -1]), False)\n",
      "8 ('r', 8, array([-1, -1, -1, -1]), False)\n",
      "9 ('r', 9, array([-0.1, -0.1, -1. , -1. ]), False)\n",
      "10 ('u', 5, array([-0.1, -0.1, -1. , -1. ]), False)\n",
      "11 ('r', 12, array([10., 10., 10., 10.]), True)\n",
      "12 ('l', 12, array([10., 10., 10., 10.]), True)\n",
      "13 ('u', 13, array([-1. , -1. , 10. , -0.1]), False)\n",
      "14 ('r', 14, array([-0.1, -0.1, -0.1, -1. ]), False)\n",
      "15 ('r', 15, array([-0.1, -0.1, -1. , -1. ]), False)\n",
      "16 ('l', 16, array([-1, -1, -1, -1]), False)\n",
      "17 ('r', 17, array([-1, -1, -1, -1]), False)\n",
      "18 ('d', 18, array([-0.1, -0.1, -0.1, -0.1]), False)\n",
      "19 ('r', 19, array([-0.1, -0.1, -1. , -1. ]), False)\n",
      "20 ('r', 21, array([-1. , -1. , -0.1, -0.1]), False)\n",
      "21 ('l', 20, array([-0.1, -1. , -1. , -0.1]), False)\n",
      "22 ('r', 23, array([-1. , -1. , -0.1, -0.1]), False)\n",
      "23 ('u', 23, array([-1. , -1. , -0.1, -0.1]), False)\n",
      "24 ('u', 19, array([-0.1, -0.1, -1. , -1. ]), False)\n"
     ]
    }
   ],
   "source": [
    "def take_action(state, policy):\n",
    "    '''Function takes action given state using the transition probabilities \n",
    "    of the policy'''\n",
    "    ## Find the action given the transistion probabilities defined by the policy.\n",
    "    action = action_lookup(nr.choice(range(len(policy[0].keys())), p = list(policy[state].values()))) \n",
    "    s_prime, reward, terminal = simulate_environment(state, action)\n",
    "    return (action, s_prime, reward, terminal)\n",
    "\n",
    "## Test function for several states\n",
    "for s in range(25):\n",
    "    print('{} {}'.format(s, take_action(s, initial_policy)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('r', 1, array([-1. , -1. , -0.1, -0.1]), False)\n",
      "1 ('l', 0, array([-1. , -0.1, -1. , -0.1]), False)\n",
      "2 ('l', 1, array([-1. , -1. , -0.1, -0.1]), False)\n",
      "3 ('r', 4, array([-1. , -0.1, -0.1, -1. ]), False)\n",
      "4 ('u', 4, array([-1. , -0.1, -0.1, -1. ]), False)\n",
      "5 ('r', 5, array([-0.1, -0.1, -1. , -1. ]), False)\n",
      "9 ('d', 14, array([-0.1, -0.1, -0.1, -1. ]), False)\n",
      "10 ('l', 10, array([-0.1, -0.1, -1. , -0.1]), False)\n",
      "11 ('l', 10, array([-0.1, -0.1, -1. , -0.1]), False)\n",
      "12 ('u', 12, array([10., 10., 10., 10.]), True)\n",
      "13 ('d', 13, array([-1. , -1. , 10. , -0.1]), False)\n",
      "14 ('d', 19, array([-0.1, -0.1, -1. , -1. ]), False)\n",
      "15 ('l', 15, array([-0.1, -0.1, -1. , -1. ]), False)\n",
      "19 ('d', 24, array([-0.1, -1. , -0.1, -1. ]), False)\n",
      "20 ('l', 20, array([-0.1, -1. , -1. , -0.1]), False)\n",
      "21 ('d', 21, array([-1. , -1. , -0.1, -0.1]), False)\n",
      "22 ('u', 22, array([-1. , -1. , -0.1, -0.1]), False)\n",
      "23 ('d', 23, array([-1. , -1. , -0.1, -0.1]), False)\n",
      "24 ('r', 24, array([-0.1, -1. , -0.1, -1. ]), False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for s in range(25):\n",
    "    if s not in taboos:\n",
    "        tmp = take_action(s, initial_policy)\n",
    "        print('{} {}'.format(s, tmp))\n",
    "        res.append(tmp[1] in taboos)\n",
    "True in list(set(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taboo states are not visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           up      down       left      right\n",
      "0    6.423639  8.455722   5.902723   7.710505\n",
      "1    5.854538  4.337789   7.781621   7.762289\n",
      "2    5.751832  5.223717   7.781621   7.869662\n",
      "3    5.759075  6.038772   7.725403   8.423515\n",
      "4    6.492246  9.063100   7.714373   5.438930\n",
      "5    7.889689  9.387642   7.346402   5.435206\n",
      "6    0.000000  0.000000   0.000000   0.000000\n",
      "7    0.000000  0.000000   0.000000   0.000000\n",
      "8    0.000000  0.000000   0.000000   0.000000\n",
      "9    7.718596  9.970794   5.570216   5.245708\n",
      "10   8.615413  8.796795   6.565969  12.113791\n",
      "11   9.370627  9.486400   7.572669  11.111111\n",
      "12   0.000000  0.000000   0.000000   0.000000\n",
      "13   9.242155  8.625520  11.111111   7.483555\n",
      "14   8.880189  8.735171  11.465429   4.674157\n",
      "15   9.113585  8.154488   6.831617   5.377303\n",
      "16   0.000000  0.000000   0.000000   0.000000\n",
      "17   0.000000  0.000000   0.000000   0.000000\n",
      "18   0.000000  0.000000   0.000000   0.000000\n",
      "19  10.125211  7.981740   6.707825   4.953963\n",
      "20   8.549950  6.032111   5.014034   7.680084\n",
      "21   5.811142  5.711407   8.175182   7.407516\n",
      "22   5.770737  5.817496   7.769647   7.442189\n",
      "23   4.761995  5.221853   7.484934   7.901515\n",
      "24   8.544992  3.269829   7.578580   4.842269\n"
     ]
    }
   ],
   "source": [
    "def print_Q(Q):\n",
    "    Q = pd.DataFrame(Q, columns = ['up', 'down', 'left', 'right'])\n",
    "    print(Q)\n",
    "\n",
    "def update_Q(Q, current_state, a_index, reward, alpha, gamma):\n",
    "    \"\"\"Function to update the actions values in the Q matrix\"\"\"\n",
    "    ## Get s_prime given s and a\n",
    "    s_prime, reward_prime, terminal = simulate_environment(current_state, action_lookup(a_index))\n",
    "    a_prime_index = nr.choice(np.where(reward_prime == max(reward_prime))[0], size = 1)[0]\n",
    "    ## Update the action values \n",
    "    Q[current_state,a_index] = Q[current_state,a_index] + alpha * (reward + gamma * (Q[s_prime,a_prime_index] - Q[current_state,a_index]))\n",
    "    return Q, s_prime, reward_prime, terminal, a_prime_index\n",
    "\n",
    "def Q_learning_0(policy, episodes, alpha = 0.2, gamma = 0.9):\n",
    "    \"\"\"\n",
    "    Function to perform Q-learning(0) control policy improvement.\n",
    "    \"\"\"\n",
    "    ## Initialize the state list and action values\n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    n_actions = len(policy[0].keys())\n",
    "    \n",
    "    ## Initialize Q matrix\n",
    "    Q = np.zeros((n_states,n_actions))\n",
    "    \n",
    "    for _ in range(episodes): # Loop over the episodes\n",
    "        terminal = False\n",
    "        ## Find the inital state, action index and reward\n",
    "        current_state, a_index, reward = start_episode(n_states,n_actions)\n",
    "        \n",
    "        while(not terminal): # Episode ends where get to terminal state   \n",
    "            ## Update the action values in Q\n",
    "            Q, s_prime, reward_prime, terminal, a_prime_index = update_Q(Q, current_state, a_index, reward, alpha, gamma)\n",
    "            ## Set action, reward and state for next iteration\n",
    "            a_index = a_prime_index\n",
    "            current_state = s_prime\n",
    "            reward = reward_prime[a_prime_index]\n",
    "    return(Q)\n",
    "\n",
    "Q = Q_learning_0(initial_policy, 1000)\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'u': 0.1, 'd': 0.7, 'l': 0.1, 'r': 0.1},\n",
       " 1: {'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1},\n",
       " 2: {'u': 0.1, 'd': 0.1, 'l': 0.1, 'r': 0.7},\n",
       " 3: {'u': 0.1, 'd': 0.1, 'l': 0.1, 'r': 0.7},\n",
       " 4: {'u': 0.1, 'd': 0.7, 'l': 0.1, 'r': 0.1},\n",
       " 5: {'u': 0.1, 'd': 0.7, 'l': 0.1, 'r': 0.1},\n",
       " 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 7: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 8: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 9: {'u': 0.1, 'd': 0.7, 'l': 0.1, 'r': 0.1},\n",
       " 10: {'u': 0.1, 'd': 0.1, 'l': 0.1, 'r': 0.7},\n",
       " 11: {'u': 0.1, 'd': 0.1, 'l': 0.1, 'r': 0.7},\n",
       " 12: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 13: {'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1},\n",
       " 14: {'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1},\n",
       " 15: {'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1},\n",
       " 16: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 17: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 18: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 19: {'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1},\n",
       " 20: {'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1},\n",
       " 21: {'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1},\n",
       " 22: {'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1},\n",
       " 23: {'u': 0.1, 'd': 0.1, 'l': 0.1, 'r': 0.7},\n",
       " 24: {'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}}"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_policy(policy, Q, epsilon):\n",
    "    '''Updates the policy based on estiamtes of Q using \n",
    "    an epslion greedy algorithm. The action with the highest\n",
    "    action value is used.'''\n",
    "    \n",
    "    ## Find the keys for the actions in the policy\n",
    "    keys = list(policy[0].keys())\n",
    "    \n",
    "    ## Iterate over the states and find the maximm action value.\n",
    "    for state in range(len(policy)):\n",
    "        ## First find the index of the max Q values  \n",
    "        q = Q[state,:]\n",
    "        max_action_index = np.where(q == max(q))[0]\n",
    "\n",
    "        ## Find the probabilities for the transitions\n",
    "        n_transitions = float(len(q))\n",
    "        n_max_transitions = float(len(max_action_index))\n",
    "        p_max_transitions = (1.0 - epsilon *(n_transitions - n_max_transitions))/(n_max_transitions)\n",
    "  \n",
    "        ## Now assign the probabilities to the policy as epsilon greedy.\n",
    "        for key in keys:\n",
    "            if(index_lookup(key) in max_action_index): policy[state][key] = p_max_transitions\n",
    "            else: policy[state][key] = epsilon\n",
    "    return(policy)                \n",
    "\n",
    "update_policy(initial_policy, Q, 0.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine your results. Are the action values consistent with the transitions?\n",
    "\n",
    "ANS:  Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you need to create a function to compute the state values using the TD(0) algorithm. You should use the function you just created  to find the rewards and next state given a state and action. You are welcome to use the `td_0_state_values` function from the TD/Q-learning notebook as a starting point.  \n",
    "\n",
    "Execute your function for 1,000 episodes and examine the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo Code for TD(0)\n",
    "\n",
    "```\n",
    "Evaluate_Policy(policy):\n",
    "  randomly_initialize_non_terminal_states_values()\n",
    "Loop number_of_episodes:\n",
    "  let s = start_state()\n",
    "  # Play episode until the end\n",
    "  Loop until game_over():\n",
    "    let a = get_action(policy, s, 0.1) \n",
    "                      # get action to perform on state s according \n",
    "                      # to the given policy 90% of the time, and a\n",
    "                      # random action 10% of the time.\n",
    "    let (s', r) = make_move(s, a) #make move from s using a and get \n",
    "                                  #the new state s' and the reward r\n",
    "\n",
    "     # incrementally compute the average at V(s). Notice that V(s)\n",
    "     # depends on an estimate of V(s') and not on the return \n",
    "     # G as in MC \n",
    "     let V(s) = V(s) + alpha * [r + gamma * V(s') - V(s)]\n",
    "    let s = s'\n",
    " End Loop\n",
    "End Loop\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15\n",
      "reward=[-0.1 -0.1 -1.  -0.1]\n",
      "delta=[-0.1 -0.1 -1.  -0.1]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (4) into shape (1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-b7ad36364df5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mtd_0_state_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-192-b7ad36364df5>\u001b[0m in \u001b[0;36mtd_0_state_values\u001b[0;34m(policy, n_samps, alpha, gamma)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m## Update the state value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcurrent_state\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtaboos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (4) into shape (1)"
     ]
    }
   ],
   "source": [
    "def td_0_state_values(policy, n_samps, alpha = 0.2, gamma = 1.0):\n",
    "    \"\"\"\n",
    "    Function for TD(0) policy evalutation\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Find the starting state\n",
    "    n_states = len(policy)\n",
    "    n_actions = len(policy[0].keys())\n",
    "    current_state = start_episode(n_states, n_actions )[0]\n",
    "    terminal = False\n",
    "    ## Array for state values\n",
    "    v = np.zeros((n_states,1))\n",
    "    \n",
    "    for _ in range(n_samps):\n",
    "        ## Find the next action and reward\n",
    "        print('{} {}'.format(_, current_state))\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "        ## Compute the TD error\n",
    "        \n",
    "        delta = reward + gamma*v[s_prime] - v[current_state]\n",
    "        print('reward={}'.format(reward))\n",
    "        print('delta={}'.format(delta))\n",
    "            ## Update the state value\n",
    "        if current_state not in taboos:\n",
    "            v[current_state] = v[current_state] + alpha*delta\n",
    "        else:\n",
    "            v[current_state] = 0\n",
    "        current_state = s_prime\n",
    "        if(terminal): ## start new episode when terminal\n",
    "            current_state = start_episode(n_states, n_actions)[0]\n",
    "    return(v)\n",
    "\n",
    "td_0_state_values(initial_policy, 1000).reshape((5,5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From IntroductionToTDLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = {0:{'u':0, 'd':5, 'l':0, 'r':1},\n",
    "          1:{'u':1, 'd':1, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':2, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':3, 'l':2, 'r':4},\n",
    "          4:{'u':4, 'd':9, 'l':3, 'r':4},\n",
    "          5:{'u':0, 'd':10, 'l':5, 'r':5},\n",
    "          6:{'u':6, 'd':6, 'l':6, 'r':6},\n",
    "          7:{'u':7, 'd':7, 'l':7, 'r':7},\n",
    "          8:{'u':8, 'd':8, 'l':8, 'r':8},\n",
    "          9:{'u':4, 'd':14, 'l':9, 'r':9},\n",
    "          10:{'u':5, 'd':15, 'l':10, 'r':11},\n",
    "          11:{'u':11, 'd':11, 'l':10, 'r':12},\n",
    "          12:{'u':12, 'd':12, 'l':12, 'r':12},\n",
    "          13:{'u':13, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':9, 'd':19, 'l':13, 'r':14},\n",
    "          15:{'u':10, 'd':20, 'l':15, 'r':15},\n",
    "          16:{'u':16, 'd':16, 'l':16, 'r':16},\n",
    "          17:{'u':17, 'd':17, 'l':17, 'r':17},\n",
    "          18:{'u':18, 'd':18, 'l':18, 'r':18},\n",
    "          19:{'u':14, 'd':24, 'l':19, 'r':19},\n",
    "          20:{'u':15, 'd':20, 'l':20, 'r':21},\n",
    "          21:{'u':21, 'd':21, 'l':20, 'r':22},\n",
    "          22:{'u':22, 'd':22, 'l':21, 'r':23},\n",
    "          23:{'u':23, 'd':23, 'l':22, 'r':24},\n",
    "          24:{'u':19, 'd':24, 'l':23, 'r':24}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards =  {0:{'u':-1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "          1:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          2:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          4:{'u':-1, 'd':-0.1, 'l':-0.1, 'r':-1},\n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          6:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          7:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          8:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "          11:{'u':-1, 'd':-1, 'l':-0.1, 'r':10},\n",
    "          12:{'u':10.0, 'd':10.0, 'l':10.0, 'r':10.0}, # 12:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0}\n",
    "          13:{'u':-1, 'd':-1, 'l':10, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1},\n",
    "          15:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          16:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          17:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          18:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          19:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          20:{'u':-0.1, 'd':-1, 'l':-1, 'r':-0.1},\n",
    "          21:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          22:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          23:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          24:{'u':-0.1, 'd':-1, 'l':-0.1, 'r':-1}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "taboos = [6, 7, 8, 16, 17, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, -1, False)\n",
      "(11, -1, False)\n",
      "(12, 10, True)\n",
      "(10, -0.1, False)\n"
     ]
    }
   ],
   "source": [
    "def simulate_environment(s, action, neighbors = neighbors, rewards = rewards, terminal = 12):\n",
    "    \"\"\"\n",
    "    Function simulates the environment\n",
    "    returns s_prime and reward given s and action\n",
    "    \"\"\"\n",
    "    s_prime = neighbors[s][action]\n",
    "    reward = rewards[s][action]\n",
    "    return (s_prime, reward, is_terminal(s_prime, terminal))\n",
    "\n",
    "def is_terminal(state, terminal = 12):\n",
    "    return state == terminal\n",
    "\n",
    "#adding a function to take care of the taboos\n",
    "def is_taboos(state, taboo = taboos):\n",
    "    return state in taboo\n",
    "\n",
    "## Test the function\n",
    "for a in ['u', 'd', 'r', 'l']:\n",
    "    print(simulate_environment(11, a,terminal=12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_policy  = {0:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      1:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      2:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      3:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      4:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      5:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      6:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      7:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      8:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      9:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      10:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      11:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                      13:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      14:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      15:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      16:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      17:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      18:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      19:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      20:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      21:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      22:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      23:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      24:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14, 24, 23, 19, 1, 20, 21, 3, 2, 24]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def start_episode(n_states):\n",
    "    '''Function to find a random starting value for the episode\n",
    "    that is not the terminal state'''\n",
    "    state = nr.choice(range(n_states))\n",
    "    while(is_terminal(state) or is_taboos(state)):\n",
    "         state = nr.choice(range(n_states))\n",
    "    return state\n",
    "\n",
    "## test the function to make sure never starting in terminal state\n",
    "[start_episode(25) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "starts = [i[0] for i in test]\n",
    "True not in list(set([i in taboos for i in starts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('d', 5, -0.1, False)\n",
      "1 ('r', 2, -0.1, False)\n",
      "2 ('r', 3, -0.1, False)\n",
      "3 ('d', 3, -1, False)\n",
      "4 ('u', 4, -1, False)\n",
      "5 ('r', 5, -1, False)\n",
      "6 ('r', 6, -1, False)\n",
      "7 ('d', 7, -1, False)\n",
      "8 ('l', 8, -1, False)\n",
      "9 ('r', 9, -1, False)\n",
      "10 ('r', 11, -0.1, False)\n",
      "11 ('d', 11, -1, False)\n",
      "12 ('u', 12, 10.0, True)\n",
      "13 ('r', 14, -0.1, False)\n",
      "14 ('r', 14, -1, False)\n",
      "15 ('l', 15, -1, False)\n",
      "16 ('u', 16, -1, False)\n",
      "17 ('l', 17, -1, False)\n",
      "18 ('u', 18, -0.1, False)\n",
      "19 ('u', 14, -0.1, False)\n",
      "20 ('d', 20, -1, False)\n",
      "21 ('d', 21, -1, False)\n",
      "22 ('d', 22, -1, False)\n",
      "23 ('u', 23, -1, False)\n",
      "24 ('r', 24, -1, False)\n"
     ]
    }
   ],
   "source": [
    "def take_action(state, policy, actions = {1:'u', 2:'d', 3:'l', 4:'r'}):\n",
    "    '''Function takes action given state using the transition probabilities \n",
    "    of the policy'''\n",
    "    ## Find the action given the transistion probabilities defined by the policy.\n",
    "    action = actions[nr.choice(range(len(actions)), p = list(policy[state].values())) + 1]\n",
    "    s_prime, reward, terminal = simulate_environment(state, action)\n",
    "    return (action, s_prime, reward, terminal)\n",
    "\n",
    "## Test function for several states\n",
    "for s in range(25):\n",
    "    print('{} {}'.format(s,take_action(s, initial_policy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ('r', 1, -0.1, False)\n",
      "1 ('u', 1, -1, False)\n",
      "2 ('u', 2, -1, False)\n",
      "3 ('l', 2, -0.1, False)\n",
      "4 ('l', 3, -0.1, False)\n",
      "5 ('r', 5, -1, False)\n",
      "9 ('d', 14, -0.1, False)\n",
      "10 ('d', 15, -0.1, False)\n",
      "11 ('l', 10, -0.1, False)\n",
      "12 ('d', 12, 10.0, True)\n",
      "13 ('d', 13, -1, False)\n",
      "14 ('r', 14, -1, False)\n",
      "15 ('d', 20, -0.1, False)\n",
      "19 ('u', 14, -0.1, False)\n",
      "20 ('r', 21, -0.1, False)\n",
      "21 ('u', 21, -1, False)\n",
      "22 ('u', 22, -1, False)\n",
      "23 ('u', 23, -1, False)\n",
      "24 ('r', 24, -1, False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for s in range(25):\n",
    "    if s not in taboos:\n",
    "        tmp = take_action(s, initial_policy)\n",
    "        print('{} {}'.format(s, tmp))\n",
    "        res.append(tmp[1] in taboos)\n",
    "True in list(set(res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine your results and answer the following questions to ensure you action value function operates correctly:\n",
    "1. Are the values of the taboo states 0? ANS:\n",
    "2. Are the states with the highest values adjacent to the terminal state? ANS: \n",
    "3. Are the values of the states decreasing as the distance from the terminal state increases? ANS: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## SARSA(0) Policy Improvement\n",
    "\n",
    "Now you will perform policy improvement using the SARSA(0) algorithm.  You are welcome to start with the `select_a_prime` and `SARSA_0` functions from the TD/Q-learning notebooks.    \n",
    "\n",
    "Execute your code for 1,000 episodes, and with $\\alpha = 0.2$, and $\\epsilon = 0.1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1 -0.1 -1.  -1. ]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-37749786fac3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSARSA_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0mprint_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-180-37749786fac3>\u001b[0m in \u001b[0;36mSARSA_0\u001b[0;34m(policy, n_samps, alpha, gamma, action_index)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m## Update the action values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m## Update the state, action and reward for the next time step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms_prime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "def print_Q(Q):\n",
    "    Q = pd.DataFrame(Q, columns = ['up', 'down', 'left', 'right'])\n",
    "    print(Q)\n",
    "\n",
    "def new_episode(n_states, policy):\n",
    "    '''This function provides a start for a TD\n",
    "    episode making sure the first transition is not \n",
    "    the termnal state'''\n",
    "    n_actions = len(policy[0].keys())\n",
    "    current_state = start_episode(n_states,n_actions )[0]\n",
    "    ## Find fist action and reward\n",
    "    action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "    return(current_state, action, s_prime, reward, terminal)    \n",
    "\n",
    "\n",
    "def SARSA_0(policy, n_samps, alpha = 0.1, gamma = 0.9, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    \"\"\"\n",
    "    Function for TD(0) policy evalutation\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Find the starting state\n",
    "    n_states = len(policy)\n",
    "    n_actions = len(policy[0].keys())\n",
    "    current_state, action, s_prime, reward, terminal = new_episode(n_states, policy)\n",
    "    action_idx = action_index[action]\n",
    "    \n",
    "    ## Array for state values\n",
    "    q = np.zeros((n_states, len(policy[0])))\n",
    "    \n",
    "    for _ in range(n_samps):\n",
    "        ## Find the next action and reward\n",
    "        action_prime, s_prime_prime, reward_prime, terminal_prime = take_action(s_prime, policy)\n",
    "        action_idx_prime = action_index[action_prime]\n",
    "        ## Compute the TD error\n",
    "        delta = reward + gamma*q[s_prime, action_idx_prime] - q[current_state, action_idx]\n",
    "        print(delta)\n",
    "        ## Update the action values\n",
    "        q[current_state, action_idx] = q[current_state, action_idx] + alpha*delta\n",
    "        ## Update the state, action and reward for the next time step\n",
    "        current_state = s_prime\n",
    "        s_prime = s_prime_prime\n",
    "        action = action_prime\n",
    "        reward = reward_prime\n",
    "        terminal = terminal_prime\n",
    "        action_idx = action_idx_prime\n",
    "\n",
    "        ## Check if end of episode\n",
    "        if(terminal): \n",
    "            ## start new episode\n",
    "            current_state, action, s_prime, reward, terminal = new_episode(n_states, policy)        \n",
    "    return(q)\n",
    "\n",
    "\n",
    "Q = SARSA_0(initial_policy, 1000, alpha = 0.2, gamma = 0.1)\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the action values you have computed. Ensure that the action values are 0 for the goal and taboo states. Also check that the actions with the largest values for each state make sense in terms of reaching the goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the action value function completed, you will now create and test code to perform GPI with SARSA(0).  You are welcome to use the `SRASA_0_GPI` function from the TD/Q-learning notebook as a starting point. \n",
    "\n",
    "Execute your code for 10 cycles of 100 episodes, with $\\alpha = 0.2$, $\\gamma = 0.9$ and $\\epsilon = 0.01$, and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your results make sense? For example, starting at state 2 or 22, do the most probable actions follow a shortest path?\n",
    "\n",
    "ANS: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Double Q-Learning\n",
    "\n",
    "As a next step, you will apply Double Q-learning(0) to the warehouse navigation problem. In the cell below create and test a function to perform Double Q-Learning for this problem. You are welcome to use the `double_Q_learning` function from the TD/Q-learning notebook as a starting point.\n",
    "\n",
    "Execute your code for 10 cycles of 500 episodes, with $\\alpha = 0.2$, and $\\gamma = 0.9$ and examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the action values you have computed. Ensure that the action values are 0 for the goal and taboo states. Also check that the actions with the largest values for each state make sense in terms of reaching the goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the action value function completed, you will now create and test code to perform GPI with Double Q-Learning(0).  You are welcome to use the `double_Q_learning_0_GPI` function from the TD/Q-learning notebook as a starting point. \n",
    "\n",
    "Execute your code for 10 cycles of 500 episodes, with $\\alpha = 0.2$, $\\gamma = 0.9$ and $\\epsilon = 0.01$, and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that your results make sense? For example, starting at state 2 or 22, do the most probable actions follow a shortest path?\n",
    "\n",
    "ANS: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Step TD Learning\n",
    "\n",
    "Finally, you will apply N-Step TD learning and N-Step SARSA to the warehouse navigation problem.  First create a function to perform N-step TD policy evaluation. You are welcome to start with the `TD_n` policy evaluation function from the TD/Q-Learning notebook. \n",
    "\n",
    "Test your function using 1,000 episodes, $n = 4$, $\\gamma = 0.9$, and $\\alpha = 0.2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the result you obtained appears correct. Are the values of the goal and taboo states all 0? Do the state values decrease with distance from the goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have an estimate of the best values for the number of steps and the learning rate you can compute the action values using multi-step SARSA. In the cell below, create and test a function to compute the action values using N-step SARSA. You are welcome to use the `SRARSA_n` function from the TD/Q-learning notebook as a starting point. \n",
    "\n",
    "Test your function by executing 4-step SARSA for 1,000 episodes with $\\alpha = 0.2$ and $\\gamma = 0.9$ and using the optimum number of steps and learning rate you have determined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the results you have computed appear correct using the aforementioned criteria. \n",
    "\n",
    "Finally, create a function to use the GPI algorithm with N-step SARSA in the cell below. You are welcome to start with the `SARSA_n_GPI` function from the TD/Q-learning notebook. \n",
    "\n",
    "Execute your function using 4 step SARSA for 5 cycles of 500 episodes, with $\\alpha = 0.2$, $\\epsilon = 0.1$, and $\\gamma = 0.9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine your results. Verify that the most probable paths to the goal from states 2 and 22 are the shortest possible.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
