{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Sequence-to-Sequence: Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In this assignment you will use a database of pairs of (English,French) sentences to train an RNN model to translate from English to French.\n",
    "\n",
    "The directory ../resource/asnlib/publicdata contains two files, \"small_vocab_en.txt\" and \"small_vocab_fr.txt\". Line \"n\" of the first file corresponds to line \"n\" of the second file.\n",
    "\n",
    "Also see data here: http://www.statmt.org/wmt14/translation-task.html\n",
    "\n",
    "Keras resources: \n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "* https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "* https://stackoverflow.com/questions/38714959/understanding-keras-lstms/50235563#50235563\n",
    "\n",
    "Neural Language Translation Resources:\n",
    "* https://arxiv.org/abs/1703.01619\n",
    "* https://www.tensorflow.org/tutorials/seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Example Keras github](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py)\n",
    "- [Language translation toward data science](https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571)\n",
    "- [How to Develop an Encoder-Decoder Model for Sequence-to-Sequence Prediction in Keras](https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import os\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Input, GRU, LSTM, Dense, Masking, Dropout, Embedding, Flatten, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Configure Tensorflow to be less aggressive about RAM utilization when it starts up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1  # Start with 10% of the GPU RAM\n",
    "config.gpu_options.allow_growth = True                    # Dynamically grow the memory used on the GPU\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)                                         # Set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.7           # % of data in training set\n",
    "\n",
    "NUM_LSTM_NODES = 256             # Num of intermediate LSTM nodes\n",
    "CONTEXT_VECTOR_SIZE = 256        # Size of context vector (num of LSTM nodes in final LSTM layer)\n",
    "\n",
    "EMBEDDING_DIM = 100              # Embedding layer size for input words\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "NUM_DATA_EXAMPLES = 5000         # limit memory usage while experimenting\n",
    "\n",
    "LR = 0.01\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Text Preprocessing\n",
    "These are provided so you can focus on the neural net modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A useful string full of characters to remove\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_space_around_punctuation(s):\n",
    "    result = ''\n",
    "    for c in s:\n",
    "        if c in string.punctuation and c != \"'\":  # Apostrophes are important\n",
    "            result += ' %s ' % c\n",
    "        else:\n",
    "            result += c\n",
    "    return result\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    s = add_space_around_punctuation(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Functions to get words from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_words_from_sentence(s, add_start_symbol=False, add_end_symbol=False, reverse=False):\n",
    "    words = list(filter(None, s.split(' ')))\n",
    "    if reverse:\n",
    "        words = words[::-1]\n",
    "    if add_start_symbol:\n",
    "        words = ['<S>'] + words\n",
    "    if add_end_symbol:\n",
    "        words.append('</S>')\n",
    "    return words\n",
    "\n",
    "def get_word_list_from_sentence_string(s, add_start_symbol=False, add_end_symbol=False, reverse=False):\n",
    "    return get_words_from_sentence(clean_sentence(s), add_start_symbol, add_end_symbol, reverse)    \n",
    "    \n",
    "def get_sentences(path, filename, add_start_symbol=False, add_end_symbol=False, reverse=False):\n",
    "    with open(os.path.join(path, filename), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        return [get_word_list_from_sentence_string(s, add_start_symbol, add_end_symbol, reverse) \n",
    "                for s in lines]\n",
    "\n",
    "def get_word_set(sentences):\n",
    "    words = set()\n",
    "    for s in sentences:\n",
    "        for word in s:\n",
    "            words.add(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Read the data and build useful data structures, such as a list of sentences and lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guill\\Desktop\\test\\UW_ML_Q3_Deep_Learning\\Lab_6\n",
      "19\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# Store the input sentences (English) in s1\n",
    "# Store the target senteces (French) in s2\n",
    "\n",
    "# Consider reversing the input sentences to improve trianing.\n",
    "# Add start and stop symbols for the decoder.\n",
    "PATH = '../resource/asnlib/publicdata'\n",
    "local_bool = False\n",
    "# Store the input sentences (English) in s1\n",
    "# Store the target senteces (French) in s2\n",
    "import os\n",
    "# Consider reversing the input sentences to improve trianing.\n",
    "# Add start and stop symbols for the decoder.\n",
    "try:\n",
    "    PATH = '../resource/asnlib/publicdata'\n",
    "    s1 = get_sentences(PATH, 'small_vocab_en.txt', add_start_symbol=True, add_end_symbol=True)       # TODO\n",
    "    s1 = [lst[::-1] for lst in s1]\n",
    "    s2 = get_sentences(PATH, 'small_vocab_fr.txt', add_start_symbol=True, add_end_symbol=True)  \n",
    "     \n",
    "except FileNotFoundError:\n",
    "    local_bool = True\n",
    "    os.chdir(r\"C:\\Users\\guill\\Desktop\\test\\UW_ML_Q3_Deep_Learning\\Lab_6\")\n",
    "    print(os.getcwd())\n",
    "    PATH = ''\n",
    "    s1 = get_sentences(PATH, 'small_vocab_en.txt', add_start_symbol=True, add_end_symbol=True)       # TODO\n",
    "    s1 = [lst[::-1] for lst in s1]\n",
    "    s2 = get_sentences(PATH, 'small_vocab_fr.txt', add_start_symbol=True, add_end_symbol=True)  \n",
    " \n",
    "     # TODO\n",
    "max_encoder_seq_length = max([len(txt) for txt in s1])\n",
    "max_decoder_seq_length = max([len(txt) for txt in s2])\n",
    "print(max_encoder_seq_length )\n",
    "print(max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137860\n",
      "137860\n"
     ]
    }
   ],
   "source": [
    "# Restruct to a subset of the data\n",
    "print(len(s1))\n",
    "print(len(s2))\n",
    "s1 = s1[:NUM_DATA_EXAMPLES]\n",
    "s2 = s2[:NUM_DATA_EXAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</S> . april in snowy is it and , autumn during quiet sometimes is jersey new <S>',\n",
       " \"<S> new jersey est parfois calme pendant l' automne , et il est neigeux en avril . </S>\")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a sample sentence pair.\n",
    "' '.join(s1[0]), ' '.join(s2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_s(s):\n",
    "    func = lambda l: [item for sublist in l for item in sublist]\n",
    "    tmp = list(set(func(s)))\n",
    "    #final = [i for i in tmp if i[0] not in  string.punctuation]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two lists, w1 and w2, which hold the set of all words that show up in s1 and s2.\n",
    "\n",
    "w1 = split_s(s1) \n",
    "w2 = split_s(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert('<S>' in w1)\n",
    "assert('</S>' in w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Utilities for mapping words to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_word_to_index_dict(words):\n",
    "    return {w: i+1 for i,w in enumerate(words)}  # use i+1 to reserve 0 for the mask index\n",
    "def reverse_dict(d):\n",
    "    return {v: k for k,v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_to_index1 = get_word_to_index_dict(w1)\n",
    "word_to_index2 = get_word_to_index_dict(w2)\n",
    "index_to_word1 = reverse_dict(word_to_index1)\n",
    "index_to_word2 = reverse_dict(word_to_index2)\n",
    "index_to_word1[0] = '<MASK>'\n",
    "index_to_word2[0] = '<MASK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'tower',\n",
       " 2: ',',\n",
       " 3: 'school',\n",
       " 4: 'wonderful',\n",
       " 5: 'spring',\n",
       " 6: 'banana',\n",
       " 7: 'monkey',\n",
       " 8: 'orange',\n",
       " 9: 'pleasant',\n",
       " 10: 'winter',\n",
       " 11: 'bananas',\n",
       " 12: 'are',\n",
       " 13: 'it',\n",
       " 14: 'jersey',\n",
       " 15: 'autumn',\n",
       " 16: 'translate',\n",
       " 17: 'grocery',\n",
       " 18: 'mild',\n",
       " 19: 'that',\n",
       " 20: 'dry',\n",
       " 21: 'they',\n",
       " 22: 'paris',\n",
       " 23: 'eiffel',\n",
       " 24: 'wet',\n",
       " 25: 'is',\n",
       " 26: 'saw',\n",
       " 27: 'shark',\n",
       " 28: 'big',\n",
       " 29: 'this',\n",
       " 30: 'rabbits',\n",
       " 31: 'usually',\n",
       " 32: 'and',\n",
       " 33: 'dislike',\n",
       " 34: 'horses',\n",
       " 35: 'apple',\n",
       " 36: 'during',\n",
       " 37: 'monkeys',\n",
       " 38: 'bears',\n",
       " 39: 'was',\n",
       " 40: 'plans',\n",
       " 41: 'thinks',\n",
       " 42: 'fruit',\n",
       " 43: 'peaches',\n",
       " 44: 'he',\n",
       " 45: 'has',\n",
       " 46: 'california',\n",
       " 47: 'rainy',\n",
       " 48: 'mango',\n",
       " 49: 'peach',\n",
       " 50: 'where',\n",
       " 51: 'cats',\n",
       " 52: 'chilly',\n",
       " 53: 'to',\n",
       " 54: 'hot',\n",
       " 55: 'translating',\n",
       " 56: 'favorite',\n",
       " 57: 'difficult',\n",
       " 58: 'never',\n",
       " 59: 'loved',\n",
       " 60: 'you',\n",
       " 61: 'dog',\n",
       " 62: 'snake',\n",
       " 63: 'september',\n",
       " 64: 'busy',\n",
       " 65: '<S>',\n",
       " 66: 'mouse',\n",
       " 67: 'think',\n",
       " 68: 'car',\n",
       " 69: 'animals',\n",
       " 70: 'lake',\n",
       " 71: 'limes',\n",
       " 72: 'liked',\n",
       " 73: 'would',\n",
       " 74: 'relaxing',\n",
       " 75: 'october',\n",
       " 76: 'visit',\n",
       " 77: 'dogs',\n",
       " 78: 'states',\n",
       " 79: 'the',\n",
       " 80: 'blue',\n",
       " 81: 'birds',\n",
       " 82: 'how',\n",
       " 83: 'do',\n",
       " 84: 'bird',\n",
       " 85: 'english',\n",
       " 86: 'his',\n",
       " 87: 'february',\n",
       " 88: 'drives',\n",
       " 89: 'india',\n",
       " 90: 'shiny',\n",
       " 91: 'wanted',\n",
       " 92: 'old',\n",
       " 93: 'march',\n",
       " 94: 'my',\n",
       " 95: 'apples',\n",
       " 96: 'least',\n",
       " 97: \"aren't\",\n",
       " 98: 'cold',\n",
       " 99: 'weather',\n",
       " 100: 'plan',\n",
       " 101: 'did',\n",
       " 102: 'been',\n",
       " 103: 'grapes',\n",
       " 104: 'easy',\n",
       " 105: 'lion',\n",
       " 106: 'grape',\n",
       " 107: 'new',\n",
       " 108: 'last',\n",
       " 109: 'in',\n",
       " 110: 'august',\n",
       " 111: 'elephant',\n",
       " 112: 'her',\n",
       " 113: 'cat',\n",
       " 114: 'disliked',\n",
       " 115: \"it's\",\n",
       " 116: 'does',\n",
       " 117: 'november',\n",
       " 118: 'green',\n",
       " 119: 'go',\n",
       " 120: 'red',\n",
       " 121: 'warm',\n",
       " 122: 'we',\n",
       " 123: 'france',\n",
       " 124: 'were',\n",
       " 125: 'black',\n",
       " 126: 'most',\n",
       " 127: 'summer',\n",
       " 128: 'when',\n",
       " 129: 'portuguese',\n",
       " 130: 'june',\n",
       " 131: 'pears',\n",
       " 132: 'their',\n",
       " 133: 'i',\n",
       " 134: 'between',\n",
       " 135: 'dislikes',\n",
       " 136: 'our',\n",
       " 137: 'field',\n",
       " 138: 'snowy',\n",
       " 139: 'pear',\n",
       " 140: 'lemon',\n",
       " 141: 'bear',\n",
       " 142: 'beautiful',\n",
       " 143: 'snakes',\n",
       " 144: 'may',\n",
       " 145: 'nice',\n",
       " 146: 'feared',\n",
       " 147: 'next',\n",
       " 148: 'automobile',\n",
       " 149: 'spanish',\n",
       " 150: '</S>',\n",
       " 151: 'but',\n",
       " 152: 'little',\n",
       " 153: 'rabbit',\n",
       " 154: 'driving',\n",
       " 155: 'store',\n",
       " 156: 'truck',\n",
       " 157: 'your',\n",
       " 158: 'animal',\n",
       " 159: 'december',\n",
       " 160: 'china',\n",
       " 161: 'sharks',\n",
       " 162: 'went',\n",
       " 163: 'rusty',\n",
       " 164: 'oranges',\n",
       " 165: 'strawberry',\n",
       " 166: 'french',\n",
       " 167: 'fun',\n",
       " 168: '.',\n",
       " 169: 'quiet',\n",
       " 170: 'white',\n",
       " 171: 'strawberries',\n",
       " 172: 'likes',\n",
       " 173: 'might',\n",
       " 174: 'january',\n",
       " 175: 'freezing',\n",
       " 176: 'like',\n",
       " 177: 'lime',\n",
       " 178: 'april',\n",
       " 179: 'horse',\n",
       " 180: 'why',\n",
       " 181: 'yellow',\n",
       " 182: 'football',\n",
       " 183: 'july',\n",
       " 184: 'she',\n",
       " 185: 'united',\n",
       " 186: '?',\n",
       " 187: 'lemons',\n",
       " 188: 'sometimes',\n",
       " 189: 'mangoes',\n",
       " 190: 'going',\n",
       " 191: \"didn't\",\n",
       " 192: 'a',\n",
       " 193: 'elephants',\n",
       " 194: 'chinese',\n",
       " 195: 'fall',\n",
       " 196: 'drove',\n",
       " 197: 'wants',\n",
       " 198: 'grapefruit',\n",
       " 0: '<MASK>'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<S>', 'vous', 'aimez', 'les', 'raisins', '.', '</S>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = get_word_list_from_sentence_string('vous aimez les raisins.', add_start_symbol=True, add_end_symbol=True,)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_indices(s, word_to_index):\n",
    "    \"\"\"Input s is a sentence string. word_to_index is a dict mapping words to indices.\n",
    "    \n",
    "    This function should convert a sentence to a list of indices, such as [5, 2, 17, 3], and return the list.\"\"\"\n",
    "    tmp = [i for i in s if i[0] not in  string.punctuation]\n",
    "    final = [word_to_index[i] for i in tmp]\n",
    "    return final \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def indices_to_sentence(indices, index_to_word):\n",
    "    \"\"\"indices is a list of word indices. word_to_index is a dict mapping indices to words.\n",
    "    \n",
    "    This function should convert the indices list, such as [5, 2, 17, 3], to a list of word strings, and \n",
    "    return the list.\"\"\"\n",
    "    final = [index_to_word[i] for i in indices]\n",
    "    return final \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[92, 28, 68, 202]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the functions.\n",
    "x = sentence_to_indices(get_word_list_from_sentence_string('vous aimez les raisins.', add_start_symbol=True, add_end_symbol=True,), word_to_index2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vous', 'aimez', 'les', 'raisins']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_to_sentence(x, index_to_word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 311)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record the number of words in the input and output data, respectively.\n",
    "num_words_X = len(w1) + 1  # add 1 to reserve 0 for mask\n",
    "num_words_y = len(w2) + 1  # add 1 to reserve 0 for mask\n",
    "num_words_X, num_words_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert the input sentences in s1 to a list of sentences each represented as a list of integers.\n",
    "# For example, the output list might look like [[5, 2, 17, 3], [1, 9, 85, 3, 22, 9], ...]\n",
    "# Do the same for the output sentences.\n",
    "inputs_as_indices = [sentence_to_indices(i, word_to_index1)  for i in s1]\n",
    "outputs_as_indices = [sentence_to_indices(i, word_to_index2)  for i in s2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Pad_sequence](https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0 178 ...  25  14 107]\n",
      " [117 109 175 ...  78 185  79]\n",
      " [  0   0 130 ...  31  25  46]\n",
      " ...\n",
      " [  0   0   0 ... 121  25 123]\n",
      " [  0   0 110 ...  25  14 107]\n",
      " [  0   0   0 ... 189 172  44]]\n"
     ]
    }
   ],
   "source": [
    "# Now pad the input and output index sequences with a filler (index 0) so that all sequences for each LSTM have the \n",
    "# same length. Use the keras function pad_sequences to do this easily.\n",
    "# Hint: For the inputs, padding should be on the left, like so: [[0, 0, 5, 2, 17, 3], ...]\n",
    "#       For the outputs, padding should be on the right, like so: [[9, 7, 5, 4, 0, 0, 0], ...]\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "inputs = pad_sequences(inputs_as_indices, padding='pre')\n",
    "outputs = pad_sequences(outputs_as_indices, padding='post')\n",
    "#outputs = pad_sequences(outputs_as_indices, padding='pre')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 19)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the maximum sequence length of the inputs and outputs, just to see how they look.\n",
    "max_seq_len_X = len(inputs[0])\n",
    "max_seq_len_y = len(outputs[0])\n",
    "max_seq_len_X, max_seq_len_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for convenience: define some more expressive variable names\n",
    "max_input_seq_len = max_seq_len_X\n",
    "max_output_seq_len = max_seq_len_y\n",
    "num_input_words = num_words_X\n",
    "num_output_words = num_words_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3499, 15), (1501, 15), (3499, 19), (1501, 19))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, \n",
    "                                                    test_size=1 - TRAIN_TEST_SPLIT,\n",
    "                                                    random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We need to make a one-hot-encoded version of the outputs ourselves for use in the loss function. \n",
    "# The inputs get this for free via use of Embedding layers in Keras.\n",
    "#\n",
    "# Hint: use the keras function to_categorical.\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now we need to write code to build the SeqToSeq model. **Important**: In Keras we have to use the \"functional API\" in order to access the LSTM internal state that we use as the \"context vector\" or \"encoding\" of a sentence. We also need to store hooks into the model to be able to run the translator on new sentences after training.\n",
    "\n",
    "This code will create variables representing the entire SeqToSeq model (for use in training), as well as the individual encoder segment and decoder segment of the model, for use in inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will implement the following architecture for the encoder section of the seq2se1 model:\n",
    "    \n",
    "1. Encoder input (encoder_inputs): Input layer, shape (max_seq_len_X,). For convenience, name the layer: name='encoder_input'\n",
    "2. Masking layer (encoder_masking): doesn't change shape. Ignores leading mask value (\"0\"s) in short sequences.\n",
    "3. Embedding layer (encoder_embedding): output shape (max_seq_len_X, EMBEDDING_DIM)\n",
    "4. LSTM layer: size is NUM_LSTM_NODES. uses dropout at rate given by DROPOUT.\n",
    "\n",
    "Hint: Be sure to set the \"return_sequences\" and \"return_state\" parameters appropriately in the LSTM for the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [How to Use Word Embedding Layers for Deep Learning with Keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nencoder_inputs = Input(shape=(None,max_output_seq_len), name='encoder_input')\\n\\nencoder_masking = Masking(mask_value=0., input_shape=(None,max_input_seq_len))\\n\\n# encoder_masking = Masking(mask_value=0., input_shape=encoder_inputs)\\n\\n#encoder_embedding = Embedding()\\nencoder = LSTM(NUM_LSTM_NODES,return_state=True, return_sequences=False, name='encoder_lstm_1')\\nencoder_outputs, state_h, state_c = encoder(encoder_inputs) # TODO\\n\\n# Discard `encoder_outputs` and only keep the states. We don't use the outputs in the encoder.\\n# Recall that the LSTM has two states we have to keep track of: c and h.\\nencoder_states = [state_h, state_c]\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build RNN model.\n",
    "'''\n",
    "encoder_inputs = Input(shape=(None,max_output_seq_len), name='encoder_input')\n",
    "\n",
    "encoder_masking = Masking(mask_value=0., input_shape=(None,max_input_seq_len))\n",
    "\n",
    "# encoder_masking = Masking(mask_value=0., input_shape=encoder_inputs)\n",
    "\n",
    "#encoder_embedding = Embedding()\n",
    "encoder = LSTM(NUM_LSTM_NODES,return_state=True, return_sequences=False, name='encoder_lstm_1')\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs) # TODO\n",
    "\n",
    "# Discard `encoder_outputs` and only keep the states. We don't use the outputs in the encoder.\n",
    "# Recall that the LSTM has two states we have to keep track of: c and h.\n",
    "encoder_states = [state_h, state_c]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(max_seq_len_X,), name='encoder_input')\n",
    "\n",
    "encoder_masking = Masking()(encoder_inputs) # TODO\n",
    "\n",
    "encoder_embedding = Embedding(input_dim=num_input_words + 1, output_dim=EMBEDDING_DIM)(encoder_masking) # TODO\n",
    "\n",
    "encoder_outputs, state_h, state_c = LSTM(units=NUM_LSTM_NODES,dropout=DROPOUT, name='encoder_lstm_1',return_sequences=False, return_state=True)(encoder_embedding) # TODO\n",
    "\n",
    "# Discard `encoder_outputs` and only keep the states. We don't use the outputs in the encoder.\n",
    "# Recall that the LSTM has two states we have to keep track of: c and h.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# The decoder should have the following architecture:\n",
    "    \n",
    "1. Decoder input (decoder_input): shape (None,)\n",
    "2. Masking layer (decoder_masking), as above.\n",
    "3. Embedding layer (decoder_embedding): output shape (max_seq_len_y, EMBEDDING_DIM)\n",
    "4. LSTM layer (decoder_lstm), as above. However, keep a function around to easy recreate the LSTM layer later on, during generation.\n",
    "6. Dense layer with softmax activation (decoder_output): output shape (num_output_words,)\n",
    "\n",
    "Hint: Be sure to set the \"return_sequences\" and \"return_state\" parameters appropriately in the LSTM for the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Decoder section\\n# Set up the decoder, using encoder_states as initial state.\\ndecoder_inputs = Input(shape=(None,max_output_seq_len), name=\\'decoder_input\\')\\n#decoder_inputs_masking = #TODO\\n#decoder_inputs_embedded = #TODO\\ndecoder_lstm = LSTM(return_sequences=True, return_state=True)  #TODO  # N.B. Just define an LSTM here, but don\\'t pass in the previous layer variable yet.\\n\\nz, _, _ = decoder_lstm(decoder_inputs_embedded)     # TODO: Pass in the context vector using the \"initial_state\" param\\n\\ndecoder_dense = Dense(max_output_seq_len, activation=\\'softmax\\') #TODO # Like LSTM above: define function for later use\\ndecoder_outputs = decoder_dense(z)\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Decoder section\n",
    "# Set up the decoder, using encoder_states as initial state.\n",
    "decoder_inputs = Input(shape=(None,max_output_seq_len), name='decoder_input')\n",
    "#decoder_inputs_masking = #TODO\n",
    "#decoder_inputs_embedded = #TODO\n",
    "decoder_lstm = LSTM(return_sequences=True, return_state=True)  #TODO  # N.B. Just define an LSTM here, but don't pass in the previous layer variable yet.\n",
    "\n",
    "z, _, _ = decoder_lstm(decoder_inputs_embedded)     # TODO: Pass in the context vector using the \"initial_state\" param\n",
    "\n",
    "decoder_dense = Dense(max_output_seq_len, activation='softmax') #TODO # Like LSTM above: define function for later use\n",
    "decoder_outputs = decoder_dense(z)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder section\n",
    "# Set up the decoder, using encoder_states as initial state.\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
    "decoder_inputs_masking = Masking()(decoder_inputs) #TODO\n",
    "decoder_inputs_embedded = Embedding(input_dim=num_output_words + 1, output_dim=EMBEDDING_DIM)(decoder_inputs_masking)#TODO\n",
    "decoder_lstm = LSTM(units=NUM_LSTM_NODES,dropout=DROPOUT, name='decoder_lstm_1',return_sequences=True, return_state=True)  #TODO  # N.B. Just define an LSTM here, but don't pass in the previous layer variable yet.\n",
    "\n",
    "z, _, _ = decoder_lstm(decoder_inputs_embedded, initial_state = encoder_states)     # TODO: Pass in the context vector using the \"initial_state\" param\n",
    "\n",
    "decoder_dense = Dense(num_output_words, activation=\"softmax\") #TODO # Like LSTM above: define function for later use\n",
    "decoder_outputs = decoder_dense(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Put it all together into one model, and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_1 (Masking)             (None, 15)           0           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None)         0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 15, 100)      20000       masking_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    31200       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm_1 (LSTM)           [(None, 256), (None, 365568      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_1 (LSTM)           [(None, None, 256),  365568      embedding_2[0][0]                \n",
      "                                                                 encoder_lstm_1[0][1]             \n",
      "                                                                 encoder_lstm_1[0][2]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 311)    79927       decoder_lstm_1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 862,263\n",
      "Trainable params: 862,263\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the complete seq2seq model.\n",
    "# This will take encoder_input_data & decoder_input_data as inputs and learn to output the decoder_target_data.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not  local_bool:\n",
    "    SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "scrolled": false
   },
   "source": [
    "## Prepare train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_input_data = X_train\n",
    "decoder_input_data = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# decoder_target_data will be ahead by one timestep\n",
    "# and will not include the start token.\n",
    "decoder_target_data = np.zeros(y_train_one_hot.shape)\n",
    "decoder_target_data[:,:-1] = y_train_one_hot[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decoder_target_data_test = np.zeros(y_test_one_hot.shape)\n",
    "decoder_target_data_test[:,:-1] = y_test_one_hot[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='auto', \n",
    "                                cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3499 samples, validate on 1501 samples\n",
      "Epoch 1/500\n",
      "3499/3499 [==============================] - 4s 1ms/step - loss: 3.3393 - val_loss: 2.5689\n",
      "Epoch 2/500\n",
      "3499/3499 [==============================] - 3s 735us/step - loss: 2.3463 - val_loss: 2.0644\n",
      "Epoch 3/500\n",
      "3499/3499 [==============================] - 3s 729us/step - loss: 1.7776 - val_loss: 1.4971\n",
      "Epoch 4/500\n",
      "3499/3499 [==============================] - 3s 734us/step - loss: 1.3413 - val_loss: 1.2002\n",
      "Epoch 5/500\n",
      "3499/3499 [==============================] - 3s 728us/step - loss: 1.1017 - val_loss: 1.0158\n",
      "Epoch 6/500\n",
      "3499/3499 [==============================] - 3s 734us/step - loss: 0.9529 - val_loss: 0.9024\n",
      "Epoch 7/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.8606 - val_loss: 0.8207\n",
      "Epoch 8/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.7907 - val_loss: 0.7647\n",
      "Epoch 9/500\n",
      "3499/3499 [==============================] - 3s 730us/step - loss: 0.7444 - val_loss: 0.7259\n",
      "Epoch 10/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.7059 - val_loss: 0.6976\n",
      "Epoch 11/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.6761 - val_loss: 0.6691\n",
      "Epoch 12/500\n",
      "3499/3499 [==============================] - 3s 729us/step - loss: 0.6521 - val_loss: 0.6454\n",
      "Epoch 13/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.6262 - val_loss: 0.6277\n",
      "Epoch 14/500\n",
      "3499/3499 [==============================] - 3s 733us/step - loss: 0.6064 - val_loss: 0.6068\n",
      "Epoch 15/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.5855 - val_loss: 0.5986\n",
      "Epoch 16/500\n",
      "3499/3499 [==============================] - 3s 730us/step - loss: 0.5646 - val_loss: 0.5735\n",
      "Epoch 17/500\n",
      "3499/3499 [==============================] - 3s 735us/step - loss: 0.5458 - val_loss: 0.5600\n",
      "Epoch 18/500\n",
      "3499/3499 [==============================] - 3s 730us/step - loss: 0.5269 - val_loss: 0.5352\n",
      "Epoch 19/500\n",
      "3499/3499 [==============================] - 3s 729us/step - loss: 0.5043 - val_loss: 0.5155\n",
      "Epoch 20/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.4837 - val_loss: 0.4978\n",
      "Epoch 21/500\n",
      "3499/3499 [==============================] - 3s 728us/step - loss: 0.4609 - val_loss: 0.4781\n",
      "Epoch 22/500\n",
      "3499/3499 [==============================] - 3s 735us/step - loss: 0.4402 - val_loss: 0.4593\n",
      "Epoch 23/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.4210 - val_loss: 0.4474\n",
      "Epoch 24/500\n",
      "3499/3499 [==============================] - 3s 733us/step - loss: 0.4018 - val_loss: 0.4227\n",
      "Epoch 25/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.3810 - val_loss: 0.4061\n",
      "Epoch 26/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.3662 - val_loss: 0.3880\n",
      "Epoch 27/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.3404 - val_loss: 0.3681\n",
      "Epoch 28/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.3204 - val_loss: 0.3474\n",
      "Epoch 29/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.2967 - val_loss: 0.3230\n",
      "Epoch 30/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.2747 - val_loss: 0.3050\n",
      "Epoch 31/500\n",
      "3499/3499 [==============================] - 3s 734us/step - loss: 0.2535 - val_loss: 0.2862\n",
      "Epoch 32/500\n",
      "3499/3499 [==============================] - 3s 734us/step - loss: 0.2332 - val_loss: 0.2632\n",
      "Epoch 33/500\n",
      "3499/3499 [==============================] - 3s 733us/step - loss: 0.2121 - val_loss: 0.2447\n",
      "Epoch 34/500\n",
      "3499/3499 [==============================] - 3s 730us/step - loss: 0.1962 - val_loss: 0.2262\n",
      "Epoch 35/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.1749 - val_loss: 0.2079\n",
      "Epoch 36/500\n",
      "3499/3499 [==============================] - 3s 729us/step - loss: 0.1569 - val_loss: 0.1918\n",
      "Epoch 37/500\n",
      "3499/3499 [==============================] - 3s 738us/step - loss: 0.1400 - val_loss: 0.1791\n",
      "Epoch 38/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.1278 - val_loss: 0.1705\n",
      "Epoch 39/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.1144 - val_loss: 0.1590\n",
      "Epoch 40/500\n",
      "3499/3499 [==============================] - 3s 727us/step - loss: 0.1024 - val_loss: 0.1480\n",
      "Epoch 41/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.0926 - val_loss: 0.1392\n",
      "Epoch 42/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.0860 - val_loss: 0.1375\n",
      "Epoch 43/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.0787 - val_loss: 0.1283\n",
      "Epoch 44/500\n",
      "3499/3499 [==============================] - 3s 730us/step - loss: 0.0699 - val_loss: 0.1226\n",
      "Epoch 45/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.0649 - val_loss: 0.1170\n",
      "Epoch 46/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0602 - val_loss: 0.1177\n",
      "Epoch 47/500\n",
      "3499/3499 [==============================] - 3s 733us/step - loss: 0.0554 - val_loss: 0.1156\n",
      "Epoch 48/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.0515 - val_loss: 0.1076\n",
      "Epoch 49/500\n",
      "3499/3499 [==============================] - 3s 728us/step - loss: 0.0472 - val_loss: 0.1064\n",
      "Epoch 50/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.0437 - val_loss: 0.1017\n",
      "Epoch 51/500\n",
      "3499/3499 [==============================] - 3s 729us/step - loss: 0.0404 - val_loss: 0.1016\n",
      "Epoch 52/500\n",
      "3499/3499 [==============================] - 3s 733us/step - loss: 0.0372 - val_loss: 0.0990\n",
      "Epoch 53/500\n",
      "3499/3499 [==============================] - 3s 734us/step - loss: 0.0353 - val_loss: 0.0977\n",
      "Epoch 54/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.0335 - val_loss: 0.0970\n",
      "Epoch 55/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.0329 - val_loss: 0.0976\n",
      "Epoch 56/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0297 - val_loss: 0.0961\n",
      "Epoch 57/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0273 - val_loss: 0.0937\n",
      "Epoch 58/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0258 - val_loss: 0.0942\n",
      "Epoch 59/500\n",
      "3499/3499 [==============================] - 3s 734us/step - loss: 0.0261 - val_loss: 0.0946\n",
      "Epoch 60/500\n",
      "3499/3499 [==============================] - 3s 727us/step - loss: 0.0236 - val_loss: 0.0912\n",
      "Epoch 61/500\n",
      "3499/3499 [==============================] - 3s 733us/step - loss: 0.0205 - val_loss: 0.0913\n",
      "Epoch 62/500\n",
      "3499/3499 [==============================] - 3s 729us/step - loss: 0.0192 - val_loss: 0.0894\n",
      "Epoch 63/500\n",
      "3499/3499 [==============================] - 3s 735us/step - loss: 0.0181 - val_loss: 0.0906\n",
      "Epoch 64/500\n",
      "3499/3499 [==============================] - 3s 731us/step - loss: 0.0169 - val_loss: 0.0902\n",
      "Epoch 65/500\n",
      "3499/3499 [==============================] - 3s 733us/step - loss: 0.0157 - val_loss: 0.0909\n",
      "Epoch 66/500\n",
      "3499/3499 [==============================] - 3s 735us/step - loss: 0.0152 - val_loss: 0.0893\n",
      "\n",
      "Epoch 00066: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 67/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0131 - val_loss: 0.0876\n",
      "Epoch 68/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0116 - val_loss: 0.0862\n",
      "Epoch 69/500\n",
      "3499/3499 [==============================] - 3s 730us/step - loss: 0.0112 - val_loss: 0.0869\n",
      "Epoch 70/500\n",
      "3499/3499 [==============================] - 3s 734us/step - loss: 0.0108 - val_loss: 0.0869\n",
      "Epoch 71/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0107 - val_loss: 0.0866\n",
      "Epoch 72/500\n",
      "3499/3499 [==============================] - 3s 733us/step - loss: 0.0102 - val_loss: 0.0859\n",
      "Epoch 73/500\n",
      "3499/3499 [==============================] - 3s 734us/step - loss: 0.0099 - val_loss: 0.0859\n",
      "Epoch 74/500\n",
      "3499/3499 [==============================] - 3s 734us/step - loss: 0.0097 - val_loss: 0.0875\n",
      "Epoch 75/500\n",
      "3499/3499 [==============================] - 3s 735us/step - loss: 0.0098 - val_loss: 0.0857\n",
      "Epoch 76/500\n",
      "3499/3499 [==============================] - 3s 734us/step - loss: 0.0090 - val_loss: 0.0866\n",
      "Epoch 77/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0089 - val_loss: 0.0869\n",
      "Epoch 78/500\n",
      "3499/3499 [==============================] - 3s 737us/step - loss: 0.0084 - val_loss: 0.0862\n",
      "Epoch 79/500\n",
      "3499/3499 [==============================] - 3s 738us/step - loss: 0.0090 - val_loss: 0.0866\n",
      "\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 80/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0077 - val_loss: 0.0853\n",
      "Epoch 81/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0073 - val_loss: 0.0853\n",
      "Epoch 82/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0071 - val_loss: 0.0859\n",
      "Epoch 83/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0070 - val_loss: 0.0859\n",
      "Epoch 84/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0070 - val_loss: 0.0865\n",
      "\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 85/500\n",
      "3499/3499 [==============================] - 3s 730us/step - loss: 0.0066 - val_loss: 0.0858\n",
      "Epoch 86/500\n",
      "3499/3499 [==============================] - 3s 729us/step - loss: 0.0065 - val_loss: 0.0860\n",
      "Epoch 87/500\n",
      "3499/3499 [==============================] - 3s 733us/step - loss: 0.0065 - val_loss: 0.0859\n",
      "Epoch 88/500\n",
      "3499/3499 [==============================] - 3s 732us/step - loss: 0.0064 - val_loss: 0.0857\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 89/500\n",
      "3499/3499 [==============================] - 3s 729us/step - loss: 0.0063 - val_loss: 0.0857\n",
      "Epoch 90/500\n",
      "3499/3499 [==============================] - 3s 734us/step - loss: 0.0063 - val_loss: 0.0858\n",
      "Epoch 00090: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d2531e19b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data=([X_test, y_test], decoder_target_data_test),\n",
    "          callbacks=[lr_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We have trained a model, but how do we use it to actually translate sentences? We have to do more work ourselves here than with a non-recurrent neural net, so we'll write a function to help out. Here are the steps:\n",
    "\n",
    "1. **Encode**:\n",
    "    1. Run the entire input sentence through the encoder part of the model.\n",
    "    1. Write down the \"context vector\" -- this is the state of the last LSTM encoder layer.<br><br>\n",
    "\n",
    "2. **Decode in a loop**:\n",
    "    1. Seed the decoder LSTM with the context vector.\n",
    "    1. Run a *single step* of the decoder with the input \"`<S>`\" (the start symbol).\n",
    "    1. Store the output. This is a word of the translation!\n",
    "    1. Return to step 2B, but feed in the word from step 2C as the new input. Repeat until the decoder returns \"`</S>`\" (the end symbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "masking_1 (Masking)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 15, 100)           20000     \n",
      "_________________________________________________________________\n",
      "encoder_lstm_1 (LSTM)        [(None, 256), (None, 256) 365568    \n",
      "=================================================================\n",
      "Total params: 385,568\n",
      "Trainable params: 385,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a version of our model for use in sampling (as opposed to training).\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not local_bool:\n",
    "    SVG(model_to_dot(encoder_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_2 (Masking)             (None, None)         0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 100)    31200       masking_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_1 (LSTM)           [(None, None, 256),  365568      embedding_2[0][0]                \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 311)    79927       decoder_lstm_1[1][0]             \n",
      "==================================================================================================\n",
      "Total params: 476,695\n",
      "Trainable params: 476,695\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the decoder.\n",
    "decoder_state_input_h = Input(shape=(NUM_LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(NUM_LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs_embedded, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if not local_bool:\n",
    "    SVG(model_to_dot(decoder_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def translate_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    # TODO: Use the encoder_model to get the h and c vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first word of target sequence with the start symbol '<S>'.\n",
    "    target_seq[0, 0] = word_to_index2['<S>']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    decoded_sentence = ''\n",
    "    stop_condition = False\n",
    "    step = 0\n",
    "    # TODO: complete the loop.\n",
    "    while not stop_condition:\n",
    "        # Use the decoder to get the output token vector and the h and c vectors\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value) \n",
    "\n",
    "        # Find the largest value in the probability output vector, and use that index as your output word\n",
    "        # at this time step.\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = index_to_word2[sampled_token_index]\n",
    "\n",
    "        # Add the word to the output sentence string\n",
    "        #if sampled_word != '<MASK>':\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "        \n",
    "        # Stopping condition: either hit max length or find the stop token '</S>'.\n",
    "        if (sampled_word == '</S>' or\n",
    "           len(decoded_sentence) > 100):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Test your network: feed in 10 sentences and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-\n",
      "Input sentence:\n",
      " the united states is pleasant during december and it is rainy in winter \n",
      "Decoded sentence:\n",
      "  jersey est agrã©able en dã©cembre et il pleut en hiver\n",
      "Translation:\n",
      " les ã©tats unis est agrã©able en dã©cembre et il pleut en hiver \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " she likes apples limes and oranges \n",
      "Decoded sentence:\n",
      "  aime les pommes les citrons verts et oranges\n",
      "Translation:\n",
      " elle aime les pommes les citrons verts et oranges \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " france is usually freezing during november and it is never rainy in may \n",
      "Decoded sentence:\n",
      "  jersey est le gel habituellement au mois de novembre et il est jamais pluvieux en mai\n",
      "Translation:\n",
      " la france est le gel habituellement au mois de novembre et il est jamais pluvieux en mai \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " china is never quiet during spring but it is sometimes dry in september \n",
      "Decoded sentence:\n",
      "  est jamais tranquille au printemps mais il est parfois sã¨che en septembre\n",
      "Translation:\n",
      " chine est jamais tranquille au printemps mais il est parfois sã¨che en septembre \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " china is warm during spring and it is sometimes cold in february \n",
      "Decoded sentence:\n",
      "  est chaud au printemps et il est parfois froid en fã©vrier\n",
      "Translation:\n",
      " chine est chaud au printemps et il est parfois froid en fã©vrier \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " india is usually chilly during october but it is usually rainy in summer \n",
      "Decoded sentence:\n",
      "  jersey est gã©nã©ralement froid en octobre mais il est gã©nã©ralement pluvieux en ã©tã©\n",
      "Translation:\n",
      " l' inde est gã©nã©ralement froid en octobre mais il est gã©nã©ralement pluvieux en ã©tã© \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " she likes oranges strawberries and lemons \n",
      "Decoded sentence:\n",
      "  aime les oranges les fraises et les citrons\n",
      "Translation:\n",
      " elle aime les oranges les fraises et les citrons \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " california is usually wonderful during spring and it is sometimes pleasant in february \n",
      "Decoded sentence:\n",
      "  est gã©nã©ralement merveilleux au printemps et il est parfois agrã©able en fã©vrier\n",
      "Translation:\n",
      " californie est gã©nã©ralement merveilleux au printemps et il est parfois agrã©able en fã©vrier \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " their least favorite fruit is the lime but my least favorite is the peach \n",
      "Decoded sentence:\n",
      "  fruit prã©fã©rã© est moins la chaux mais mon prã©fã©rã© moins est la pãªche\n",
      "Translation:\n",
      " leur fruit prã©fã©rã© est moins la chaux mais mon prã©fã©rã© moins est la pãªche \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " california is nice during october but it is freezing in september \n",
      "Decoded sentence:\n",
      "  est agrã©able en octobre mais il gã¨le en septembre\n",
      "Translation:\n",
      " californie est agrã©able en octobre mais il gã¨le en septembre \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " the lime is our least favorite fruit but the grape is their least favorite \n",
      "Decoded sentence:\n",
      "  fruit prã©fã©rã© est moins le raisin mais notre moins prã©fã©rã© est la banane\n",
      "Translation:\n",
      " la chaux est notre fruit prã©fã©rã© moins mais le raisin est leur moins prã©fã©rã© \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " i like limes apples and mangoes \n",
      "Decoded sentence:\n",
      "  citrons verts les pommes et les mangues\n",
      "Translation:\n",
      " j'aime citrons verts les pommes et les mangues \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " the united states is snowy during july and it is sometimes wonderful in august \n",
      "Decoded sentence:\n",
      "  jersey est relaxant en juillet et il est parfois merveilleux en aoã»t\n",
      "Translation:\n",
      " les ã©tats unis est neigeux en juillet et il est parfois merveilleux en aoã»t \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " paris is wet during october but it is sometimes relaxing in summer \n",
      "Decoded sentence:\n",
      "  est mouillã© au mois d' octobre mais il est relaxant parfois en ã©tã©\n",
      "Translation:\n",
      " paris est mouillã© au mois d' octobre mais il est relaxant parfois en ã©tã© \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " india is rainy during winter and it is sometimes warm in january \n",
      "Decoded sentence:\n",
      "  pluvieux est pluvieux pendant l' hiver et il est parfois chaud en janvier\n",
      "Translation:\n",
      " l' inde est pluvieux pendant l' hiver et il est parfois chaud en janvier \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " she disliked that old red automobile \n",
      "Decoded sentence:\n",
      "  dã©testait cette vieille voiture rouge\n",
      "Translation:\n",
      " elle dã©testait cette vieille voiture rouge \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " california is snowy during june and it is usually pleasant in july \n",
      "Decoded sentence:\n",
      "  est enneigã©e en juin et il est gã©nã©ralement agrã©able en juillet\n",
      "Translation:\n",
      " californie est enneigã©e en juin et il est gã©nã©ralement agrã©able en juillet \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " china is wet during spring but it is never chilly in winter \n",
      "Decoded sentence:\n",
      "  est humide au printemps mais il est jamais froid en hiver\n",
      "Translation:\n",
      " chine est humide au printemps mais il est jamais froid en hiver \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " france is never nice during winter and it is sometimes dry in january \n",
      "Decoded sentence:\n",
      "  jersey est jamais agrã©able pendant l' hiver et il est parfois sã¨che en janvier\n",
      "Translation:\n",
      " la france est jamais agrã©able pendant l' hiver et il est parfois sã¨che en janvier \n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " he dislikes bananas peaches and grapefruit \n",
      "Decoded sentence:\n",
      "  dã©teste les bananes les pãªches et le pamplemousse\n",
      "Translation:\n",
      " il dã©teste les bananes les pãªches et le pamplemousse \n"
     ]
    }
   ],
   "source": [
    "def trans(lst, bool=True):\n",
    "    if not bool:\n",
    "        lst = lst[::-1]\n",
    "    tmp = [i for i in lst if i != '<MASK>']\n",
    "    return ('{} ' * len(tmp)).format(*tmp)\n",
    "\n",
    "for i in range(20):\n",
    "    # Print an input sentence\n",
    "    # Translate it and print the output sentence\n",
    "    \n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[i: i + 1]\n",
    "    #print(input_seq)\n",
    "    output_seq = decoder_input_data[i: i + 1]\n",
    "    #print(output_seq)\n",
    "    \n",
    "    #print(translate_sequence(input_seq))\n",
    "    decoded_sentence = translate_sequence(input_seq)[:translate_sequence(input_seq).find('<') -1]\n",
    "    print('\\n-')\n",
    "    print('Input sentence:\\n', trans(indices_to_sentence(input_seq[0], index_to_word1),bool=False))\n",
    "    print('Decoded sentence:\\n', decoded_sentence)\n",
    "    print('Translation:\\n', trans(indices_to_sentence(output_seq[0], index_to_word2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# copying the file for home usage\\nPATH = \\'../resource/asnlib/publicdata/\\'\\nLOCAL_PATH = \\'./\\'\\n\\nfr =  \\'small_vocab_fr.txt\\'\\nen = \\'small_vocab_en.txt\\'\\n\\nfiles = [[PATH + fr , LOCAL_PATH + fr], [PATH + en , LOCAL_PATH + en]]\\n\\nfor i in files:\\n    with open(i[0],\"r\") as f:\\n        with open(i[1], \"w\") as copy:\\n            for line in f:\\n                copy.write(line)\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# copying the file for home usage\n",
    "PATH = '../resource/asnlib/publicdata/'\n",
    "LOCAL_PATH = './'\n",
    "\n",
    "fr =  'small_vocab_fr.txt'\n",
    "en = 'small_vocab_en.txt'\n",
    "\n",
    "files = [[PATH + fr , LOCAL_PATH + fr], [PATH + en , LOCAL_PATH + en]]\n",
    "\n",
    "for i in files:\n",
    "    with open(i[0],\"r\") as f:\n",
    "        with open(i[1], \"w\") as copy:\n",
    "            for line in f:\n",
    "                copy.write(line)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Question: How well do you think the model did? Discuss any problems you ran into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the model is not perfect, I have an issue with the first word but beside the translation is somehow in the ballpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Evaluate model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Compute the accuracy of the model on the test set.\n",
    "In a detailed study we would calculate the \"BLEU\" score for the translation task.\n",
    "For this assignment, we'll keep things simple. Just calculate an all-or-nothing accuracy score on each translated sentence. If all the words appear in the output, in the correct order, without extra words, the score on that example is 1. Otherwise 0. Compute the accuracy over all examples in the test set. You may ignore punctuation (commas) and `<S>` and `</S>` symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def score():\n",
    "    i = 0\n",
    "    for i in range(encoder_input_data.shape[0]):\n",
    "        # Print an input sentence\n",
    "        # Translate it and print the output sentence\n",
    "\n",
    "        # Take one sequence (part of the training set)\n",
    "        # for trying out decoding.\n",
    "        input_seq = encoder_input_data[i: i + 1]\n",
    "        #print(input_seq)\n",
    "        output_seq = decoder_input_data[i: i + 1]\n",
    "        #print(output_seq)\n",
    "\n",
    "        #print(translate_sequence(input_seq))\n",
    "        decoded_sentence = translate_sequence(input_seq)[:translate_sequence(input_seq).find('<') -1]\n",
    "        if decoded_sentence == trans(indices_to_sentence(output_seq[0], index_to_word2)):\n",
    "            i+=1\n",
    "        return round(i * 100 / encoder_input_data.shape[0], 2)\n",
    "        \n",
    "print(score())      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Report the accuracy value you obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have obtained 0.0 as I have an issue with the first word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
