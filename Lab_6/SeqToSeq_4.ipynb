{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Sequence-to-Sequence: Language Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "In this assignment you will use a database of pairs of (English,French) sentences to train an RNN model to translate from English to French.\n",
    "\n",
    "The directory ../resource/asnlib/publicdata contains two files, \"small_vocab_en.txt\" and \"small_vocab_fr.txt\". Line \"n\" of the first file corresponds to line \"n\" of the second file.\n",
    "\n",
    "Also see data here: http://www.statmt.org/wmt14/translation-task.html\n",
    "\n",
    "Keras resources: \n",
    "* https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "* https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/\n",
    "* https://stackoverflow.com/questions/38714959/understanding-keras-lstms/50235563#50235563\n",
    "\n",
    "Neural Language Translation Resources:\n",
    "* https://arxiv.org/abs/1703.01619\n",
    "* https://www.tensorflow.org/tutorials/seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Example Keras github](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py)\n",
    "- [Language translation toward data science](https://towardsdatascience.com/language-translation-with-rnns-d84d43b40571)\n",
    "- [How to Develop an Encoder-Decoder Model for Sequence-to-Sequence Prediction in Keras](https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.layers import Input, GRU, LSTM, Dense, Masking, Dropout, Embedding, Flatten, Dense\n",
    "from keras.models import Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Configure Tensorflow to be less aggressive about RAM utilization when it starts up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.1  # Start with 10% of the GPU RAM\n",
    "config.gpu_options.allow_growth = True                    # Dynamically grow the memory used on the GPU\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)                                         # Set this TensorFlow session as the default session for Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TRAIN_TEST_SPLIT = 0.7           # % of data in training set\n",
    "\n",
    "NUM_LSTM_NODES = 256             # Num of intermediate LSTM nodes\n",
    "CONTEXT_VECTOR_SIZE = 256        # Size of context vector (num of LSTM nodes in final LSTM layer)\n",
    "\n",
    "EMBEDDING_DIM = 100              # Embedding layer size for input words\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 500\n",
    "\n",
    "NUM_DATA_EXAMPLES = 5000         # limit memory usage while experimenting\n",
    "\n",
    "LR = 0.01\n",
    "DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Text Preprocessing\n",
    "These are provided so you can focus on the neural net modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A useful string full of characters to remove\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def add_space_around_punctuation(s):\n",
    "    result = ''\n",
    "    for c in s:\n",
    "        if c in string.punctuation and c != \"'\":  # Apostrophes are important\n",
    "            result += ' %s ' % c\n",
    "        else:\n",
    "            result += c\n",
    "    return result\n",
    "\n",
    "def clean_sentence(s):\n",
    "    s = s.strip()\n",
    "    s = s.lower()\n",
    "    s = add_space_around_punctuation(s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Functions to get words from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_words_from_sentence(s, add_start_symbol=False, add_end_symbol=False, reverse=False):\n",
    "    words = list(filter(None, s.split(' ')))\n",
    "    if reverse:\n",
    "        words = words[::-1]\n",
    "    if add_start_symbol:\n",
    "        words = ['<S>'] + words\n",
    "    if add_end_symbol:\n",
    "        words.append('</S>')\n",
    "    return words\n",
    "\n",
    "def get_word_list_from_sentence_string(s, add_start_symbol=False, add_end_symbol=False, reverse=False):\n",
    "    return get_words_from_sentence(clean_sentence(s), add_start_symbol, add_end_symbol, reverse)    \n",
    "    \n",
    "def get_sentences(path, filename, add_start_symbol=False, add_end_symbol=False, reverse=False):\n",
    "    with open(os.path.join(path, filename), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        return [get_word_list_from_sentence_string(s, add_start_symbol, add_end_symbol, reverse) \n",
    "                for s in lines]\n",
    "\n",
    "def get_word_set(sentences):\n",
    "    words = set()\n",
    "    for s in sentences:\n",
    "        for word in s:\n",
    "            words.add(word)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Read the data and build useful data structures, such as a list of sentences and lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "26\n"
     ]
    }
   ],
   "source": [
    "# Store the input sentences (English) in s1\n",
    "# Store the target senteces (French) in s2\n",
    "\n",
    "# Consider reversing the input sentences to improve trianing.\n",
    "# Add start and stop symbols for the decoder.\n",
    "PATH = '../resource/asnlib/publicdata'\n",
    "s1 = get_sentences(PATH, 'small_vocab_en.txt', add_start_symbol=True, add_end_symbol=True)       # TODO\n",
    "s1 = [lst[::-1] for lst in s1]\n",
    "s2 = get_sentences(PATH, 'small_vocab_fr.txt', add_start_symbol=True, add_end_symbol=True)       # TODO\n",
    "max_encoder_seq_length = max([len(txt) for txt in s1])\n",
    "max_decoder_seq_length = max([len(txt) for txt in s2])\n",
    "print(max_encoder_seq_length )\n",
    "print(max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137860\n",
      "137860\n"
     ]
    }
   ],
   "source": [
    "# Restruct to a subset of the data\n",
    "print(len(s1))\n",
    "print(len(s2))\n",
    "s1 = s1[:NUM_DATA_EXAMPLES]\n",
    "s2 = s2[:NUM_DATA_EXAMPLES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('</S> . april in snowy is it and , autumn during quiet sometimes is jersey new <S>',\n",
       " \"<S> new jersey est parfois calme pendant l' automne , et il est neigeux en avril . </S>\")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show a sample sentence pair.\n",
    "' '.join(s1[0]), ' '.join(s2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_s(s):\n",
    "    func = lambda l: [item for sublist in l for item in sublist]\n",
    "    tmp = list(set(func(s)))\n",
    "    #final = [i for i in tmp if i[0] not in  string.punctuation]\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create two lists, w1 and w2, which hold the set of all words that show up in s1 and s2.\n",
    "\n",
    "w1 = split_s(s1) \n",
    "w2 = split_s(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert('<S>' in w1)\n",
    "assert('</S>' in w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Utilities for mapping words to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_word_to_index_dict(words):\n",
    "    return {w: i+1 for i,w in enumerate(words)}  # use i+1 to reserve 0 for the mask index\n",
    "def reverse_dict(d):\n",
    "    return {v: k for k,v in d.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "word_to_index1 = get_word_to_index_dict(w1)\n",
    "word_to_index2 = get_word_to_index_dict(w2)\n",
    "index_to_word1 = reverse_dict(word_to_index1)\n",
    "index_to_word2 = reverse_dict(word_to_index2)\n",
    "index_to_word1[0] = '<MASK>'\n",
    "index_to_word2[0] = '<MASK>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<S>', 'vous', 'aimez', 'les', 'raisins', '.', '</S>']"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = get_word_list_from_sentence_string('vous aimez les raisins.', add_start_symbol=True, add_end_symbol=True,)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sentence_to_indices(s, word_to_index):\n",
    "    \"\"\"Input s is a sentence string. word_to_index is a dict mapping words to indices.\n",
    "    \n",
    "    This function should convert a sentence to a list of indices, such as [5, 2, 17, 3], and return the list.\"\"\"\n",
    "    tmp = [i for i in s if i[0] not in  string.punctuation]\n",
    "    final = [word_to_index[i] for i in tmp]\n",
    "    return final \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def indices_to_sentence(indices, index_to_word):\n",
    "    \"\"\"indices is a list of word indices. word_to_index is a dict mapping indices to words.\n",
    "    \n",
    "    This function should convert the indices list, such as [5, 2, 17, 3], to a list of word strings, and \n",
    "    return the list.\"\"\"\n",
    "    final = [index_to_word[i] for i in indices]\n",
    "    return final \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[235, 167, 234, 9]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the functions.\n",
    "x = sentence_to_indices(get_word_list_from_sentence_string('vous aimez les raisins.', add_start_symbol=True, add_end_symbol=True,), word_to_index2)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vous', 'aimez', 'les', 'raisins']"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_to_sentence(x, index_to_word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199, 310)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Record the number of words in the input and output data, respectively.\n",
    "num_words_X = len(w1) + 1  # add 1 to reserve 0 for mask\n",
    "num_words_y = len(w2) + 1  # add 1 to reserve 0 for mask\n",
    "num_words_X, num_words_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert the input sentences in s1 to a list of sentences each represented as a list of integers.\n",
    "# For example, the output list might look like [[5, 2, 17, 3], [1, 9, 85, 3, 22, 9], ...]\n",
    "# Do the same for the output sentences.\n",
    "inputs_as_indices = [sentence_to_indices(i, word_to_index1)  for i in s1]\n",
    "outputs_as_indices = [sentence_to_indices(i, word_to_index2)  for i in s2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Pad_sequence](https://machinelearningmastery.com/data-preparation-variable-length-input-sequences-sequence-prediction/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0 179 ...  91  85 193]\n",
      " [156  68   7 ... 168  79 123]\n",
      " [  0   0  84 ... 196  91 107]\n",
      " ...\n",
      " [  0   0   0 ... 160  91 190]\n",
      " [  0   0 186 ...  91  85 193]\n",
      " [  0   0   0 ...   6  61 151]]\n"
     ]
    }
   ],
   "source": [
    "# Now pad the input and output index sequences with a filler (index 0) so that all sequences for each LSTM have the \n",
    "# same length. Use the keras function pad_sequences to do this easily.\n",
    "# Hint: For the inputs, padding should be on the left, like so: [[0, 0, 5, 2, 17, 3], ...]\n",
    "#       For the outputs, padding should be on the right, like so: [[9, 7, 5, 4, 0, 0, 0], ...]\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "inputs = pad_sequences(inputs_as_indices, padding='pre')\n",
    "outputs = pad_sequences(outputs_as_indices, padding='post')\n",
    "#outputs = pad_sequences(outputs_as_indices, padding='pre')\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15, 19)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the maximum sequence length of the inputs and outputs, just to see how they look.\n",
    "max_seq_len_X = len(inputs[0])\n",
    "max_seq_len_y = len(outputs[0])\n",
    "max_seq_len_X, max_seq_len_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just for convenience: define some more expressive variable names\n",
    "max_input_seq_len = max_seq_len_X\n",
    "max_output_seq_len = max_seq_len_y\n",
    "num_input_words = num_words_X\n",
    "num_output_words = num_words_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3499, 15), (1501, 15), (3499, 19), (1501, 19))"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create train and test sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, \n",
    "                                                    test_size=1 - TRAIN_TEST_SPLIT,\n",
    "                                                    random_state=42)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We need to make a one-hot-encoded version of the outputs ourselves for use in the loss function. \n",
    "# The inputs get this for free via use of Embedding layers in Keras.\n",
    "#\n",
    "# Hint: use the keras function to_categorical.\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now we need to write code to build the SeqToSeq model. **Important**: In Keras we have to use the \"functional API\" in order to access the LSTM internal state that we use as the \"context vector\" or \"encoding\" of a sentence. We also need to store hooks into the model to be able to run the translator on new sentences after training.\n",
    "\n",
    "This code will create variables representing the entire SeqToSeq model (for use in training), as well as the individual encoder segment and decoder segment of the model, for use in inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We will implement the following architecture for the encoder section of the seq2se1 model:\n",
    "    \n",
    "1. Encoder input (encoder_inputs): Input layer, shape (max_seq_len_X,). For convenience, name the layer: name='encoder_input'\n",
    "2. Masking layer (encoder_masking): doesn't change shape. Ignores leading mask value (\"0\"s) in short sequences.\n",
    "3. Embedding layer (encoder_embedding): output shape (max_seq_len_X, EMBEDDING_DIM)\n",
    "4. LSTM layer: size is NUM_LSTM_NODES. uses dropout at rate given by DROPOUT.\n",
    "\n",
    "Hint: Be sure to set the \"return_sequences\" and \"return_state\" parameters appropriately in the LSTM for the encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [How to Use Word Embedding Layers for Deep Learning with Keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nencoder_inputs = Input(shape=(None,max_output_seq_len), name='encoder_input')\\n\\nencoder_masking = Masking(mask_value=0., input_shape=(None,max_input_seq_len))\\n\\n# encoder_masking = Masking(mask_value=0., input_shape=encoder_inputs)\\n\\n#encoder_embedding = Embedding()\\nencoder = LSTM(NUM_LSTM_NODES,return_state=True, return_sequences=False, name='encoder_lstm_1')\\nencoder_outputs, state_h, state_c = encoder(encoder_inputs) # TODO\\n\\n# Discard `encoder_outputs` and only keep the states. We don't use the outputs in the encoder.\\n# Recall that the LSTM has two states we have to keep track of: c and h.\\nencoder_states = [state_h, state_c]\\n\""
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build RNN model.\n",
    "'''\n",
    "encoder_inputs = Input(shape=(None,max_output_seq_len), name='encoder_input')\n",
    "\n",
    "encoder_masking = Masking(mask_value=0., input_shape=(None,max_input_seq_len))\n",
    "\n",
    "# encoder_masking = Masking(mask_value=0., input_shape=encoder_inputs)\n",
    "\n",
    "#encoder_embedding = Embedding()\n",
    "encoder = LSTM(NUM_LSTM_NODES,return_state=True, return_sequences=False, name='encoder_lstm_1')\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs) # TODO\n",
    "\n",
    "# Discard `encoder_outputs` and only keep the states. We don't use the outputs in the encoder.\n",
    "# Recall that the LSTM has two states we have to keep track of: c and h.\n",
    "encoder_states = [state_h, state_c]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(max_seq_len_X,), name='encoder_input')\n",
    "\n",
    "encoder_masking = Masking()(encoder_inputs) # TODO\n",
    "\n",
    "encoder_embedding = Embedding(input_dim=num_input_words + 1, output_dim=EMBEDDING_DIM)(encoder_masking) # TODO\n",
    "\n",
    "encoder_outputs, state_h, state_c = LSTM(units=NUM_LSTM_NODES,dropout=DROPOUT, name='encoder_lstm_1',return_sequences=False, return_state=True)(encoder_embedding) # TODO\n",
    "\n",
    "# Discard `encoder_outputs` and only keep the states. We don't use the outputs in the encoder.\n",
    "# Recall that the LSTM has two states we have to keep track of: c and h.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# The decoder should have the following architecture:\n",
    "    \n",
    "1. Decoder input (decoder_input): shape (None,)\n",
    "2. Masking layer (decoder_masking), as above.\n",
    "3. Embedding layer (decoder_embedding): output shape (max_seq_len_y, EMBEDDING_DIM)\n",
    "4. LSTM layer (decoder_lstm), as above. However, keep a function around to easy recreate the LSTM layer later on, during generation.\n",
    "6. Dense layer with softmax activation (decoder_output): output shape (num_output_words,)\n",
    "\n",
    "Hint: Be sure to set the \"return_sequences\" and \"return_state\" parameters appropriately in the LSTM for the encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Decoder section\\n# Set up the decoder, using encoder_states as initial state.\\ndecoder_inputs = Input(shape=(None,max_output_seq_len), name=\\'decoder_input\\')\\n#decoder_inputs_masking = #TODO\\n#decoder_inputs_embedded = #TODO\\ndecoder_lstm = LSTM(return_sequences=True, return_state=True)  #TODO  # N.B. Just define an LSTM here, but don\\'t pass in the previous layer variable yet.\\n\\nz, _, _ = decoder_lstm(decoder_inputs_embedded)     # TODO: Pass in the context vector using the \"initial_state\" param\\n\\ndecoder_dense = Dense(max_output_seq_len, activation=\\'softmax\\') #TODO # Like LSTM above: define function for later use\\ndecoder_outputs = decoder_dense(z)\\n'"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Decoder section\n",
    "# Set up the decoder, using encoder_states as initial state.\n",
    "decoder_inputs = Input(shape=(None,max_output_seq_len), name='decoder_input')\n",
    "#decoder_inputs_masking = #TODO\n",
    "#decoder_inputs_embedded = #TODO\n",
    "decoder_lstm = LSTM(return_sequences=True, return_state=True)  #TODO  # N.B. Just define an LSTM here, but don't pass in the previous layer variable yet.\n",
    "\n",
    "z, _, _ = decoder_lstm(decoder_inputs_embedded)     # TODO: Pass in the context vector using the \"initial_state\" param\n",
    "\n",
    "decoder_dense = Dense(max_output_seq_len, activation='softmax') #TODO # Like LSTM above: define function for later use\n",
    "decoder_outputs = decoder_dense(z)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decoder section\n",
    "# Set up the decoder, using encoder_states as initial state.\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_input')\n",
    "decoder_inputs_masking = Masking()(decoder_inputs) #TODO\n",
    "decoder_inputs_embedded = Embedding(input_dim=num_output_words + 1, output_dim=EMBEDDING_DIM)(decoder_inputs_masking)#TODO\n",
    "decoder_lstm = LSTM(units=NUM_LSTM_NODES,dropout=DROPOUT, name='decoder_lstm_1',return_sequences=True, return_state=True)  #TODO  # N.B. Just define an LSTM here, but don't pass in the previous layer variable yet.\n",
    "\n",
    "z, _, _ = decoder_lstm(decoder_inputs_embedded, initial_state = encoder_states)     # TODO: Pass in the context vector using the \"initial_state\" param\n",
    "\n",
    "decoder_dense = Dense(num_output_words, activation=\"softmax\") #TODO # Like LSTM above: define function for later use\n",
    "decoder_outputs = decoder_dense(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Put it all together into one model, and visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 15)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_5 (Masking)             (None, 15)           0           encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "masking_6 (Masking)             (None, None)         0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 15, 100)      20000       masking_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 100)    31100       masking_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm_1 (LSTM)           [(None, 256), (None, 365568      embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_1 (LSTM)           [(None, None, 256),  365568      embedding_6[0][0]                \n",
      "                                                                 encoder_lstm_1[0][1]             \n",
      "                                                                 encoder_lstm_1[0][2]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 310)    79670       decoder_lstm_1[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 861,906\n",
      "Trainable params: 861,906\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the complete seq2seq model.\n",
    "# This will take encoder_input_data & decoder_input_data as inputs and learn to output the decoder_target_data.\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"470pt\" viewBox=\"0.00 0.00 803.50 470.00\" width=\"804pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 466)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-466 799.5,-466 799.5,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140476272141592 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140476272141592</title>\n",
       "<polygon fill=\"none\" points=\"75,-415.5 75,-461.5 367,-461.5 367,-415.5 75,-415.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-434.8\">encoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"236,-415.5 236,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-446.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"236,-438.5 291,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-423.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"291,-415.5 291,-461.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-446.3\">(None, 15)</text>\n",
       "<polyline fill=\"none\" points=\"291,-438.5 367,-438.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-423.3\">(None, 15)</text>\n",
       "</g>\n",
       "<!-- 140476272141424 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140476272141424</title>\n",
       "<polygon fill=\"none\" points=\"89.5,-332.5 89.5,-378.5 352.5,-378.5 352.5,-332.5 89.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-351.8\">masking_5: Masking</text>\n",
       "<polyline fill=\"none\" points=\"221.5,-332.5 221.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"221.5,-355.5 276.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"276.5,-332.5 276.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314.5\" y=\"-363.3\">(None, 15)</text>\n",
       "<polyline fill=\"none\" points=\"276.5,-355.5 352.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314.5\" y=\"-340.3\">(None, 15)</text>\n",
       "</g>\n",
       "<!-- 140476272141592&#45;&gt;140476272141424 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140476272141592-&gt;140476272141424</title>\n",
       "<path d=\"M221,-415.366C221,-407.152 221,-397.658 221,-388.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-388.607 221,-378.607 217.5,-388.607 224.5,-388.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140475320395760 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140475320395760</title>\n",
       "<polygon fill=\"none\" points=\"474,-332.5 474,-378.5 782,-378.5 782,-332.5 474,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"554.5\" y=\"-351.8\">decoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"635,-332.5 635,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"662.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"635,-355.5 690,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"662.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"690,-332.5 690,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"736\" y=\"-363.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"690,-355.5 782,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"736\" y=\"-340.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 140475320396656 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140475320396656</title>\n",
       "<polygon fill=\"none\" points=\"488.5,-249.5 488.5,-295.5 767.5,-295.5 767.5,-249.5 488.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"554.5\" y=\"-268.8\">masking_6: Masking</text>\n",
       "<polyline fill=\"none\" points=\"620.5,-249.5 620.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"648\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"620.5,-272.5 675.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"648\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"675.5,-249.5 675.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721.5\" y=\"-280.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"675.5,-272.5 767.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"721.5\" y=\"-257.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 140475320395760&#45;&gt;140475320396656 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140475320395760-&gt;140475320396656</title>\n",
       "<path d=\"M628,-332.366C628,-324.152 628,-314.658 628,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"631.5,-305.607 628,-295.607 624.5,-305.607 631.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140476271279912 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140476271279912</title>\n",
       "<polygon fill=\"none\" points=\"61,-249.5 61,-295.5 381,-295.5 381,-249.5 61,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141.5\" y=\"-268.8\">embedding_5: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"222,-249.5 222,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"222,-272.5 277,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"277,-249.5 277,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-280.3\">(None, 15)</text>\n",
       "<polyline fill=\"none\" points=\"277,-272.5 381,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-257.3\">(None, 15, 100)</text>\n",
       "</g>\n",
       "<!-- 140476272141424&#45;&gt;140476271279912 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140476272141424-&gt;140476271279912</title>\n",
       "<path d=\"M221,-332.366C221,-324.152 221,-314.658 221,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-305.607 221,-295.607 217.5,-305.607 224.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140475320396488 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140475320396488</title>\n",
       "<polygon fill=\"none\" points=\"460.5,-166.5 460.5,-212.5 795.5,-212.5 795.5,-166.5 460.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"541\" y=\"-185.8\">embedding_6: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"621.5,-166.5 621.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"649\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"621.5,-189.5 676.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"649\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"676.5,-166.5 676.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"736\" y=\"-197.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"676.5,-189.5 795.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"736\" y=\"-174.3\">(None, None, 100)</text>\n",
       "</g>\n",
       "<!-- 140475320396656&#45;&gt;140475320396488 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140475320396656-&gt;140475320396488</title>\n",
       "<path d=\"M628,-249.366C628,-241.152 628,-231.658 628,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"631.5,-222.607 628,-212.607 624.5,-222.607 631.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140474675731480 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140474675731480</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 442,-212.5 442,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-185.8\">encoder_lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"148,-166.5 148,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"148,-189.5 203,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"203,-166.5 203,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-197.3\">(None, 15, 100)</text>\n",
       "<polyline fill=\"none\" points=\"203,-189.5 442,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-174.3\">[(None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 140476271279912&#45;&gt;140474675731480 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140476271279912-&gt;140474675731480</title>\n",
       "<path d=\"M221,-249.366C221,-241.152 221,-231.658 221,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-222.607 221,-212.607 217.5,-222.607 224.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140475320351040 -->\n",
       "<g class=\"node\" id=\"node8\"><title>140475320351040</title>\n",
       "<polygon fill=\"none\" points=\"185,-83.5 185,-129.5 663,-129.5 663,-83.5 185,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"259\" y=\"-102.8\">decoder_lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"333,-83.5 333,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"333,-106.5 388,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"388,-83.5 388,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"525.5\" y=\"-114.3\">[(None, None, 100), (None, 256), (None, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"388,-106.5 663,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"525.5\" y=\"-91.3\">[(None, None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 140475320396488&#45;&gt;140475320351040 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140475320396488-&gt;140475320351040</title>\n",
       "<path d=\"M572.526,-166.473C546.668,-156.206 515.733,-143.924 488.806,-133.232\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"490.049,-129.96 479.463,-129.522 487.466,-136.466 490.049,-129.96\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140474675731480&#45;&gt;140475320351040 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>140474675731480-&gt;140475320351040</title>\n",
       "<path d=\"M276.203,-166.473C301.933,-156.206 332.716,-143.924 359.512,-133.232\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"360.818,-136.479 368.808,-129.522 358.223,-129.978 360.818,-136.479\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140475216226568 -->\n",
       "<g class=\"node\" id=\"node9\"><title>140475216226568</title>\n",
       "<polygon fill=\"none\" points=\"286,-0.5 286,-46.5 562,-46.5 562,-0.5 286,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"337\" y=\"-19.8\">dense_3: Dense</text>\n",
       "<polyline fill=\"none\" points=\"388,-0.5 388,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"415.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"388,-23.5 443,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"415.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"443,-0.5 443,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"502.5\" y=\"-31.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"443,-23.5 562,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"502.5\" y=\"-8.3\">(None, None, 310)</text>\n",
       "</g>\n",
       "<!-- 140475320351040&#45;&gt;140475216226568 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>140475320351040-&gt;140475216226568</title>\n",
       "<path d=\"M424,-83.3664C424,-75.1516 424,-65.6579 424,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"427.5,-56.6068 424,-46.6068 420.5,-56.6069 427.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    },
    "scrolled": false
   },
   "source": [
    "## Prepare train/test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encoder_input_data = X_train\n",
    "decoder_input_data = y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# decoder_target_data will be ahead by one timestep\n",
    "# and will not include the start token.\n",
    "decoder_target_data = np.zeros(y_train_one_hot.shape)\n",
    "decoder_target_data[:,:-1] = y_train_one_hot[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "decoder_target_data_test = np.zeros(y_test_one_hot.shape)\n",
    "decoder_target_data_test[:,:-1] = y_test_one_hot[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=.001)\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='auto', \n",
    "                                cooldown=0, min_lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3499 samples, validate on 1501 samples\n",
      "Epoch 1/500\n",
      "3499/3499 [==============================] - 7s 2ms/step - loss: 3.3158 - val_loss: 2.5714\n",
      "Epoch 2/500\n",
      "3499/3499 [==============================] - 5s 2ms/step - loss: 2.3762 - val_loss: 2.1513\n",
      "Epoch 3/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 1.8435 - val_loss: 1.5306\n",
      "Epoch 4/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 1.3430 - val_loss: 1.1892\n",
      "Epoch 5/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 1.0894 - val_loss: 1.0017\n",
      "Epoch 6/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.9325 - val_loss: 0.8752\n",
      "Epoch 7/500\n",
      "3499/3499 [==============================] - 5s 2ms/step - loss: 0.8365 - val_loss: 0.8019\n",
      "Epoch 8/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.7730 - val_loss: 0.7516\n",
      "Epoch 9/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.7266 - val_loss: 0.7125\n",
      "Epoch 10/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.6867 - val_loss: 0.6783\n",
      "Epoch 11/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.6549 - val_loss: 0.6477\n",
      "Epoch 12/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.6256 - val_loss: 0.6179\n",
      "Epoch 13/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.5915 - val_loss: 0.5921\n",
      "Epoch 14/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.5648 - val_loss: 0.5631\n",
      "Epoch 15/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.5396 - val_loss: 0.5413\n",
      "Epoch 16/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.5109 - val_loss: 0.5195\n",
      "Epoch 17/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.4898 - val_loss: 0.5008\n",
      "Epoch 18/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.4659 - val_loss: 0.4747\n",
      "Epoch 19/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.4411 - val_loss: 0.4509\n",
      "Epoch 20/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.4140 - val_loss: 0.4296\n",
      "Epoch 21/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.3901 - val_loss: 0.4056\n",
      "Epoch 22/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.3674 - val_loss: 0.3847\n",
      "Epoch 23/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.3403 - val_loss: 0.3594\n",
      "Epoch 24/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.3139 - val_loss: 0.3364\n",
      "Epoch 25/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.2895 - val_loss: 0.3127\n",
      "Epoch 26/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.2654 - val_loss: 0.2934\n",
      "Epoch 27/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.2441 - val_loss: 0.2669\n",
      "Epoch 28/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.2203 - val_loss: 0.2507\n",
      "Epoch 29/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.2016 - val_loss: 0.2268\n",
      "Epoch 30/500\n",
      "3499/3499 [==============================] - 5s 2ms/step - loss: 0.1816 - val_loss: 0.2120\n",
      "Epoch 31/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.1629 - val_loss: 0.1975\n",
      "Epoch 32/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.1495 - val_loss: 0.1814\n",
      "Epoch 33/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.1361 - val_loss: 0.1669\n",
      "Epoch 34/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.1217 - val_loss: 0.1572\n",
      "Epoch 35/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.1116 - val_loss: 0.1501\n",
      "Epoch 36/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0998 - val_loss: 0.1380\n",
      "Epoch 37/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0897 - val_loss: 0.1318\n",
      "Epoch 38/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0839 - val_loss: 0.1273\n",
      "Epoch 39/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0750 - val_loss: 0.1181\n",
      "Epoch 40/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0693 - val_loss: 0.1178\n",
      "Epoch 41/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0645 - val_loss: 0.1103\n",
      "Epoch 42/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0587 - val_loss: 0.1061\n",
      "Epoch 43/500\n",
      "3499/3499 [==============================] - 5s 2ms/step - loss: 0.0542 - val_loss: 0.1024\n",
      "Epoch 44/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0496 - val_loss: 0.1007\n",
      "Epoch 45/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0464 - val_loss: 0.0981\n",
      "Epoch 46/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0428 - val_loss: 0.0949\n",
      "Epoch 47/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0396 - val_loss: 0.0936\n",
      "Epoch 48/500\n",
      "3499/3499 [==============================] - 5s 2ms/step - loss: 0.0374 - val_loss: 0.0924\n",
      "Epoch 49/500\n",
      "3499/3499 [==============================] - 5s 2ms/step - loss: 0.0348 - val_loss: 0.0902\n",
      "Epoch 50/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0334 - val_loss: 0.0897\n",
      "Epoch 51/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0312 - val_loss: 0.0903\n",
      "Epoch 52/500\n",
      "3499/3499 [==============================] - 5s 2ms/step - loss: 0.0288 - val_loss: 0.0877\n",
      "Epoch 53/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0270 - val_loss: 0.0879\n",
      "Epoch 54/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0247 - val_loss: 0.0859\n",
      "Epoch 55/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0237 - val_loss: 0.0850\n",
      "Epoch 56/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0226 - val_loss: 0.0874\n",
      "Epoch 57/500\n",
      "3499/3499 [==============================] - 5s 2ms/step - loss: 0.0217 - val_loss: 0.0864\n",
      "Epoch 58/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0193 - val_loss: 0.0838\n",
      "Epoch 59/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0180 - val_loss: 0.0848\n",
      "Epoch 60/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0177 - val_loss: 0.0846\n",
      "Epoch 61/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0170 - val_loss: 0.0835\n",
      "Epoch 62/500\n",
      "3499/3499 [==============================] - 5s 2ms/step - loss: 0.0151 - val_loss: 0.0840\n",
      "Epoch 63/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0143 - val_loss: 0.0830\n",
      "Epoch 64/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0137 - val_loss: 0.0837\n",
      "Epoch 65/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0133 - val_loss: 0.0850\n",
      "Epoch 66/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0139 - val_loss: 0.0830\n",
      "Epoch 67/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0122 - val_loss: 0.0847\n",
      "\n",
      "Epoch 00067: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "Epoch 68/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0097 - val_loss: 0.0813\n",
      "Epoch 69/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0085 - val_loss: 0.0809\n",
      "Epoch 70/500\n",
      "3499/3499 [==============================] - 5s 2ms/step - loss: 0.0083 - val_loss: 0.0816\n",
      "Epoch 71/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0078 - val_loss: 0.0815\n",
      "Epoch 72/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0076 - val_loss: 0.0813\n",
      "Epoch 73/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0075 - val_loss: 0.0817\n",
      "\n",
      "Epoch 00073: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Epoch 74/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0070 - val_loss: 0.0811\n",
      "Epoch 75/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0066 - val_loss: 0.0808\n",
      "Epoch 76/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0067 - val_loss: 0.0809\n",
      "Epoch 77/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0063 - val_loss: 0.0807\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0063 - val_loss: 0.0811\n",
      "Epoch 79/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0061 - val_loss: 0.0812\n",
      "Epoch 80/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0062 - val_loss: 0.0814\n",
      "Epoch 81/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0060 - val_loss: 0.0815\n",
      "\n",
      "Epoch 00081: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "Epoch 82/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0059 - val_loss: 0.0812\n",
      "Epoch 83/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0058 - val_loss: 0.0813\n",
      "Epoch 84/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0057 - val_loss: 0.0811\n",
      "Epoch 85/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0056 - val_loss: 0.0810\n",
      "\n",
      "Epoch 00085: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "Epoch 86/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0056 - val_loss: 0.0812\n",
      "Epoch 87/500\n",
      "3499/3499 [==============================] - 6s 2ms/step - loss: 0.0054 - val_loss: 0.0811\n",
      "Epoch 00087: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc2e9b54e80>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run training\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data=([X_test, y_test], decoder_target_data_test),\n",
    "          callbacks=[lr_callback, early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We have trained a model, but how do we use it to actually translate sentences? We have to do more work ourselves here than with a non-recurrent neural net, so we'll write a function to help out. Here are the steps:\n",
    "\n",
    "1. **Encode**:\n",
    "    1. Run the entire input sentence through the encoder part of the model.\n",
    "    1. Write down the \"context vector\" -- this is the state of the last LSTM encoder layer.<br><br>\n",
    "\n",
    "2. **Decode in a loop**:\n",
    "    1. Seed the decoder LSTM with the context vector.\n",
    "    1. Run a *single step* of the decoder with the input \"`<S>`\" (the start symbol).\n",
    "    1. Store the output. This is a word of the translation!\n",
    "    1. Return to step 2B, but feed in the word from step 2C as the new input. Repeat until the decoder returns \"`</S>`\" (the end symbol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "masking_5 (Masking)          (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 15, 100)           20000     \n",
      "_________________________________________________________________\n",
      "encoder_lstm_1 (LSTM)        [(None, 256), (None, 256) 365568    \n",
      "=================================================================\n",
      "Total params: 385,568\n",
      "Trainable params: 385,568\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define a version of our model for use in sampling (as opposed to training).\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"304pt\" viewBox=\"0.00 0.00 450.00 304.00\" width=\"450pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 300)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-300 446,-300 446,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140476272141592 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140476272141592</title>\n",
       "<polygon fill=\"none\" points=\"75,-249.5 75,-295.5 367,-295.5 367,-249.5 75,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-268.8\">encoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"236,-249.5 236,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"236,-272.5 291,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"263.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"291,-249.5 291,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-280.3\">(None, 15)</text>\n",
       "<polyline fill=\"none\" points=\"291,-272.5 367,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-257.3\">(None, 15)</text>\n",
       "</g>\n",
       "<!-- 140476272141424 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140476272141424</title>\n",
       "<polygon fill=\"none\" points=\"89.5,-166.5 89.5,-212.5 352.5,-212.5 352.5,-166.5 89.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"155.5\" y=\"-185.8\">masking_5: Masking</text>\n",
       "<polyline fill=\"none\" points=\"221.5,-166.5 221.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"221.5,-189.5 276.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"276.5,-166.5 276.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314.5\" y=\"-197.3\">(None, 15)</text>\n",
       "<polyline fill=\"none\" points=\"276.5,-189.5 352.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314.5\" y=\"-174.3\">(None, 15)</text>\n",
       "</g>\n",
       "<!-- 140476272141592&#45;&gt;140476272141424 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140476272141592-&gt;140476272141424</title>\n",
       "<path d=\"M221,-249.366C221,-241.152 221,-231.658 221,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-222.607 221,-212.607 217.5,-222.607 224.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140476271279912 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140476271279912</title>\n",
       "<polygon fill=\"none\" points=\"61,-83.5 61,-129.5 381,-129.5 381,-83.5 61,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"141.5\" y=\"-102.8\">embedding_5: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"222,-83.5 222,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"222,-106.5 277,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"249.5\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"277,-83.5 277,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-114.3\">(None, 15)</text>\n",
       "<polyline fill=\"none\" points=\"277,-106.5 381,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"329\" y=\"-91.3\">(None, 15, 100)</text>\n",
       "</g>\n",
       "<!-- 140476272141424&#45;&gt;140476271279912 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140476272141424-&gt;140476271279912</title>\n",
       "<path d=\"M221,-166.366C221,-158.152 221,-148.658 221,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-139.607 221,-129.607 217.5,-139.607 224.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140474675731480 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140474675731480</title>\n",
       "<polygon fill=\"none\" points=\"-2.84217e-14,-0.5 -2.84217e-14,-46.5 442,-46.5 442,-0.5 -2.84217e-14,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"74\" y=\"-19.8\">encoder_lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"148,-0.5 148,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"148,-23.5 203,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"175.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"203,-0.5 203,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-31.3\">(None, 15, 100)</text>\n",
       "<polyline fill=\"none\" points=\"203,-23.5 442,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"322.5\" y=\"-8.3\">[(None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 140476271279912&#45;&gt;140474675731480 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140476271279912-&gt;140474675731480</title>\n",
       "<path d=\"M221,-83.3664C221,-75.1516 221,-65.6579 221,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"224.5,-56.6068 221,-46.6068 217.5,-56.6069 224.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(encoder_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking_6 (Masking)             (None, None)         0           decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, None, 100)    31100       masking_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm_1 (LSTM)           [(None, None, 256),  365568      embedding_6[0][0]                \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, None, 310)    79670       decoder_lstm_1[1][0]             \n",
      "==================================================================================================\n",
      "Total params: 476,338\n",
      "Trainable params: 476,338\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build the decoder.\n",
    "decoder_state_input_h = Input(shape=(NUM_LSTM_NODES,))\n",
    "decoder_state_input_c = Input(shape=(NUM_LSTM_NODES,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs_embedded, initial_state=decoder_states_inputs)\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs,\n",
    "                      [decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"387pt\" viewBox=\"0.00 0.00 905.00 387.00\" width=\"905pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-383 901,-383 901,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140475320395760 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140475320395760</title>\n",
       "<polygon fill=\"none\" points=\"13.5,-332.5 13.5,-378.5 321.5,-378.5 321.5,-332.5 13.5,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94\" y=\"-351.8\">decoder_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"174.5,-332.5 174.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"174.5,-355.5 229.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"202\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"229.5,-332.5 229.5,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275.5\" y=\"-363.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"229.5,-355.5 321.5,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275.5\" y=\"-340.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 140475320396656 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140475320396656</title>\n",
       "<polygon fill=\"none\" points=\"28,-249.5 28,-295.5 307,-295.5 307,-249.5 28,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"94\" y=\"-268.8\">masking_6: Masking</text>\n",
       "<polyline fill=\"none\" points=\"160,-249.5 160,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187.5\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"160,-272.5 215,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187.5\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"215,-249.5 215,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261\" y=\"-280.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"215,-272.5 307,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"261\" y=\"-257.3\">(None, None)</text>\n",
       "</g>\n",
       "<!-- 140475320395760&#45;&gt;140475320396656 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140475320395760-&gt;140475320396656</title>\n",
       "<path d=\"M167.5,-332.366C167.5,-324.152 167.5,-314.658 167.5,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"171,-305.607 167.5,-295.607 164,-305.607 171,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140475320396488 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140475320396488</title>\n",
       "<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 335,-212.5 335,-166.5 0,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"80.5\" y=\"-185.8\">embedding_6: Embedding</text>\n",
       "<polyline fill=\"none\" points=\"161,-166.5 161,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"188.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"161,-189.5 216,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"188.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"216,-166.5 216,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275.5\" y=\"-197.3\">(None, None)</text>\n",
       "<polyline fill=\"none\" points=\"216,-189.5 335,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"275.5\" y=\"-174.3\">(None, None, 100)</text>\n",
       "</g>\n",
       "<!-- 140475320396656&#45;&gt;140475320396488 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140475320396656-&gt;140475320396488</title>\n",
       "<path d=\"M167.5,-249.366C167.5,-241.152 167.5,-231.658 167.5,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"171,-222.607 167.5,-212.607 164,-222.607 171,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140475320351040 -->\n",
       "<g class=\"node\" id=\"node6\"><title>140475320351040</title>\n",
       "<polygon fill=\"none\" points=\"245.5,-83.5 245.5,-129.5 723.5,-129.5 723.5,-83.5 245.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"319.5\" y=\"-102.8\">decoder_lstm_1: LSTM</text>\n",
       "<polyline fill=\"none\" points=\"393.5,-83.5 393.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"421\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"393.5,-106.5 448.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"421\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"448.5,-83.5 448.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"586\" y=\"-114.3\">[(None, None, 100), (None, 256), (None, 256)]</text>\n",
       "<polyline fill=\"none\" points=\"448.5,-106.5 723.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"586\" y=\"-91.3\">[(None, None, 256), (None, 256), (None, 256)]</text>\n",
       "</g>\n",
       "<!-- 140475320396488&#45;&gt;140475320351040 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>140475320396488-&gt;140475320351040</title>\n",
       "<path d=\"M253.703,-166.473C295.375,-155.825 345.533,-143.009 388.43,-132.048\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"389.492,-135.389 398.314,-129.522 387.759,-128.607 389.492,-135.389\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140474956295248 -->\n",
       "<g class=\"node\" id=\"node4\"><title>140474956295248</title>\n",
       "<polygon fill=\"none\" points=\"353,-166.5 353,-212.5 616,-212.5 616,-166.5 353,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"415.5\" y=\"-185.8\">input_5: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"478,-166.5 478,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"505.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"478,-189.5 533,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"505.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"533,-166.5 533,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"574.5\" y=\"-197.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"533,-189.5 616,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"574.5\" y=\"-174.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140474956295248&#45;&gt;140475320351040 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>140474956295248-&gt;140475320351040</title>\n",
       "<path d=\"M484.5,-166.366C484.5,-158.152 484.5,-148.658 484.5,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"488,-139.607 484.5,-129.607 481,-139.607 488,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140474956295136 -->\n",
       "<g class=\"node\" id=\"node5\"><title>140474956295136</title>\n",
       "<polygon fill=\"none\" points=\"634,-166.5 634,-212.5 897,-212.5 897,-166.5 634,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"696.5\" y=\"-185.8\">input_6: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"759,-166.5 759,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"786.5\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"759,-189.5 814,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"786.5\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"814,-166.5 814,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"855.5\" y=\"-197.3\">(None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"814,-189.5 897,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"855.5\" y=\"-174.3\">(None, 256)</text>\n",
       "</g>\n",
       "<!-- 140474956295136&#45;&gt;140475320351040 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>140474956295136-&gt;140475320351040</title>\n",
       "<path d=\"M689.087,-166.473C652.536,-155.937 608.62,-143.279 570.863,-132.395\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"571.476,-128.929 560.898,-129.522 569.537,-135.655 571.476,-128.929\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140475216226568 -->\n",
       "<g class=\"node\" id=\"node7\"><title>140475216226568</title>\n",
       "<polygon fill=\"none\" points=\"346.5,-0.5 346.5,-46.5 622.5,-46.5 622.5,-0.5 346.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"397.5\" y=\"-19.8\">dense_3: Dense</text>\n",
       "<polyline fill=\"none\" points=\"448.5,-0.5 448.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"476\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"448.5,-23.5 503.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"476\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"503.5,-0.5 503.5,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"563\" y=\"-31.3\">(None, None, 256)</text>\n",
       "<polyline fill=\"none\" points=\"503.5,-23.5 622.5,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"563\" y=\"-8.3\">(None, None, 310)</text>\n",
       "</g>\n",
       "<!-- 140475320351040&#45;&gt;140475216226568 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>140475320351040-&gt;140475216226568</title>\n",
       "<path d=\"M484.5,-83.3664C484.5,-75.1516 484.5,-65.6579 484.5,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"488,-56.6068 484.5,-46.6068 481,-56.6069 488,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVG(model_to_dot(decoder_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def translate_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    # TODO: Use the encoder_model to get the h and c vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first word of target sequence with the start symbol '<S>'.\n",
    "    target_seq[0, 0] = word_to_index2['<S>']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    decoded_sentence = ''\n",
    "    stop_condition = False\n",
    "    step = 0\n",
    "    # TODO: complete the loop.\n",
    "    while not stop_condition:\n",
    "        # Use the decoder to get the output token vector and the h and c vectors\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value) \n",
    "\n",
    "        # Find the largest value in the probability output vector, and use that index as your output word\n",
    "        # at this time step.\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = index_to_word2[sampled_token_index]\n",
    "\n",
    "        # Add the word to the output sentence string\n",
    "        #if sampled_word != '<MASK>':\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "        \n",
    "        # Stopping condition: either hit max length or find the stop token '</S>'.\n",
    "        if (sampled_word == '</S>' or\n",
    "           len(decoded_sentence) > 100):\n",
    "            stop_condition = True\n",
    "        \n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Test your network: feed in 10 sentences and show the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-\n",
      "Input sentence:\n",
      " the united states is pleasant during december and it is rainy in winter \n",
      "Decoded sentence:\n",
      "  états unis est agréable en décembre et il pleut en hiver\n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " she likes apples limes and oranges \n",
      "Decoded sentence:\n",
      "  aime les pommes les citrons verts et oranges\n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " france is usually freezing during november and it is never rainy in may \n",
      "Decoded sentence:\n",
      "  est le gel habituellement au mois de novembre et il est jamais pluvieux en mai\n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " china is never quiet during spring but it is sometimes dry in september \n",
      "Decoded sentence:\n",
      "  est jamais tranquille au printemps mais il est parfois sèche en septembre\n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " china is warm during spring and it is sometimes cold in february \n",
      "Decoded sentence:\n",
      "  est chaud au printemps et il est parfois froid en février\n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " india is usually chilly during october but it is usually rainy in summer \n",
      "Decoded sentence:\n",
      "  est généralement froid en octobre mais il est généralement pluvieux en été\n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " she likes oranges strawberries and lemons \n",
      "Decoded sentence:\n",
      "  aime les oranges les fraises et les citrons\n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " california is usually wonderful during spring and it is sometimes pleasant in february \n",
      "Decoded sentence:\n",
      "  est généralement merveilleux au printemps et il est parfois agréable en février\n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " their least favorite fruit is the lime but my least favorite is the peach \n",
      "Decoded sentence:\n",
      "  fruit préféré est moins la chaux mais mon préféré moins est la pêche\n",
      "\n",
      "-\n",
      "Input sentence:\n",
      " california is nice during october but it is freezing in september \n",
      "Decoded sentence:\n",
      "  est agréable en octobre mais il gèle en septembre\n"
     ]
    }
   ],
   "source": [
    "def trans(lst):\n",
    "    \n",
    "    tmp = [i for i in lst[::-1] if i != '<MASK>']\n",
    "    return ('{} ' * len(tmp)).format(*tmp)\n",
    "\n",
    "for i in range(10):\n",
    "    # Print an input sentence\n",
    "    # Translate it and print the output sentence\n",
    "    \n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[i: i + 1]\n",
    "    #print(input_seq)\n",
    "    decoded_sentence = translate_sequence(input_seq)[:translate_sequence(input_seq).find('<') -1]\n",
    "    print('\\n-')\n",
    "    print('Input sentence:\\n', trans(indices_to_sentence(input_seq[0], index_to_word1)))\n",
    "    print('Decoded sentence:\\n', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# copying the file for home usage\\nPATH = \\'../resource/asnlib/publicdata/\\'\\nLOCAL_PATH = \\'./\\'\\n\\nfr =  \\'small_vocab_fr.txt\\'\\nen = \\'small_vocab_en.txt\\'\\n\\nfiles = [[PATH + fr , LOCAL_PATH + fr], [PATH + en , LOCAL_PATH + en]]\\n\\nfor i in files:\\n    with open(i[0],\"r\") as f:\\n        with open(i[1], \"w\") as copy:\\n            for line in f:\\n                copy.write(line)\\n'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# copying the file for home usage\n",
    "PATH = '../resource/asnlib/publicdata/'\n",
    "LOCAL_PATH = './'\n",
    "\n",
    "fr =  'small_vocab_fr.txt'\n",
    "en = 'small_vocab_en.txt'\n",
    "\n",
    "files = [[PATH + fr , LOCAL_PATH + fr], [PATH + en , LOCAL_PATH + en]]\n",
    "\n",
    "for i in files:\n",
    "    with open(i[0],\"r\") as f:\n",
    "        with open(i[1], \"w\") as copy:\n",
    "            for line in f:\n",
    "                copy.write(line)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Question: How well do you think the model did? Discuss any problems you ran into."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Evaluate model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Compute the accuracy of the model on the test set.\n",
    "In a detailed study we would calculate the \"BLEU\" score for the translation task.\n",
    "For this assignment, we'll keep things simple. Just calculate an all-or-nothing accuracy score on each translated sentence. If all the words appear in the output, in the correct order, without extra words, the score on that example is 1. Otherwise 0. Compute the accuracy over all examples in the test set. You may ignore punctuation (commas) and `<S>` and `</S>` symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Report the accuracy value you obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 [3.6]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
