{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "\n",
    "In the lesson we constructed a representation of a simple grid world. Dynamic programming (DP) was used to find optimal plans for a robot to navigate from any starting location on the grid to the goal. This problem is an analog for more complex real-world robot navigation problems. \n",
    "\n",
    "In this homework you will use DP to solve a slightly more complex robotic navigation problem in a grid world. This grid world is a simple version of the problem a material transport robot might encounter in a warehouse. The situation is illustrated in the figure below.\n",
    "\n",
    "<img src=\"GridWorldFactory.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **Grid World for Factory Navigation Example** </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference:\n",
    "\n",
    "- [Gridworld - Stanford](https://cs.stanford.edu/people/karpathy/reinforcejs/gridworld_dp.html)\n",
    "- [Grid - Python](https://www.hackerearth.com/fr/practice/notes/dynamic-programming-problems-involving-grids/) \n",
    "- [Python Optimization](https://medium.com/@annishared/searching-for-optimal-policies-in-python-an-intro-to-optimization-7182d6fe4dba) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is for the robot to deliver some material to position (state) 12, shown in blue. Since there is a goal state or **terminal state** this an **episodic task**. \n",
    "\n",
    "There are some barriers comprised of the states $\\{ 6, 7, 8 \\}$ and $\\{ 16, 17, 18 \\}$, shown with hash marks. In a real warehouse, these positions might be occupied by shelving or equipment. We do not want the robot to hit these barriers. Thus, we say that transitioning to these barrier states is **taboo**.\n",
    "\n",
    "As before, we do not want the robot to hit the edges of the grid world, which represent the outer walls of the warehouse. \n",
    "\n",
    "## Representation\n",
    "\n",
    "As with many such problems, the starting place is creating the **representation**. In the cell below encode your representation for the possible action-state transitions. From each state there are 4 possible actions:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "There are a few special cases you need to consider:\n",
    "- Any action transitioning state off the grid or into a barrier should keep the state unchanged. \n",
    "- Any action in the goal state keeps the state unchanged. \n",
    "- Any transition within the taboo (barrier) states can keep the state unchanged. If you experiment, you will see that other encodings work as well since the value of a barrier states are always zero and there are no actions transitioning into these states. \n",
    "\n",
    "> **Hint:** It may help you create a pencil and paper sketch of the transitions, rewards, and probabilities or policy. This can help you to keep the bookkeeping correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "policy = {0:{'u':0, 'd':5, 'l':0, 'r':1},\n",
    "          1:{'u':1, 'd':1, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':2, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':3, 'l':2, 'r':4},\n",
    "          4:{'u':4, 'd':9, 'l':3, 'r':4},\n",
    "          5:{'u':0, 'd':10, 'l':5, 'r':5},\n",
    "          6:{'u':6, 'd':6, 'l':6, 'r':6},\n",
    "          7:{'u':7, 'd':7, 'l':7, 'r':7},\n",
    "          8:{'u':8, 'd':8, 'l':8, 'r':8},\n",
    "          9:{'u':4, 'd':14, 'l':9, 'r':9},\n",
    "          10:{'u':5, 'd':15, 'l':10, 'r':11},\n",
    "          11:{'u':11, 'd':11, 'l':10, 'r':12},\n",
    "          12:{'u':12, 'd':12, 'l':12, 'r':12},\n",
    "          13:{'u':13, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':9, 'd':19, 'l':13, 'r':14},\n",
    "          15:{'u':10, 'd':20, 'l':15, 'r':15},\n",
    "          16:{'u':16, 'd':16, 'l':16, 'r':16},\n",
    "          17:{'u':17, 'd':17, 'l':17, 'r':17},\n",
    "          18:{'u':18, 'd':18, 'l':18, 'r':18},\n",
    "          19:{'u':14, 'd':24, 'l':19, 'r':19},\n",
    "          20:{'u':15, 'd':20, 'l':20, 'r':21},\n",
    "          21:{'u':21, 'd':21, 'l':20, 'r':22},\n",
    "          22:{'u':22, 'd':22, 'l':21, 'r':23},\n",
    "          23:{'u':23, 'd':23, 'l':22, 'r':24},\n",
    "          24:{'u':19, 'd':24, 'l':23, 'r':24}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to define the initial transition probabilities for the Markov process. Set the probabilities for each transition as a **uniform distribution** leading to random action by the robot. \n",
    "\n",
    "> **Note:** As these are just starting values, the exact values of the transition probabilities are not actually all that important in terms of solving the DP problem. Also, notice that it does not matter how the taboo state transitions are encoded. The point of the DP algorithm is to learn the transition policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_to_state_probs = {0:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      1:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      2:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      3:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      4:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      5:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      6:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      7:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      8:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      9:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      10:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      11:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      12:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "                      13:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      14:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      15:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      16:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      17:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      18:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      19:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      20:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      21:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      22:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      23:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25},\n",
    "                      24:{'u':0.25, 'd':0.25, 'l': 0.25, 'r':0.25}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The robot receives the following rewards:\n",
    "- +10 for achieving the goal. \n",
    "- -1 for attempting to leave the warehouse or hitting the barriers. In other words, we penalize the robot for hitting the edges of the grid or the barriers.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another.  \n",
    "\n",
    "In the code cell below encode your representation of this reward structure.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards =  {0:{'u':-1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "          1:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          2:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          4:{'u':-1, 'd':-0.1, 'l':-0.1, 'r':-1},\n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          6:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          7:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          8:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-0.1},\n",
    "          11:{'u':-1, 'd':-1, 'l':-0.1, 'r':10},\n",
    "          12:{'u':0.0, 'd':0.0, 'l':0.0, 'r':0.0},\n",
    "          13:{'u':-1, 'd':-1, 'l':10, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1},\n",
    "          15:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          16:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          17:{'u':-1, 'd':-1, 'l':-1, 'r':-1},\n",
    "          18:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          19:{'u':-0.1, 'd':-0.1, 'l':-1, 'r':-1},\n",
    "          20:{'u':-0.1, 'd':-1, 'l':-1, 'r':-0.1},\n",
    "          21:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          22:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          23:{'u':-1, 'd':-1, 'l':-0.1, 'r':-0.1},\n",
    "          24:{'u':-0.1, 'd':-1, 'l':-0.1, 'r':-1}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find it useful to create a list of taboo states, which you can encode in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "taboos = [6, 7, 8, 16, 17, 18]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Evaluation\n",
    "\n",
    "With your representation defined, you can now create and test a function for **policy evaluation**. You will need this function for your policy iteration code. \n",
    "\n",
    "You are welcome to state with the `compute_state_value` function from the DP notebook. However, keep in mind that you must modify this code to correctly treat the taboo states. Specifically, taboo states should have 0 value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-38.28926029, -41.56189427, -42.65499935, -41.56857879,\n",
       "        -38.30169862],\n",
       "       [-32.84169421,   0.        ,   0.        ,   0.        ,\n",
       "        -32.8525447 ],\n",
       "       [-25.20972897,  -8.65258033,   0.        ,  -8.65482963,\n",
       "        -25.21851918],\n",
       "       [-32.84716098,   0.        ,   0.        ,   0.        ,\n",
       "        -32.85784594],\n",
       "       [-38.3016983 , -41.5751609 , -42.66847476, -41.58164302,\n",
       "        -38.31375999]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_state_value(pi, probs, reward, taboos, gamma = 1.0, theta = 0.01, display = False):\n",
    "    '''Function for policy evaluation  \n",
    "    '''\n",
    "    delta = theta\n",
    "    values = np.zeros(len(probs)) # Initialize the value array\n",
    "    while(delta >= theta):\n",
    "        v = np.copy(values) ## save the values for computing the difference later\n",
    "        for s in probs.keys():\n",
    "            temp_values = 0.0 ## Initial the sum of values for this state\n",
    "            if s not in taboos: ## Adding a condition to test for taboos \n",
    "                for action in rewards[s].keys():\n",
    "                    s_prime = pi[s][action]\n",
    "                    temp_values = temp_values + probs[s][action] * (reward[s][action] + gamma * values[s_prime])\n",
    "            values[s] = temp_values\n",
    "            \n",
    "        ## Compute the difference metric to see if convergence has been reached.    \n",
    "        diffs = np.sum(np.abs(np.subtract(v, values)))\n",
    "        delta = min([delta, diffs])\n",
    "        if(display): \n",
    "            print('difference metric = ' + str(diffs))\n",
    "            print(values.reshape(5,5))\n",
    "    return values\n",
    "\n",
    "compute_state_value(policy, state_to_state_probs, rewards, taboos, theta = 0.1, display = False).reshape(5,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the state values you have computed using a random walk for the robot. Answer the following questions:\n",
    "\n",
    "1. Are the values of the goal and taboo states zero? ANS: \n",
    "2. Do the values of the states increase closer to the goal? ANS: \n",
    "3. Do the goal and barrier states all have zero values? ANS: \n",
    "\n",
    "If your answer to any of the above questions is no, you need to do some more work on your code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration\n",
    "\n",
    "Now that you have your representation and a function to perform policy evaluation you have everything you need to use the policy iteration algorithm to create an optimal policy for the robot to reach the goal. \n",
    "\n",
    "If your policy evaluation function works correctly, you should be able to use the `policy_iteration` function from the DP notebook. Make sure you print the state values as well as the policy you discovered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "diff = 134.74001676680857\n",
      "Current policy\n",
      "{0: {'u': 0.0, 'd': 0.5, 'l': 0.0, 'r': 0.5}, 1: {'u': 0.0, 'd': 0.0, 'l': 0.5, 'r': 0.5}, 2: {'u': 0.0, 'd': 0.0, 'l': 0.5, 'r': 0.5}, 3: {'u': 0.0, 'd': 0.0, 'l': 0.5, 'r': 0.5}, 4: {'u': 0.0, 'd': 0.5, 'l': 0.5, 'r': 0.0}, 5: {'u': 0.5, 'd': 0.5, 'l': 0.0, 'r': 0.0}, 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 7: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 8: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 9: {'u': 0.5, 'd': 0.5, 'l': 0.0, 'r': 0.0}, 10: {'u': 0.3333333333333333, 'd': 0.3333333333333333, 'l': 0.0, 'r': 0.3333333333333333}, 11: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 12: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 13: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 14: {'u': 0.3333333333333333, 'd': 0.3333333333333333, 'l': 0.3333333333333333, 'r': 0.0}, 15: {'u': 0.5, 'd': 0.5, 'l': 0.0, 'r': 0.0}, 16: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 17: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 18: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 19: {'u': 0.5, 'd': 0.5, 'l': 0.0, 'r': 0.0}, 20: {'u': 0.5, 'd': 0.0, 'l': 0.0, 'r': 0.5}, 21: {'u': 0.0, 'd': 0.0, 'l': 0.5, 'r': 0.5}, 22: {'u': 0.0, 'd': 0.0, 'l': 0.5, 'r': 0.5}, 23: {'u': 0.0, 'd': 0.0, 'l': 0.5, 'r': 0.5}, 24: {'u': 0.5, 'd': 0.0, 'l': 0.5, 'r': 0.0}}\n",
      "With state values\n",
      "[[ 7.00426586  6.69649666  6.59649666  6.70372331  7.0171673 ]\n",
      " [ 7.52540325  0.          0.          0.          7.53545571]\n",
      " [ 8.25027265 10.          0.         10.          8.25697326]\n",
      " [ 7.53062108  0.          0.          0.          7.539969  ]\n",
      " [ 7.01718399  6.71046544  6.61046544  6.7167151   7.02834205]]\n",
      "\n",
      "diff = 40.45998323319145\n",
      "Current policy\n",
      "{0: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 2: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 3: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 4: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 5: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 7: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 8: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 9: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 10: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 11: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 12: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 13: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 14: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 15: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 16: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 17: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 18: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 19: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 20: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 21: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 22: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 23: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 24: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}}\n",
      "With state values\n",
      "[[ 9.7  9.6  9.5  9.6  9.7]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.9 10.   0.  10.   9.9]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.7  9.6  9.5  9.6  9.7]]\n",
      "\n",
      "diff = 0.0\n",
      "Current policy\n",
      "{0: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 2: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 3: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 4: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 5: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 7: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 8: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 9: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0}, 10: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 11: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 12: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 13: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 14: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 15: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 16: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 17: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 18: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}, 19: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 20: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}, 21: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0}, 22: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 23: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0}, 24: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}}\n",
      "With state values\n",
      "[[ 9.7  9.6  9.5  9.6  9.7]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.9 10.   0.  10.   9.9]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.7  9.6  9.5  9.6  9.7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0},\n",
       " 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 2: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0},\n",
       " 3: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0},\n",
       " 4: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0},\n",
       " 5: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0},\n",
       " 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 7: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 8: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 9: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0},\n",
       " 10: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0},\n",
       " 11: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0},\n",
       " 12: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 13: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 14: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 15: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 16: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 17: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 18: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 19: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 20: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 21: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 22: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0},\n",
       " 23: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0},\n",
       " 24: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "def policy_iteration(pi, probs, reward, taboos, gamma = 1.0, theta = 0.1, output = False):\n",
    "    delta = theta\n",
    "    v = np.zeros(len(probs))\n",
    "    state_values = np.zeros(len(probs))\n",
    "    current_policy = copy.deepcopy(probs)\n",
    "    while(delta >= theta): # Continue until convergence.\n",
    "        for s in probs.keys(): # Iterate over all states\n",
    "            temp_values = []\n",
    "            for action in rewards[s].keys(): # Iterate over all possible actions for the state\n",
    "                # Compute list of values given action from the state\n",
    "                s_prime = pi[s][action]\n",
    "                temp_values.append(current_policy[s][action] * (reward[s][action] + gamma * state_values[s_prime]))\n",
    "            \n",
    "            ## Find the max value and update current policy\n",
    "            max_index = np.where(np.array(temp_values) == max(temp_values))[0]\n",
    "            prob_for_policy = 1.0/float(len(max_index))\n",
    "            for i,action in enumerate(current_policy[s].keys()): \n",
    "                if(i in max_index): current_policy[s][action] = prob_for_policy\n",
    "                else: current_policy[s][action] = 0.0\n",
    "                \n",
    "        \n",
    "        ## Compute state values with new policy to determine if there is an improvement\n",
    "        ## Uses the compute_state_value function\n",
    "        state_values = compute_state_value(pi, current_policy, rewards, taboos, theta = .1)\n",
    "        diff = np.sum(np.abs(np.subtract(v, state_values)))\n",
    "        if(output): \n",
    "            print('\\ndiff = ' + str(diff))\n",
    "            print('Current policy')\n",
    "            print(current_policy)\n",
    "            print('With state values')\n",
    "            print(state_values.reshape(5,5))\n",
    "        \n",
    "        delta = min([delta, np.sum(np.abs(np.subtract(v, state_values)))])\n",
    "        v = np.copy(state_values) ## copy of state values to evaluate next iteration\n",
    "    return current_policy\n",
    "\n",
    "policy_iteration(policy, state_to_state_probs, rewards, taboos, gamma = 1.0, output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine your results. First look at the state values at convergence of the policy iteration algorithm and answer the following questions:\n",
    "1. Are non-taboo state values closest to the goal the largest? ANS: yes\n",
    "2. Are the non-taboo state values farthest from the goal the smallest? Keep in mind the robot must travel around the barrier. ANS: yes\n",
    "3. Are the non-taboo state values symmetric (e.g. same) with respect to distance from the goal? ANS: Yes.\n",
    "4. Do the taboo states have 0 values? ANS: Yes.\n",
    "\n",
    "If your answer to any of the above questions is No, there is an error in your code. \n",
    "\n",
    "Next, examine the policy you have computed. Do the follow:\n",
    "1. Follow the optimal paths from the 4 corners of the grid to the goal. How does the symmetry and length of these paths make sense in terms of length and state values? ANS: \n",
    "2. Imagine that the door for the warehouse is at position (state) 2. Insert an illustration showing the paths of the optimal plans below. You are welcome to start with the PowerPoint illustration in the course Github repository.  \n",
    "\n",
    "<img src=\"OptimalPlansOnGridWorld.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **Grid world optimal plans from state 2 to the goal shown goes here** </center>\n",
    "\n",
    "> **Jagriti: the above is a sample answer.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration \n",
    "\n",
    "Finally, your will use the value iteration algorithm to compute an optimal policy for the robot reaching the goal. Keep in mind that you will need to maintain a state value of 0 for the taboo states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.7\n",
      "Difference = 50.7\n",
      "[[-0.1 -0.1 -0.1 -0.1 -0.1]\n",
      " [-0.1  0.   0.   0.  -0.1]\n",
      " [-0.1 10.   0.  10.   9.9]\n",
      " [-0.1  0.   0.   0.   9.8]\n",
      " [-0.1 -0.1 -0.1 -0.1  9.7]]\n",
      "69.2\n",
      "Difference = 69.2\n",
      "[[-0.2 -0.2 -0.2 -0.2 -0.2]\n",
      " [-0.2  0.   0.   0.   9.8]\n",
      " [ 9.9 10.   0.  10.   9.9]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.7  9.6  9.5  9.6  9.7]]\n",
      "20.299999999999997\n",
      "Difference = 20.299999999999997\n",
      "[[-0.3 -0.3 -0.3 -0.3  9.7]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.9 10.   0.  10.   9.9]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.7  9.6  9.5  9.6  9.7]]\n",
      "39.60000000000001\n",
      "Difference = 39.60000000000001\n",
      "[[ 9.7  9.6  9.5  9.6  9.7]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.9 10.   0.  10.   9.9]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.7  9.6  9.5  9.6  9.7]]\n",
      "0.0\n",
      "Difference = 0.0\n",
      "[[ 9.7  9.6  9.5  9.6  9.7]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.9 10.   0.  10.   9.9]\n",
      " [ 9.8  0.   0.   0.   9.8]\n",
      " [ 9.7  9.6  9.5  9.6  9.7]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0},\n",
       " 1: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 2: {'u': 0.0, 'd': 0.0, 'l': 0.5, 'r': 0.5},\n",
       " 3: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0},\n",
       " 4: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0},\n",
       " 5: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0},\n",
       " 6: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 7: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 8: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 9: {'u': 0.0, 'd': 1.0, 'l': 0.0, 'r': 0.0},\n",
       " 10: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0},\n",
       " 11: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0},\n",
       " 12: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 13: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 14: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 15: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 16: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 17: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 18: {'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25},\n",
       " 19: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 20: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0},\n",
       " 21: {'u': 0.0, 'd': 0.0, 'l': 1.0, 'r': 0.0},\n",
       " 22: {'u': 0.0, 'd': 0.0, 'l': 0.5, 'r': 0.5},\n",
       " 23: {'u': 0.0, 'd': 0.0, 'l': 0.0, 'r': 1.0},\n",
       " 24: {'u': 1.0, 'd': 0.0, 'l': 0.0, 'r': 0.0}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def value_iteration(pi, probs, reward, goal, taboos, gamma = 1.0, theta = 0.1, output = False):\n",
    "    delta = theta\n",
    "    v = np.zeros(len(probs))\n",
    "    state_values = np.zeros(len(probs))\n",
    "    current_policy = copy.deepcopy(probs)\n",
    "    \n",
    "    while(delta >= theta):\n",
    "        for s in probs.keys(): # iteratve over all states\n",
    "            temp_values = []\n",
    "            ## Find the values for all possible actions in the state.\n",
    "            for action in rewards[s].keys():\n",
    "                s_prime = pi[s][action]\n",
    "                if s in taboos:\n",
    "                    temp_values.append(0)\n",
    "                else:\n",
    "                    temp_values.append((reward[s][action] + gamma * state_values[s_prime]))\n",
    "                #print('{} {}'.format(s, temp_values))\n",
    "            \n",
    "            ## Find the max value and update the value for the state\n",
    "            state_values[s] = max(temp_values)\n",
    "        ## Determine if convergence is achieved\n",
    "        diff = np.sum(np.abs(np.subtract(v, state_values)))\n",
    "        delta = min([delta, np.sum(np.abs(np.subtract(v, state_values)))])\n",
    "        print(np.sum(np.abs(np.subtract(v, state_values))))\n",
    "        v = np.copy(state_values)\n",
    "        if(output):\n",
    "            print('Difference = ' + str(diff))\n",
    "            print(state_values.reshape(5,5))\n",
    "    \n",
    "    ## Now we need to find the policy that makes max value state transitions\n",
    "    for s in current_policy.keys(): # iterate over all states\n",
    "        ## Find the indicies of maximum state transition values\n",
    "        ## There are two cases. \n",
    "        ## First, the special case of a state adjacent to the goal\n",
    "        ## In this case need to ensure the only possible transition is to the goal.\n",
    "        ## Start by creating a list of the adjacent states.\n",
    "        possible_s_prime = [pi[s][key] for key in current_policy[s].keys()]\n",
    "        ## Check if goal is adjacent, but state is not the goal.\n",
    "        if(goal in possible_s_prime and s != goal):\n",
    "            temp_values = []\n",
    "            for s_prime in current_policy[s].keys(): # Iterate over adjacent states\n",
    "                if(pi[s][s_prime] == goal):  ## account for the special case adjacent to goal\n",
    "                    temp_values.append(reward[s][s_prime])\n",
    "                else: temp_values.append(0.0) ## Other transisions have 0 value.\n",
    "        ## The other case is rather easy requires a lookup of the value of the \n",
    "        ## adjacent state and handled with a list comprehension.             \n",
    "        else: temp_values = [state_values[pi[s][s_prime]] for s_prime in pi[s].keys()]         \n",
    "                \n",
    "        ## Find the index for the state transistions with the largest values \n",
    "        ## May be more than one. \n",
    "        max_index = np.where(np.array(temp_values) == max(temp_values))[0]  \n",
    "        prob_for_policy = 1.0/float(len(max_index)) ## Probabilities of transition\n",
    "        for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "            if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "            else: current_policy[s][key] = 0.0       \n",
    "    return current_policy\n",
    "\n",
    "value_iteration(policy, state_to_state_probs, rewards, 12, taboos,  output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare your results from the value iteration algorithm to your results from the policy iteration algorithm and answer the following questions:\n",
    "1. Are the state values identical between the two methods? ANS: \n",
    "2. Ignoring the taboo states, are the plans computed by the two methods identical? ANS: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
