{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Lesson 5b: Recurrent Neural Networks: Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Recurrent Neural Networks (RNNs) can be used in many different ways, such as classification, single-step prediction, and the generation of an entire sequence. \n",
    "\n",
    "* **Classification**: the input is a sequence, and the output is a single category - this is the focus of this assignment. (Alternatively, a sequence of categories could be generated, one for each partial sequence as it is processed).\n",
    "\n",
    "* **Prediction**: the input is a sequence, and the output is a prediction for the next element in the sequence. You will explore this in lesson 5b.\n",
    "\n",
    "* **Sequence Generation** (Seq-to-Seq): both the input and the output are entire sequences. For example, RNN-based language translation may take in an input sequence (of characters or word tokens) in English, and generate as output a sequence (of characters or word tokens) in French.\n",
    "\n",
    "RNNs can be used to process inputs that occur naturally in time (such as an audio recording of speech or music represented as a stream of timestamped MIDI messages), but they can also be applied to material that has an order to it, even if it's not necessarly temporal in natures, such as written text (which can be read one character or one word at a time) or even written numbers or math equations (which can be read one digit or symbol at a time, from left to right, for instance.)  This is the problem we investigate today: looking at numbers such as \"1423\" as a sequence of digits ['1', '4', '2', '3'].\n",
    "\n",
    "Our problem comes curtosy of Distinguished Professor Douglas R. Hofstadter of Indiana University, author of books such as _Gödel, Escher, Bach: an Eternal Golden Braid_. Hofstadter writes [private communication, shared with permission)]:\n",
    "\n",
    "---\n",
    "\n",
    "_Lately, I have been musing about the seeming power of deep neural nets.  They learn to recognize members of all sorts of categories, when those members (and non-members) are fed to them as patterns of symbols or of pixels.  So, how about the following challenges involving the natural numbers?_\n",
    "\n",
    "* To recognize the even numbers, expressed in base 3.\n",
    "     (Specifically, 0, 2, 11, 20, 22, 101, 110, 112,...)\n",
    "* To recognize the multiples of 3, expressed in base 10.\n",
    "* To recognize the multiples of 9, expressed in base 10.\n",
    "* To recognize the multiples of 7, expressed in base 10.\n",
    "* To recognize the multiples of 29, expressed in base 10.\n",
    "\n",
    "_(I suppose that if a net can learn any particular one of the above list, it can learn all of them.  Just a guess...)_\n",
    "\n",
    " _Moving right along, how about the following somewhat harder challenges?_\n",
    "\n",
    "* To recognize the correct integer additions, expressed either in base 2 or in base 10.  (For example, the string “12+29=41”.)\n",
    "* To recognize the correct integer multiplications, expressed either in base 2 or in base 10.  (For example, the string “12x29=348”.)\n",
    "\n",
    "_(The latter of this pair seems significantly harder than the former.)_\n",
    "     \n",
    "_And then, of course, the canonical challenge of this sort:_\n",
    "\n",
    "* To recognize the prime numbers, expressed either in base 2 or in base 10.\n",
    "\n",
    "_Each of the above challenges involves a number-theoretical category that can easily be described in purely syntactic terms (i.e., as a rule-based pattern of symbols).  It would be trivial to generate millions of examples of such categories mechanically, and then you just feed them to the neural net.  You can also feed the network lots of counterexamples -- marking them, of course, as non-members of the category.  Can a deep neural network learn any of these categories?  All of them?  Some of them?_\n",
    "\n",
    "---\n",
    "\n",
    "In this assignment, you will use an RNN to try to solve the divisibility-by-3 problem (the rest are challenges you might want to try in your free time!): \n",
    "\n",
    "* **\"To recognize the multiples of 3, expressed in base 10.\"**  Specifically, you must:\n",
    "    * Design an RNN that takes a sequence of digits as input. \n",
    "    * Represent digits in base 10 by using a categorical, one-hot encoding, with one node for each digit from 0 through 9.\n",
    "    * Train the RNN to categorize a number as True if it is evenly divisible by 3, False otherwise.\n",
    "    * Test the RNN on a set of previously-unseen numbers, including numbers that are 4 digits long, such as 2225 and 3333.\n",
    "    * Acheive an accuracy of at least 95% on the test set (report the accuracy in the cell marked below).\n",
    "    * Answer the questions at the end of this notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [One hot encoding](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/) \n",
    "- [Sequence Prediction](https://machinelearningmastery.com/sequence-prediction-problems-learning-lstm-recurrent-neural-networks/)\n",
    "- [LSTM Tutorial Keras](https://adventuresinmachinelearning.com/keras-lstm-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Setup\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Keras - Guide to the sequential model](https://keras.io/getting-started/sequential-model-guide/)\n",
    "- [Input](https://keras.io/layers/core/)\n",
    "- [GRU](https://keras.io/layers/recurrent/)\n",
    "- [LSTM](https://keras.io/layers/recurrent/)\n",
    "- [Dense](https://keras.io/layers/core/#Dense)\n",
    "- [Masking](https://keras.io/layers/core/)\n",
    "- [Dropout](https://keras.io/layers/core/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, GRU, LSTM, Dense, Masking, Dropout, InputLayer, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import to_categorical\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up params for dataset.\n",
    "DIVISIBILITY_NUMBER = 3         # We want to test for divisilibity by 3.\n",
    "TRAIN_TEST_SPLIT = 0.7          # Percentage of data in training set\n",
    "NUM_EXAMPLES_PER_CLASS = 1000      # Generate the first 1000 multiples of 3 for training/testing\n",
    "                                # Also generate 1000 non-multiples of 3.\n",
    "NUM_CATEGORIES = 10             # 10 digits\n",
    "MAX_DIGITS = 5                  # Number of digits allowed in input strings\n",
    "\n",
    "# Neural net hyperparameters-- just an example. Adjust these as needed.\n",
    "BATCH_SIZE = 200\n",
    "NUM_LSTM_NODES = 100             \n",
    "DROPOUT = 0.5 \n",
    "LEARING_RATE = 0.3\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "# TODO: add/modify constants as needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Helper functions to generate the dataset of training/testing examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def generate_example_numbers(base_number=DIVISIBILITY_NUMBER, num_examples_per_class=NUM_EXAMPLES_PER_CLASS):\n",
    "    \"\"\"Return a tuple of two lists: (list_of_multiples, list_of_nonmultiples).\n",
    "    \n",
    "    For example, ([0, 3, 6, 9, 12, ...2997], [1, 4, 5, 8, 11, 13, 14,...,2999]).\n",
    "    Each list contains num_examples_per_class elements.\n",
    "    \"\"\"\n",
    "    a = sample(range(0,num_examples_per_class * 60), num_examples_per_class * base_number * 2)\n",
    "    # based on the density of the multiples within a set of integers\n",
    "    multiples = []\n",
    "    non_multiples = []\n",
    "    for i in a:\n",
    "        if i % 3 == 0:\n",
    "            multiples.append(i)\n",
    "        else:\n",
    "            non_multiples.append(i)\n",
    "    \n",
    "    multiples = sample(multiples, num_examples_per_class)\n",
    "    non_multiples = sample(non_multiples, num_examples_per_class)\n",
    "    \n",
    "    return (multiples, non_multiples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min 36 max 59970 len 1000\n",
      "min 2 max 59975 len 1000\n"
     ]
    }
   ],
   "source": [
    "(a,b) = generate_example_numbers(base_number=3, num_examples_per_class=1000)\n",
    "print('min {} max {} len {}'.format(min(a), max(a), len(a)))\n",
    "print('min {} max {} len {}'.format(min(b), max(b), len(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(sum([i % 3 for i in a]) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(0 not in [i % 3 for i in b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(size_multiples, size_non_multiples):\n",
    "    \"\"\"Return two list of labels one for the True case (multiples) and one for the False case (nonmultiples).\n",
    "    \n",
    "    Represent True as 1, False as 0.\n",
    "    For example, return ([1, 1, 1, 1.....], [0, 0, 0, 0, ....]) with each list the requested size.\n",
    "    \"\"\"\n",
    "    return ([1] * size_multiples, [0] * size_non_multiples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_to_vector(digit):\n",
    "    \"\"\"Given a digit from 0-9, return a numpy array representing the digit using a 1-hot encoding.\n",
    "    keras.utils.to_categorical may be useful.\n",
    "    \"\"\"\n",
    "    tmp = to_categorical(digit, dtype='int')\n",
    "    tmp = list(tmp)\n",
    "    tmp.extend([0] * (10 - len(tmp)))\n",
    "    return np.array(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_input_example(number, max_digits=MAX_DIGITS):\n",
    "    \"\"\"Given an integer number, return a numpy float array of 0.0s and 1.0s, of the correct shape to feed into the \n",
    "    neural net.\n",
    "    \n",
    "    For example, if you have a max of 5 digits then you should have a 2D numpy matrix: 5 rows (one for each\n",
    "    sequence index), and 10 columns (1 for each digit).\n",
    "    \n",
    "    In order to train in \"batch\" mode, the RNN expects every example to have the same shape. So if you have a 2-digit\n",
    "    number such as \"42\", you need to pad the example with a \"padding\" token somehow; for example, \"???42\", and then\n",
    "    use keras.layers.Masking to ignore the leading digits. Or just pad with 0s, as in \"00042\". \n",
    "    keras.preprocessing.sequence.pad_sequences can help with this.\n",
    "    \"\"\"\n",
    "    number_str = str(number)\n",
    "    number_str = '0' * (max_digits - len(number_str)) + number_str\n",
    "    tmp = []\n",
    "    for i in number_str:\n",
    "        tmp.append(digit_to_vector(int(i)))\n",
    "    return np.array(tmp)\n",
    "        \n",
    "\n",
    "from random import shuffle\n",
    "    \n",
    "def generate_dataset(divisibility_number=DIVISIBILITY_NUMBER, train_test_split=TRAIN_TEST_SPLIT, \n",
    "                     num_examples_per_class=NUM_EXAMPLES_PER_CLASS):\n",
    "    \"\"\"Generate a dataset ready for training. Returns a list of tuples. Each tuple is of the form\n",
    "    (input_array, label). The dataset should be shuffled either here or during the training process to\n",
    "    mix divisile-by-DIVISIBILITY_NUMBER and not-divisible-by-DIVISIBILITY_NUMBER examples.\n",
    "    The dataset should consist of NUM_EXAMPLES_PER_CLASS positive examples (e.g., 1000 examples of divisible-by-3), and\n",
    "    also NUM_EXAMPLES_PER_CLASS negative examples (e.g., 1000 examples of not-divisible-by-3).\n",
    "    \"\"\"\n",
    "    (multiples, non_multiples) = generate_example_numbers(base_number=divisibility_number, \n",
    "                                                          num_examples_per_class=num_examples_per_class)\n",
    "    \n",
    "    (l_multiples, l_non_multiples) = generate_labels(num_examples_per_class, num_examples_per_class)\n",
    "    \n",
    "    MAX_DIGITS = len(str(max(max(multiples), max(non_multiples))))\n",
    "    def input_f(x):\n",
    "        return number_to_input_example(x, max_digits=MAX_DIGITS)\n",
    "    \n",
    "    \n",
    "    (multiples, non_multiples) = (list(map(input_f, multiples)), list(map(input_f, non_multiples)))\n",
    "    \n",
    "    def helper(x, y):\n",
    "        res = [(i, j) for i,j in zip(x, y)]\n",
    "        return res\n",
    "    \n",
    "    mult = helper(multiples, l_multiples)\n",
    "    non_mult = helper(non_multiples, l_non_multiples)\n",
    "    result = mult + non_mult\n",
    "    shuffle(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result[-1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Helper functions to generate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build RNN model.\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    #TODO: Add/modify layers as desired.\n",
    "    '''\n",
    "    model.add(Input(shape=(784,))) # to edit \n",
    "    model.add(LSTM(NUM_LSTM_NODES, dropout=0.3, return_sequences=False)) \n",
    "    '''\n",
    "    \n",
    "    model.add(InputLayer(input_shape=(5,10))) # to edit \n",
    "    #model.add(Dropout(0.2))\n",
    "    #model.add(Masking(mask_value=0.0))\n",
    "    model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(NUM_LSTM_NODES, dropout=0.1, return_sequences=False, recurrent_dropout=0.1 ))\n",
    "    \n",
    "    #model.add(LSTM(NUM_LSTM_NODES, dropout=0.3, return_sequences=False))\n",
    "    #model.add(LSTM(100))\n",
    "    # Use return_sequences=True for multiple hidden layers\n",
    "    #model.add(Dense(NUM_LSTM_NODES,activation='relu') )\n",
    "    #TODO: Add/modify layers as desired.\n",
    "    # model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Model should return 1 or 0 for divisible/not-divisible\n",
    "    #model.build()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 5, 32)             992       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2, 32)             0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 54,293\n",
      "Trainable params: 54,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Print the model configuration.\n",
    "model = build_model()\n",
    "adam = Adam(lr=LEARING_RATE)   # Modify learning algorithm as needed\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Generate dataset and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_dataset()\n",
    "X = [d[0] for d in data]\n",
    "y = [d[1] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_inputs = np.array(train_inputs)\n",
    "validation_inputs = np.array(validation_inputs)\n",
    "\n",
    "train_labels = np.array(train_labels)\n",
    "validation_labels = np.array(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/100\n",
      "1600/1600 [==============================] - 1s 634us/step - loss: 2.0404 - acc: 0.5094 - val_loss: 0.8007 - val_acc: 0.4525\n",
      "Epoch 2/100\n",
      "1600/1600 [==============================] - 0s 35us/step - loss: 0.7347 - acc: 0.4975 - val_loss: 0.6999 - val_acc: 0.5125\n",
      "Epoch 3/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.7150 - acc: 0.5025 - val_loss: 0.7324 - val_acc: 0.5150\n",
      "Epoch 4/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.7226 - acc: 0.5181 - val_loss: 0.7235 - val_acc: 0.5125\n",
      "Epoch 5/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.7241 - acc: 0.5163 - val_loss: 0.7519 - val_acc: 0.4875\n",
      "Epoch 6/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.7479 - acc: 0.5012 - val_loss: 0.8311 - val_acc: 0.4825\n",
      "Epoch 7/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.7289 - acc: 0.5237 - val_loss: 0.8018 - val_acc: 0.4850\n",
      "Epoch 8/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.7164 - acc: 0.5056 - val_loss: 0.7073 - val_acc: 0.4825\n",
      "Epoch 9/100\n",
      "1600/1600 [==============================] - 0s 33us/step - loss: 0.7025 - acc: 0.5375 - val_loss: 0.7087 - val_acc: 0.4650\n",
      "Epoch 10/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.7079 - acc: 0.5319 - val_loss: 0.7005 - val_acc: 0.5075\n",
      "Epoch 11/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.7161 - acc: 0.5006 - val_loss: 0.7013 - val_acc: 0.5250\n",
      "Epoch 12/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.7002 - acc: 0.5219 - val_loss: 0.7179 - val_acc: 0.4725\n",
      "Epoch 13/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.6967 - acc: 0.5375 - val_loss: 0.7460 - val_acc: 0.5000\n",
      "Epoch 14/100\n",
      "1600/1600 [==============================] - 0s 35us/step - loss: 0.6977 - acc: 0.5250 - val_loss: 0.7130 - val_acc: 0.5100\n",
      "Epoch 15/100\n",
      "1600/1600 [==============================] - 0s 33us/step - loss: 0.6880 - acc: 0.5512 - val_loss: 0.7105 - val_acc: 0.5050\n",
      "Epoch 16/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.6854 - acc: 0.5556 - val_loss: 0.6942 - val_acc: 0.5200\n",
      "Epoch 17/100\n",
      "1600/1600 [==============================] - 0s 35us/step - loss: 0.6822 - acc: 0.5525 - val_loss: 0.7180 - val_acc: 0.4725\n",
      "Epoch 18/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.6924 - acc: 0.5475 - val_loss: 0.7525 - val_acc: 0.4850\n",
      "Epoch 19/100\n",
      "1600/1600 [==============================] - 0s 35us/step - loss: 0.6998 - acc: 0.5450 - val_loss: 0.7057 - val_acc: 0.5225\n",
      "Epoch 20/100\n",
      "1600/1600 [==============================] - 0s 35us/step - loss: 0.7010 - acc: 0.5394 - val_loss: 0.7033 - val_acc: 0.4950\n",
      "Epoch 21/100\n",
      "1600/1600 [==============================] - 0s 33us/step - loss: 0.6939 - acc: 0.5369 - val_loss: 0.7013 - val_acc: 0.5500\n",
      "Epoch 22/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.6880 - acc: 0.5406 - val_loss: 0.7142 - val_acc: 0.4775\n",
      "Epoch 23/100\n",
      "1600/1600 [==============================] - 0s 34us/step - loss: 0.6896 - acc: 0.5362 - val_loss: 0.7066 - val_acc: 0.5150\n",
      "Epoch 24/100\n",
      "1600/1600 [==============================] - 0s 36us/step - loss: 0.7024 - acc: 0.5294 - val_loss: 0.7001 - val_acc: 0.5150\n",
      "Epoch 25/100\n",
      "1600/1600 [==============================] - 0s 37us/step - loss: 0.7056 - acc: 0.5106 - val_loss: 0.7047 - val_acc: 0.4900\n",
      "Epoch 26/100\n",
      "1600/1600 [==============================] - 0s 36us/step - loss: 0.6944 - acc: 0.5325 - val_loss: 0.7005 - val_acc: 0.5000\n",
      "Epoch 27/100\n",
      "1600/1600 [==============================] - 0s 36us/step - loss: 0.6997 - acc: 0.5250 - val_loss: 0.7193 - val_acc: 0.5150\n",
      "Epoch 28/100\n",
      "1600/1600 [==============================] - 0s 35us/step - loss: 0.6961 - acc: 0.5175 - val_loss: 0.7076 - val_acc: 0.5125\n",
      "Epoch 29/100\n",
      "1600/1600 [==============================] - 0s 36us/step - loss: 0.7105 - acc: 0.4975 - val_loss: 0.7266 - val_acc: 0.4775\n",
      "Epoch 30/100\n",
      "1600/1600 [==============================] - 0s 36us/step - loss: 0.7060 - acc: 0.5100 - val_loss: 0.7263 - val_acc: 0.5000\n",
      "Epoch 31/100\n",
      "1600/1600 [==============================] - 0s 37us/step - loss: 0.6950 - acc: 0.5212 - val_loss: 0.7131 - val_acc: 0.5100\n",
      "Epoch 32/100\n",
      "1600/1600 [==============================] - 0s 36us/step - loss: 0.6938 - acc: 0.5119 - val_loss: 0.7223 - val_acc: 0.4775\n",
      "Epoch 33/100\n",
      "1600/1600 [==============================] - 0s 37us/step - loss: 0.6859 - acc: 0.5469 - val_loss: 0.7562 - val_acc: 0.4875\n",
      "Epoch 34/100\n",
      "1600/1600 [==============================] - 0s 37us/step - loss: 0.7063 - acc: 0.5119 - val_loss: 0.7165 - val_acc: 0.4975\n",
      "Epoch 35/100\n",
      "1600/1600 [==============================] - 0s 36us/step - loss: 0.7046 - acc: 0.5137 - val_loss: 0.6979 - val_acc: 0.5150\n",
      "Epoch 36/100\n",
      "1600/1600 [==============================] - 0s 41us/step - loss: 0.7008 - acc: 0.5250 - val_loss: 0.7133 - val_acc: 0.4850\n",
      "Epoch 37/100\n",
      "1600/1600 [==============================] - 0s 38us/step - loss: 0.7037 - acc: 0.5181 - val_loss: 0.7227 - val_acc: 0.4825\n",
      "Epoch 38/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6948 - acc: 0.5212 - val_loss: 0.6951 - val_acc: 0.5200\n",
      "Epoch 39/100\n",
      "1600/1600 [==============================] - 0s 29us/step - loss: 0.6952 - acc: 0.5263 - val_loss: 0.6992 - val_acc: 0.5075\n",
      "Epoch 40/100\n",
      "1600/1600 [==============================] - 0s 29us/step - loss: 0.6931 - acc: 0.5050 - val_loss: 0.6973 - val_acc: 0.5100\n",
      "Epoch 41/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.7053 - acc: 0.5075 - val_loss: 0.6978 - val_acc: 0.5050\n",
      "Epoch 42/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6980 - acc: 0.5231 - val_loss: 0.7270 - val_acc: 0.4975\n",
      "Epoch 43/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6975 - acc: 0.5206 - val_loss: 0.7032 - val_acc: 0.5325\n",
      "Epoch 44/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6821 - acc: 0.5575 - val_loss: 0.7387 - val_acc: 0.4925\n",
      "Epoch 45/100\n",
      "1600/1600 [==============================] - 0s 29us/step - loss: 0.6978 - acc: 0.5281 - val_loss: 0.7096 - val_acc: 0.4900\n",
      "Epoch 46/100\n",
      "1600/1600 [==============================] - 0s 29us/step - loss: 0.6927 - acc: 0.5163 - val_loss: 0.7116 - val_acc: 0.5450\n",
      "Epoch 47/100\n",
      "1600/1600 [==============================] - 0s 29us/step - loss: 0.6928 - acc: 0.5294 - val_loss: 0.6987 - val_acc: 0.5050\n",
      "Epoch 48/100\n",
      "1600/1600 [==============================] - 0s 29us/step - loss: 0.6864 - acc: 0.5550 - val_loss: 0.7118 - val_acc: 0.5200\n",
      "Epoch 49/100\n",
      "1600/1600 [==============================] - 0s 32us/step - loss: 0.6824 - acc: 0.5656 - val_loss: 0.7016 - val_acc: 0.5250\n",
      "Epoch 50/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6900 - acc: 0.5494 - val_loss: 0.7053 - val_acc: 0.5050\n",
      "Epoch 51/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6812 - acc: 0.5463 - val_loss: 0.7105 - val_acc: 0.5075\n",
      "Epoch 52/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6910 - acc: 0.5269 - val_loss: 0.7033 - val_acc: 0.5125\n",
      "Epoch 53/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6897 - acc: 0.5394 - val_loss: 0.6996 - val_acc: 0.5150\n",
      "Epoch 54/100\n",
      "1600/1600 [==============================] - 0s 39us/step - loss: 0.6859 - acc: 0.5456 - val_loss: 0.7035 - val_acc: 0.4950\n",
      "Epoch 55/100\n",
      "1600/1600 [==============================] - 0s 53us/step - loss: 0.6904 - acc: 0.5444 - val_loss: 0.6976 - val_acc: 0.5500\n",
      "Epoch 56/100\n",
      "1600/1600 [==============================] - 0s 65us/step - loss: 0.6941 - acc: 0.5356 - val_loss: 0.6981 - val_acc: 0.5200\n",
      "Epoch 57/100\n",
      "1600/1600 [==============================] - 0s 57us/step - loss: 0.6924 - acc: 0.5231 - val_loss: 0.6958 - val_acc: 0.5125\n",
      "Epoch 58/100\n",
      "1600/1600 [==============================] - 0s 55us/step - loss: 0.6958 - acc: 0.5269 - val_loss: 0.7008 - val_acc: 0.4975\n",
      "Epoch 59/100\n",
      "1600/1600 [==============================] - 0s 54us/step - loss: 0.6991 - acc: 0.5100 - val_loss: 0.6949 - val_acc: 0.5175\n",
      "Epoch 60/100\n",
      "1600/1600 [==============================] - 0s 54us/step - loss: 0.7014 - acc: 0.5406 - val_loss: 0.6981 - val_acc: 0.5125\n",
      "Epoch 61/100\n",
      "1600/1600 [==============================] - 0s 52us/step - loss: 0.6998 - acc: 0.5056 - val_loss: 0.6982 - val_acc: 0.5225\n",
      "Epoch 62/100\n",
      "1600/1600 [==============================] - 0s 56us/step - loss: 0.6958 - acc: 0.5194 - val_loss: 0.6883 - val_acc: 0.5425\n",
      "Epoch 63/100\n",
      "1600/1600 [==============================] - 0s 51us/step - loss: 0.6982 - acc: 0.5294 - val_loss: 0.6968 - val_acc: 0.5375\n",
      "Epoch 64/100\n",
      "1600/1600 [==============================] - 0s 42us/step - loss: 0.6947 - acc: 0.5263 - val_loss: 0.7100 - val_acc: 0.5150\n",
      "Epoch 65/100\n",
      "1600/1600 [==============================] - 0s 33us/step - loss: 0.7026 - acc: 0.5169 - val_loss: 0.7045 - val_acc: 0.4950\n",
      "Epoch 66/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.7055 - acc: 0.5131 - val_loss: 0.7375 - val_acc: 0.4850\n",
      "Epoch 67/100\n",
      "1600/1600 [==============================] - 0s 29us/step - loss: 0.7053 - acc: 0.5175 - val_loss: 0.7102 - val_acc: 0.4925\n",
      "Epoch 68/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6960 - acc: 0.5288 - val_loss: 0.6942 - val_acc: 0.4875\n",
      "Epoch 69/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.6978 - acc: 0.5275 - val_loss: 0.7178 - val_acc: 0.5100\n",
      "Epoch 70/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6980 - acc: 0.5194 - val_loss: 0.7089 - val_acc: 0.5200\n",
      "Epoch 71/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6914 - acc: 0.5300 - val_loss: 0.6929 - val_acc: 0.5200\n",
      "Epoch 72/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6957 - acc: 0.5350 - val_loss: 0.7157 - val_acc: 0.5125\n",
      "Epoch 73/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.7036 - acc: 0.5312 - val_loss: 0.7366 - val_acc: 0.4925\n",
      "Epoch 74/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.7056 - acc: 0.5300 - val_loss: 0.7089 - val_acc: 0.4825\n",
      "Epoch 75/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.6947 - acc: 0.5412 - val_loss: 0.7022 - val_acc: 0.5050\n",
      "Epoch 76/100\n",
      "1600/1600 [==============================] - 0s 32us/step - loss: 0.6984 - acc: 0.5388 - val_loss: 0.7237 - val_acc: 0.5175\n",
      "Epoch 77/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.7131 - acc: 0.5281 - val_loss: 0.7033 - val_acc: 0.5200\n",
      "Epoch 78/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.7322 - acc: 0.5094 - val_loss: 0.7460 - val_acc: 0.4750\n",
      "Epoch 79/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.7264 - acc: 0.4969 - val_loss: 0.7500 - val_acc: 0.4850\n",
      "Epoch 80/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.7182 - acc: 0.5125 - val_loss: 0.7072 - val_acc: 0.5150\n",
      "Epoch 81/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.7100 - acc: 0.5269 - val_loss: 0.6974 - val_acc: 0.5225\n",
      "Epoch 82/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.6980 - acc: 0.5356 - val_loss: 0.6979 - val_acc: 0.4900\n",
      "Epoch 83/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.6884 - acc: 0.5300 - val_loss: 0.6937 - val_acc: 0.5150\n",
      "Epoch 84/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6938 - acc: 0.5225 - val_loss: 0.7368 - val_acc: 0.5025\n",
      "Epoch 85/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.7021 - acc: 0.5150 - val_loss: 0.6985 - val_acc: 0.5275\n",
      "Epoch 86/100\n",
      "1600/1600 [==============================] - 0s 32us/step - loss: 0.6963 - acc: 0.5462 - val_loss: 0.7089 - val_acc: 0.5075\n",
      "Epoch 87/100\n",
      "1600/1600 [==============================] - 0s 32us/step - loss: 0.6995 - acc: 0.5081 - val_loss: 0.6947 - val_acc: 0.4900\n",
      "Epoch 88/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6930 - acc: 0.5356 - val_loss: 0.6944 - val_acc: 0.4800\n",
      "Epoch 89/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6899 - acc: 0.5263 - val_loss: 0.7390 - val_acc: 0.4950\n",
      "Epoch 90/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.7026 - acc: 0.5100 - val_loss: 0.7079 - val_acc: 0.5200\n",
      "Epoch 91/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6992 - acc: 0.5038 - val_loss: 0.6879 - val_acc: 0.5175\n",
      "Epoch 92/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6895 - acc: 0.5400 - val_loss: 0.6951 - val_acc: 0.5075\n",
      "Epoch 93/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6932 - acc: 0.5294 - val_loss: 0.6980 - val_acc: 0.5050\n",
      "Epoch 94/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.7014 - acc: 0.5087 - val_loss: 0.7065 - val_acc: 0.5175\n",
      "Epoch 95/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6903 - acc: 0.5394 - val_loss: 0.7330 - val_acc: 0.5125\n",
      "Epoch 96/100\n",
      "1600/1600 [==============================] - 0s 30us/step - loss: 0.6973 - acc: 0.5387 - val_loss: 0.7318 - val_acc: 0.5125\n",
      "Epoch 97/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.7040 - acc: 0.5169 - val_loss: 0.7150 - val_acc: 0.4900\n",
      "Epoch 98/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.6861 - acc: 0.5288 - val_loss: 0.7098 - val_acc: 0.4875\n",
      "Epoch 99/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.6954 - acc: 0.5337 - val_loss: 0.7543 - val_acc: 0.4875\n",
      "Epoch 100/100\n",
      "1600/1600 [==============================] - 0s 31us/step - loss: 0.7108 - acc: 0.5356 - val_loss: 0.7241 - val_acc: 0.4775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1078fc080>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Configure for viewing validation loss/accuracy using the \"validation_data\" parameter.\n",
    "model.fit(train_inputs, train_labels, validation_data=(validation_inputs, validation_labels), batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Keras - fit](https://keras.io/models/sequential/)\n",
    "- [Recurrent Neural Networks by Example in Python](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470)\n",
    "- [Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Report your final accuracy on the validation dataset below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 47.75%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(validation_inputs, validation_labels, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Examine model outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.40687916]], dtype=float32),\n",
       " array([[0.43077323]], dtype=float32),\n",
       " array([[0.3480372]], dtype=float32),\n",
       " array([[0.65829575]], dtype=float32),\n",
       " array([[0.40687916]], dtype=float32),\n",
       " array([[0.40687916]], dtype=float32),\n",
       " array([[0.40687916]], dtype=float32),\n",
       " array([[0.3480372]], dtype=float32),\n",
       " array([[0.40687916]], dtype=float32),\n",
       " array([[0.43077323]], dtype=float32)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the outputs of the model on some test data.\n",
    "[model.predict(np.expand_dims(validation_inputs[i], 0)) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[validation_labels[i] for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_2 to have 3 dimensions, but got array with shape (5, 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0708a7c5c51c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# TODO something like this:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber_to_input_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/ML/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ML/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/ML/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    126\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    129\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_2 to have 3 dimensions, but got array with shape (5, 10)"
     ]
    }
   ],
   "source": [
    "# Plot some results\n",
    "results = []\n",
    "lo = 0\n",
    "hi = 1000\n",
    "rng = range(lo,hi)\n",
    "for num in rng:\n",
    "    # Hint: to run on a single example, you can use \"np.expand_dims\" to add an extra \n",
    "    # dimension to a 2D array, in order to make a \"batch\" of 1.\n",
    "    #\n",
    "    # TODO something like this:\n",
    "    results.append(model.predict(number_to_input_example(num))[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rng[:100], results[:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Further Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "1) What happens if you give a 5-digit number or a 6-digit number to the trained model, after training on 1-, 2-, 3-, and 4- digit numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "2) Pick another number from Hofstadter's list above, such as 9, 7, or 29. Train a model, and report the accuracy of your results. Did it work or not? Why or why not (your best guess)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "3) Record any other comments/insights from your model training process. What worked well? What caused trouble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "4) If you didn't have a training algorithm, how would you design a RNN-style system to recognize divisibility by 3?\n",
    "Ignoring the details of the weights, what kind of state must be carried over from step to step as each digit is read in\n",
    "successively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "5) BONUS (hard): Explain how the neural net you trained above works, with evidence from examining the node activations as the net runs. Does it do anything similar to what you would have designed as a human?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# LSTM and CNN for sequence classification in the IMDB dataset\n",
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# load the dataset but only keep the top n words, zero the rest\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "# truncate and pad input sequences\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "# create the model\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, epochs=3, batch_size=64)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Jose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, GRU, LSTM, Dense, Masking, Dropout, InputLayer, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import to_categorical\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up params for dataset.\n",
    "DIVISIBILITY_NUMBER = 3         # We want to test for divisilibity by 3.\n",
    "TRAIN_TEST_SPLIT = 0.7          # Percentage of data in training set\n",
    "NUM_EXAMPLES_PER_CLASS = 1000      # Generate the first 1000 multiples of 3 for training/testing\n",
    "                                # Also generate 1000 non-multiples of 3.\n",
    "NUM_CATEGORIES = 10             # 10 digits\n",
    "MAX_DIGITS = 5                  # Number of digits allowed in input strings\n",
    "\n",
    "# Neural net hyperparameters-- just an example. Adjust these as needed.\n",
    "BATCH_SIZE = 200\n",
    "NUM_LSTM_NODES = 100             \n",
    "DROPOUT = 0.5 \n",
    "LEARING_RATE = 0.3\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "# TODO: add/modify constants as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def generate_example_numbers(base_number=DIVISIBILITY_NUMBER, num_examples_per_class=NUM_EXAMPLES_PER_CLASS):\n",
    "    \"\"\"Return a tuple of two lists: (list_of_multiples, list_of_nonmultiples).\n",
    "    \n",
    "    For example, ([0, 3, 6, 9, 12, ...2997], [1, 4, 5, 8, 11, 13, 14,...,2999]).\n",
    "    Each list contains num_examples_per_class elements.\n",
    "    \"\"\"\n",
    "    a = sample(range(0,num_examples_per_class * 60), num_examples_per_class * base_number * 2)\n",
    "    # based on the density of the multiples within a set of integers\n",
    "    multiples = []\n",
    "    non_multiples = []\n",
    "    for i in a:\n",
    "        if i % base_number == 0:\n",
    "            multiples.append(i)\n",
    "        else:\n",
    "            non_multiples.append(i)\n",
    "    \n",
    "    multiples = sample(multiples, num_examples_per_class)\n",
    "    non_multiples = sample(non_multiples, num_examples_per_class)\n",
    "    \n",
    "    return (multiples, non_multiples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_to_vector(digit):\n",
    "    \"\"\"Given a digit from 0-9, return a numpy array representing the digit using a 1-hot encoding.\n",
    "    keras.utils.to_categorical may be useful.\n",
    "    \"\"\"\n",
    "    tmp = to_categorical(digit, dtype='int',num_classes=10)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_input_example(number, max_digits=MAX_DIGITS):\n",
    "    \"\"\"Given an integer number, return a numpy float array of 0.0s and 1.0s, of the correct shape to feed into the \n",
    "    neural net.\n",
    "    \n",
    "    For example, if you have a max of 5 digits then you should have a 2D numpy matrix: 5 rows (one for each\n",
    "    sequence index), and 10 columns (1 for each digit).\n",
    "    \n",
    "    In order to train in \"batch\" mode, the RNN expects every example to have the same shape. So if you have a 2-digit\n",
    "    number such as \"42\", you need to pad the example with a \"padding\" token somehow; for example, \"???42\", and then\n",
    "    use keras.layers.Masking to ignore the leading digits. Or just pad with 0s, as in \"00042\". \n",
    "    keras.preprocessing.sequence.pad_sequences can help with this.\n",
    "    \"\"\"\n",
    "    number_str = str(number)\n",
    "    number_str = '0' * (max_digits - len(number_str)) + number_str\n",
    "    tmp = []\n",
    "    for i in number_str:\n",
    "        tmp.append(digit_to_vector(int(i)))\n",
    "    return np.array(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(divisibility_number=DIVISIBILITY_NUMBER, \n",
    "                     train_test_split=TRAIN_TEST_SPLIT, \n",
    "                     num_examples_per_class=NUM_EXAMPLES_PER_CLASS):\n",
    "    \"\"\"Generate a dataset ready for training. Returns a list of tuples. Each tuple is of the form\n",
    "    (input_array, label). The dataset should be shuffled either here or during the training process to\n",
    "    mix divisile-by-DIVISIBILITY_NUMBER and not-divisible-by-DIVISIBILITY_NUMBER examples.\n",
    "    The dataset should consist of NUM_EXAMPLES_PER_CLASS positive examples (e.g., 1000 examples of divisible-by-3), and\n",
    "    also NUM_EXAMPLES_PER_CLASS negative examples (e.g., 1000 examples of not-divisible-by-3).\n",
    "    \"\"\"\n",
    "    example_numbers = generate_example_numbers(divisibility_number, num_examples_per_class)\n",
    "    divisible = example_numbers[0]\n",
    "    non_divisible = example_numbers[1]\n",
    "    np.random.shuffle(divisible)\n",
    "    np.random.shuffle(non_divisible)\n",
    "    \n",
    "    result = []\n",
    "    for k, m in zip(divisible, non_divisible):\n",
    "        result.append((k, 1))\n",
    "        result.append((m, 0))\n",
    "        \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (1, 5, 100)               44400     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (1, 5, 100)               10100     \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (1, 100)                  80400     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (1, 1)                    101       \n",
      "=================================================================\n",
      "Total params: 135,001\n",
      "Trainable params: 135,001\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build RNN model.\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(NUM_LSTM_NODES, dropout=DROPOUT, recurrent_dropout=DROPOUT, batch_input_shape=(1,MAX_DIGITS,10), return_sequences=True))  # Use return_sequences=True for multiple hidden layers\n",
    "    model.add(Dense(NUM_LSTM_NODES))\n",
    "    model.add(LSTM(NUM_LSTM_NODES, dropout=DROPOUT, recurrent_dropout=DROPOUT, batch_input_shape=(1,MAX_DIGITS,10), return_sequences=False))  # Use return_sequences=True for multiple hidden layers\n",
    "    model.add(Dense(1))  # Model should return 1 or 0 for divisible/not-divisible\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Print the model configuration.\n",
    "model = build_model()\n",
    "\n",
    "# Compile model\n",
    "adam = Adam(lr=LEARING_RATE)   # Modify learning algorithm as needed\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50472, 1),\n",
       " (45569, 0),\n",
       " (168, 1),\n",
       " (17029, 0),\n",
       " (10677, 1),\n",
       " (46087, 0),\n",
       " (1443, 1),\n",
       " (22996, 0),\n",
       " (35289, 1),\n",
       " (13453, 0),\n",
       " (10629, 1),\n",
       " (9823, 0),\n",
       " (44622, 1),\n",
       " (41069, 0),\n",
       " (15798, 1),\n",
       " (53125, 0),\n",
       " (56730, 1),\n",
       " (29870, 0),\n",
       " (8055, 1),\n",
       " (50467, 0),\n",
       " (2013, 1),\n",
       " (32167, 0),\n",
       " (46926, 1),\n",
       " (41444, 0),\n",
       " (12363, 1),\n",
       " (42968, 0),\n",
       " (447, 1),\n",
       " (293, 0),\n",
       " (23745, 1),\n",
       " (48020, 0),\n",
       " (42048, 1),\n",
       " (50443, 0),\n",
       " (24360, 1),\n",
       " (19901, 0),\n",
       " (20328, 1),\n",
       " (5104, 0),\n",
       " (20130, 1),\n",
       " (12347, 0),\n",
       " (57975, 1),\n",
       " (28366, 0),\n",
       " (21957, 1),\n",
       " (35710, 0),\n",
       " (12135, 1),\n",
       " (9206, 0),\n",
       " (31398, 1),\n",
       " (50048, 0),\n",
       " (53178, 1),\n",
       " (40859, 0),\n",
       " (50550, 1),\n",
       " (21214, 0),\n",
       " (57690, 1),\n",
       " (13957, 0),\n",
       " (37014, 1),\n",
       " (40744, 0),\n",
       " (13428, 1),\n",
       " (20684, 0),\n",
       " (17121, 1),\n",
       " (38206, 0),\n",
       " (13932, 1),\n",
       " (14645, 0),\n",
       " (3315, 1),\n",
       " (45350, 0),\n",
       " (23517, 1),\n",
       " (55522, 0),\n",
       " (41370, 1),\n",
       " (25151, 0),\n",
       " (33288, 1),\n",
       " (13225, 0),\n",
       " (33, 1),\n",
       " (21245, 0),\n",
       " (26115, 1),\n",
       " (2789, 0),\n",
       " (2646, 1),\n",
       " (47827, 0),\n",
       " (18801, 1),\n",
       " (25609, 0),\n",
       " (11256, 1),\n",
       " (2774, 0),\n",
       " (44952, 1),\n",
       " (29402, 0),\n",
       " (5709, 1),\n",
       " (3266, 0),\n",
       " (25635, 1),\n",
       " (27230, 0),\n",
       " (18888, 1),\n",
       " (37267, 0),\n",
       " (16098, 1),\n",
       " (48337, 0),\n",
       " (23289, 1),\n",
       " (41747, 0),\n",
       " (40770, 1),\n",
       " (17579, 0),\n",
       " (46938, 1),\n",
       " (10133, 0),\n",
       " (16896, 1),\n",
       " (59408, 0),\n",
       " (36051, 1),\n",
       " (10825, 0),\n",
       " (20976, 1),\n",
       " (8426, 0),\n",
       " (58173, 1),\n",
       " (32627, 0),\n",
       " (16467, 1),\n",
       " (34568, 0),\n",
       " (10479, 1),\n",
       " (20972, 0),\n",
       " (33267, 1),\n",
       " (25064, 0),\n",
       " (3537, 1),\n",
       " (32006, 0),\n",
       " (29808, 1),\n",
       " (55918, 0),\n",
       " (21201, 1),\n",
       " (44362, 0),\n",
       " (30873, 1),\n",
       " (34780, 0),\n",
       " (2376, 1),\n",
       " (50702, 0),\n",
       " (6315, 1),\n",
       " (38591, 0),\n",
       " (54117, 1),\n",
       " (44276, 0),\n",
       " (20316, 1),\n",
       " (15592, 0),\n",
       " (22707, 1),\n",
       " (57577, 0),\n",
       " (21534, 1),\n",
       " (52940, 0),\n",
       " (21981, 1),\n",
       " (11363, 0),\n",
       " (18936, 1),\n",
       " (31760, 0),\n",
       " (25380, 1),\n",
       " (11902, 0),\n",
       " (8934, 1),\n",
       " (20854, 0),\n",
       " (41445, 1),\n",
       " (45022, 0),\n",
       " (41334, 1),\n",
       " (54583, 0),\n",
       " (22263, 1),\n",
       " (1948, 0),\n",
       " (6444, 1),\n",
       " (57473, 0),\n",
       " (52104, 1),\n",
       " (54659, 0),\n",
       " (34641, 1),\n",
       " (14978, 0),\n",
       " (4749, 1),\n",
       " (44350, 0),\n",
       " (46020, 1),\n",
       " (48404, 0),\n",
       " (14172, 1),\n",
       " (33011, 0),\n",
       " (47625, 1),\n",
       " (30830, 0),\n",
       " (21447, 1),\n",
       " (2132, 0),\n",
       " (56121, 1),\n",
       " (58732, 0),\n",
       " (19227, 1),\n",
       " (17629, 0),\n",
       " (55329, 1),\n",
       " (18215, 0),\n",
       " (41928, 1),\n",
       " (13996, 0),\n",
       " (37881, 1),\n",
       " (4691, 0),\n",
       " (34701, 1),\n",
       " (45847, 0),\n",
       " (18282, 1),\n",
       " (5533, 0),\n",
       " (11898, 1),\n",
       " (59678, 0),\n",
       " (46746, 1),\n",
       " (6046, 0),\n",
       " (18669, 1),\n",
       " (56653, 0),\n",
       " (35790, 1),\n",
       " (8876, 0),\n",
       " (50748, 1),\n",
       " (11624, 0),\n",
       " (49971, 1),\n",
       " (18242, 0),\n",
       " (34236, 1),\n",
       " (43129, 0),\n",
       " (37707, 1),\n",
       " (46867, 0),\n",
       " (4020, 1),\n",
       " (58936, 0),\n",
       " (24261, 1),\n",
       " (15722, 0),\n",
       " (32331, 1),\n",
       " (7018, 0),\n",
       " (59412, 1),\n",
       " (11272, 0),\n",
       " (10926, 1),\n",
       " (10721, 0),\n",
       " (8574, 1),\n",
       " (18991, 0),\n",
       " (34983, 1),\n",
       " (43276, 0),\n",
       " (1824, 1),\n",
       " (55790, 0),\n",
       " (3918, 1),\n",
       " (30538, 0),\n",
       " (19236, 1),\n",
       " (49546, 0),\n",
       " (21765, 1),\n",
       " (24352, 0),\n",
       " (35214, 1),\n",
       " (32314, 0),\n",
       " (16227, 1),\n",
       " (21206, 0),\n",
       " (52533, 1),\n",
       " (59506, 0),\n",
       " (36783, 1),\n",
       " (31430, 0),\n",
       " (37992, 1),\n",
       " (12155, 0),\n",
       " (36495, 1),\n",
       " (55889, 0),\n",
       " (2793, 1),\n",
       " (45911, 0),\n",
       " (12126, 1),\n",
       " (5699, 0),\n",
       " (51924, 1),\n",
       " (20519, 0),\n",
       " (20034, 1),\n",
       " (43583, 0),\n",
       " (44277, 1),\n",
       " (36578, 0),\n",
       " (39342, 1),\n",
       " (638, 0),\n",
       " (59829, 1),\n",
       " (29312, 0),\n",
       " (47910, 1),\n",
       " (52655, 0),\n",
       " (26817, 1),\n",
       " (3400, 0),\n",
       " (12747, 1),\n",
       " (28802, 0),\n",
       " (26724, 1),\n",
       " (32617, 0),\n",
       " (51021, 1),\n",
       " (42007, 0),\n",
       " (8985, 1),\n",
       " (59348, 0),\n",
       " (5538, 1),\n",
       " (36674, 0),\n",
       " (33549, 1),\n",
       " (2069, 0),\n",
       " (16857, 1),\n",
       " (25132, 0),\n",
       " (35859, 1),\n",
       " (51794, 0),\n",
       " (43644, 1),\n",
       " (47596, 0),\n",
       " (52314, 1),\n",
       " (54403, 0),\n",
       " (24252, 1),\n",
       " (58877, 0),\n",
       " (43818, 1),\n",
       " (51248, 0),\n",
       " (33255, 1),\n",
       " (41759, 0),\n",
       " (54549, 1),\n",
       " (14075, 0),\n",
       " (49857, 1),\n",
       " (14629, 0),\n",
       " (13974, 1),\n",
       " (43363, 0),\n",
       " (48429, 1),\n",
       " (44501, 0),\n",
       " (9438, 1),\n",
       " (27410, 0),\n",
       " (53952, 1),\n",
       " (18937, 0),\n",
       " (36021, 1),\n",
       " (49642, 0),\n",
       " (41286, 1),\n",
       " (48682, 0),\n",
       " (58509, 1),\n",
       " (4804, 0),\n",
       " (38643, 1),\n",
       " (51703, 0),\n",
       " (29391, 1),\n",
       " (26572, 0),\n",
       " (1566, 1),\n",
       " (1187, 0),\n",
       " (13866, 1),\n",
       " (35531, 0),\n",
       " (41493, 1),\n",
       " (30827, 0),\n",
       " (4443, 1),\n",
       " (56512, 0),\n",
       " (4467, 1),\n",
       " (33776, 0),\n",
       " (43989, 1),\n",
       " (13538, 0),\n",
       " (1374, 1),\n",
       " (36386, 0),\n",
       " (48213, 1),\n",
       " (27224, 0),\n",
       " (22857, 1),\n",
       " (34094, 0),\n",
       " (48516, 1),\n",
       " (44624, 0),\n",
       " (28434, 1),\n",
       " (57094, 0),\n",
       " (21936, 1),\n",
       " (24661, 0),\n",
       " (8940, 1),\n",
       " (10925, 0),\n",
       " (55263, 1),\n",
       " (40846, 0),\n",
       " (6441, 1),\n",
       " (32605, 0),\n",
       " (42561, 1),\n",
       " (42262, 0),\n",
       " (55926, 1),\n",
       " (18892, 0),\n",
       " (52080, 1),\n",
       " (52781, 0),\n",
       " (41292, 1),\n",
       " (22708, 0),\n",
       " (30036, 1),\n",
       " (7496, 0),\n",
       " (45663, 1),\n",
       " (1705, 0),\n",
       " (50814, 1),\n",
       " (38266, 0),\n",
       " (12459, 1),\n",
       " (11605, 0),\n",
       " (50433, 1),\n",
       " (51128, 0),\n",
       " (46575, 1),\n",
       " (44849, 0),\n",
       " (37002, 1),\n",
       " (59828, 0),\n",
       " (45462, 1),\n",
       " (22346, 0),\n",
       " (582, 1),\n",
       " (41371, 0),\n",
       " (36660, 1),\n",
       " (39797, 0),\n",
       " (18102, 1),\n",
       " (14338, 0),\n",
       " (42942, 1),\n",
       " (57472, 0),\n",
       " (32154, 1),\n",
       " (58927, 0),\n",
       " (13065, 1),\n",
       " (40478, 0),\n",
       " (12609, 1),\n",
       " (35297, 0),\n",
       " (13449, 1),\n",
       " (53504, 0),\n",
       " (23817, 1),\n",
       " (24619, 0),\n",
       " (53040, 1),\n",
       " (28439, 0),\n",
       " (51972, 1),\n",
       " (11056, 0),\n",
       " (48090, 1),\n",
       " (37214, 0),\n",
       " (59832, 1),\n",
       " (26102, 0),\n",
       " (27291, 1),\n",
       " (39829, 0),\n",
       " (42492, 1),\n",
       " (163, 0),\n",
       " (28299, 1),\n",
       " (26912, 0),\n",
       " (51174, 1),\n",
       " (10267, 0),\n",
       " (43992, 1),\n",
       " (58834, 0),\n",
       " (25296, 1),\n",
       " (31315, 0),\n",
       " (20667, 1),\n",
       " (34897, 0),\n",
       " (52719, 1),\n",
       " (5711, 0),\n",
       " (43419, 1),\n",
       " (29189, 0),\n",
       " (13332, 1),\n",
       " (30482, 0),\n",
       " (7104, 1),\n",
       " (32795, 0),\n",
       " (53913, 1),\n",
       " (19973, 0),\n",
       " (51867, 1),\n",
       " (14050, 0),\n",
       " (40101, 1),\n",
       " (23152, 0),\n",
       " (41343, 1),\n",
       " (4823, 0),\n",
       " (141, 1),\n",
       " (56786, 0),\n",
       " (39882, 1),\n",
       " (44035, 0),\n",
       " (33939, 1),\n",
       " (48661, 0),\n",
       " (3633, 1),\n",
       " (6946, 0),\n",
       " (16641, 1),\n",
       " (21161, 0),\n",
       " (56667, 1),\n",
       " (30685, 0),\n",
       " (42588, 1),\n",
       " (25603, 0),\n",
       " (56502, 1),\n",
       " (5849, 0),\n",
       " (7830, 1),\n",
       " (17594, 0),\n",
       " (55773, 1),\n",
       " (4880, 0),\n",
       " (11931, 1),\n",
       " (55361, 0),\n",
       " (1002, 1),\n",
       " (23852, 0),\n",
       " (43617, 1),\n",
       " (2282, 0),\n",
       " (14760, 1),\n",
       " (53890, 0),\n",
       " (20673, 1),\n",
       " (40255, 0),\n",
       " (19083, 1),\n",
       " (169, 0),\n",
       " (19161, 1),\n",
       " (3538, 0),\n",
       " (2115, 1),\n",
       " (22027, 0),\n",
       " (25101, 1),\n",
       " (21697, 0),\n",
       " (21222, 1),\n",
       " (6173, 0),\n",
       " (42813, 1),\n",
       " (23113, 0),\n",
       " (21180, 1),\n",
       " (29177, 0),\n",
       " (39216, 1),\n",
       " (34831, 0),\n",
       " (5175, 1),\n",
       " (32638, 0),\n",
       " (36126, 1),\n",
       " (59855, 0),\n",
       " (40149, 1),\n",
       " (36365, 0),\n",
       " (19488, 1),\n",
       " (2990, 0),\n",
       " (22974, 1),\n",
       " (25061, 0),\n",
       " (42912, 1),\n",
       " (32026, 0),\n",
       " (52908, 1),\n",
       " (5332, 0),\n",
       " (44580, 1),\n",
       " (14270, 0),\n",
       " (17907, 1),\n",
       " (53791, 0),\n",
       " (57009, 1),\n",
       " (8626, 0),\n",
       " (33375, 1),\n",
       " (19796, 0),\n",
       " (21921, 1),\n",
       " (2240, 0),\n",
       " (22863, 1),\n",
       " (53878, 0),\n",
       " (28905, 1),\n",
       " (46058, 0),\n",
       " (37644, 1),\n",
       " (28262, 0),\n",
       " (26556, 1),\n",
       " (44386, 0),\n",
       " (39423, 1),\n",
       " (50824, 0),\n",
       " (16356, 1),\n",
       " (17495, 0),\n",
       " (9621, 1),\n",
       " (40151, 0),\n",
       " (47781, 1),\n",
       " (13487, 0),\n",
       " (13506, 1),\n",
       " (16450, 0),\n",
       " (57504, 1),\n",
       " (19829, 0),\n",
       " (1653, 1),\n",
       " (47614, 0),\n",
       " (12492, 1),\n",
       " (1592, 0),\n",
       " (5673, 1),\n",
       " (57734, 0),\n",
       " (24879, 1),\n",
       " (13417, 0),\n",
       " (55683, 1),\n",
       " (4852, 0),\n",
       " (33015, 1),\n",
       " (23921, 0),\n",
       " (18921, 1),\n",
       " (57578, 0),\n",
       " (54834, 1),\n",
       " (2072, 0),\n",
       " (42381, 1),\n",
       " (58411, 0),\n",
       " (1365, 1),\n",
       " (38090, 0),\n",
       " (25239, 1),\n",
       " (41644, 0),\n",
       " (20118, 1),\n",
       " (54998, 0),\n",
       " (55833, 1),\n",
       " (24809, 0),\n",
       " (46827, 1),\n",
       " (34778, 0),\n",
       " (12453, 1),\n",
       " (18688, 0),\n",
       " (13779, 1),\n",
       " (44308, 0),\n",
       " (21291, 1),\n",
       " (33074, 0),\n",
       " (4206, 1),\n",
       " (15184, 0),\n",
       " (17910, 1),\n",
       " (17624, 0),\n",
       " (12090, 1),\n",
       " (33023, 0),\n",
       " (21726, 1),\n",
       " (54256, 0),\n",
       " (52971, 1),\n",
       " (52804, 0),\n",
       " (18798, 1),\n",
       " (25283, 0),\n",
       " (45270, 1),\n",
       " (49601, 0),\n",
       " (13929, 1),\n",
       " (57610, 0),\n",
       " (47772, 1),\n",
       " (5072, 0),\n",
       " (57696, 1),\n",
       " (27878, 0),\n",
       " (11757, 1),\n",
       " (25517, 0),\n",
       " (11211, 1),\n",
       " (35839, 0),\n",
       " (34377, 1),\n",
       " (49171, 0),\n",
       " (7302, 1),\n",
       " (54110, 0),\n",
       " (29991, 1),\n",
       " (54811, 0),\n",
       " (53181, 1),\n",
       " (26008, 0),\n",
       " (30510, 1),\n",
       " (10924, 0),\n",
       " (49008, 1),\n",
       " (12026, 0),\n",
       " (56241, 1),\n",
       " (45889, 0),\n",
       " (3276, 1),\n",
       " (56464, 0),\n",
       " (27714, 1),\n",
       " (50437, 0),\n",
       " (57414, 1),\n",
       " (47879, 0),\n",
       " (43197, 1),\n",
       " (37999, 0),\n",
       " (2187, 1),\n",
       " (25211, 0),\n",
       " (33666, 1),\n",
       " (36700, 0),\n",
       " (13956, 1),\n",
       " (869, 0),\n",
       " (42135, 1),\n",
       " (59236, 0),\n",
       " (48681, 1),\n",
       " (50435, 0),\n",
       " (33906, 1),\n",
       " (11276, 0),\n",
       " (43326, 1),\n",
       " (6263, 0),\n",
       " (24714, 1),\n",
       " (41266, 0),\n",
       " (11193, 1),\n",
       " (9899, 0),\n",
       " (35070, 1),\n",
       " (54782, 0),\n",
       " (35856, 1),\n",
       " (53926, 0),\n",
       " (5682, 1),\n",
       " (35020, 0),\n",
       " (54519, 1),\n",
       " (54280, 0),\n",
       " (49914, 1),\n",
       " (1313, 0),\n",
       " (22950, 1),\n",
       " (15455, 0),\n",
       " (26967, 1),\n",
       " (23623, 0),\n",
       " (43512, 1),\n",
       " (34228, 0),\n",
       " (19767, 1),\n",
       " (16544, 0),\n",
       " (55053, 1),\n",
       " (18872, 0),\n",
       " (47022, 1),\n",
       " (48562, 0),\n",
       " (12711, 1),\n",
       " (37207, 0),\n",
       " (19368, 1),\n",
       " (22847, 0),\n",
       " (33084, 1),\n",
       " (25400, 0),\n",
       " (50466, 1),\n",
       " (15979, 0),\n",
       " (27099, 1),\n",
       " (30374, 0),\n",
       " (59496, 1),\n",
       " (59237, 0),\n",
       " (14565, 1),\n",
       " (50098, 0),\n",
       " (58929, 1),\n",
       " (24998, 0),\n",
       " (26211, 1),\n",
       " (15473, 0),\n",
       " (9636, 1),\n",
       " (27116, 0),\n",
       " (16824, 1),\n",
       " (21556, 0),\n",
       " (10089, 1),\n",
       " (12128, 0),\n",
       " (19395, 1),\n",
       " (12521, 0),\n",
       " (40542, 1),\n",
       " (58231, 0),\n",
       " (49599, 1),\n",
       " (1726, 0),\n",
       " (50934, 1),\n",
       " (39698, 0),\n",
       " (1833, 1),\n",
       " (58453, 0),\n",
       " (32790, 1),\n",
       " (44828, 0),\n",
       " (27267, 1),\n",
       " (25108, 0),\n",
       " (6132, 1),\n",
       " (11053, 0),\n",
       " (17685, 1),\n",
       " (32579, 0),\n",
       " (23358, 1),\n",
       " (36650, 0),\n",
       " (31176, 1),\n",
       " (11717, 0),\n",
       " (17373, 1),\n",
       " (5911, 0),\n",
       " (47958, 1),\n",
       " (35452, 0),\n",
       " (50133, 1),\n",
       " (6947, 0),\n",
       " (51645, 1),\n",
       " (20192, 0),\n",
       " (1068, 1),\n",
       " (39472, 0),\n",
       " (18384, 1),\n",
       " (48133, 0),\n",
       " (42327, 1),\n",
       " (43172, 0),\n",
       " (9447, 1),\n",
       " (25037, 0),\n",
       " (37374, 1),\n",
       " (38479, 0),\n",
       " (41625, 1),\n",
       " (27586, 0),\n",
       " (29130, 1),\n",
       " (33649, 0),\n",
       " (9669, 1),\n",
       " (53770, 0),\n",
       " (51660, 1),\n",
       " (23408, 0),\n",
       " (13059, 1),\n",
       " (23255, 0),\n",
       " (2706, 1),\n",
       " (44894, 0),\n",
       " (15183, 1),\n",
       " (3581, 0),\n",
       " (9264, 1),\n",
       " (36406, 0),\n",
       " (54273, 1),\n",
       " (9959, 0),\n",
       " (32523, 1),\n",
       " (48410, 0),\n",
       " (1773, 1),\n",
       " (54572, 0),\n",
       " (41397, 1),\n",
       " (20831, 0),\n",
       " (35775, 1),\n",
       " (55526, 0),\n",
       " (17952, 1),\n",
       " (52928, 0),\n",
       " (4224, 1),\n",
       " (36478, 0),\n",
       " (3213, 1),\n",
       " (28328, 0),\n",
       " (14166, 1),\n",
       " (34519, 0),\n",
       " (35313, 1),\n",
       " (54115, 0),\n",
       " (20499, 1),\n",
       " (49018, 0),\n",
       " (12873, 1),\n",
       " (25720, 0),\n",
       " (53070, 1),\n",
       " (52361, 0),\n",
       " (10137, 1),\n",
       " (40520, 0),\n",
       " (39219, 1),\n",
       " (52436, 0),\n",
       " (53565, 1),\n",
       " (14303, 0),\n",
       " (32949, 1),\n",
       " (22928, 0),\n",
       " (33504, 1),\n",
       " (43292, 0),\n",
       " (59070, 1),\n",
       " (53441, 0),\n",
       " (26127, 1),\n",
       " (28963, 0),\n",
       " (42930, 1),\n",
       " (30691, 0),\n",
       " (37695, 1),\n",
       " (19978, 0),\n",
       " (10419, 1),\n",
       " (28147, 0),\n",
       " (44538, 1),\n",
       " (46045, 0),\n",
       " (58209, 1),\n",
       " (1283, 0),\n",
       " (9807, 1),\n",
       " (46528, 0),\n",
       " (59778, 1),\n",
       " (8632, 0),\n",
       " (28305, 1),\n",
       " (53585, 0),\n",
       " (25650, 1),\n",
       " (29254, 0),\n",
       " (48282, 1),\n",
       " (40298, 0),\n",
       " (15819, 1),\n",
       " (42872, 0),\n",
       " (8922, 1),\n",
       " (58661, 0),\n",
       " (49185, 1),\n",
       " (49931, 0),\n",
       " (54606, 1),\n",
       " (49727, 0),\n",
       " (7524, 1),\n",
       " (20779, 0),\n",
       " (51423, 1),\n",
       " (25913, 0),\n",
       " (57732, 1),\n",
       " (49441, 0),\n",
       " (6627, 1),\n",
       " (33527, 0),\n",
       " (55272, 1),\n",
       " (57758, 0),\n",
       " (16932, 1),\n",
       " (53638, 0),\n",
       " (56136, 1),\n",
       " (11402, 0),\n",
       " (13959, 1),\n",
       " (54521, 0),\n",
       " (33987, 1),\n",
       " (649, 0),\n",
       " (4491, 1),\n",
       " (25895, 0),\n",
       " (8556, 1),\n",
       " (31964, 0),\n",
       " (9846, 1),\n",
       " (51352, 0),\n",
       " (7815, 1),\n",
       " (43648, 0),\n",
       " (21285, 1),\n",
       " (24968, 0),\n",
       " (36384, 1),\n",
       " (53972, 0),\n",
       " (24798, 1),\n",
       " (58081, 0),\n",
       " (10887, 1),\n",
       " (22241, 0),\n",
       " (44769, 1),\n",
       " (50753, 0),\n",
       " (58365, 1),\n",
       " (13123, 0),\n",
       " (56367, 1),\n",
       " (33268, 0),\n",
       " (28230, 1),\n",
       " (34484, 0),\n",
       " (12798, 1),\n",
       " (17239, 0),\n",
       " (1977, 1),\n",
       " (17822, 0),\n",
       " (38772, 1),\n",
       " (25858, 0),\n",
       " (40872, 1),\n",
       " (13777, 0),\n",
       " (357, 1),\n",
       " (43423, 0),\n",
       " (53175, 1),\n",
       " (6170, 0),\n",
       " (27333, 1),\n",
       " (32425, 0),\n",
       " (32850, 1),\n",
       " (49169, 0),\n",
       " (49566, 1),\n",
       " (25364, 0),\n",
       " (34089, 1),\n",
       " (45958, 0),\n",
       " (6780, 1),\n",
       " (45106, 0),\n",
       " (8064, 1),\n",
       " (24620, 0),\n",
       " (59352, 1),\n",
       " (22633, 0),\n",
       " (11736, 1),\n",
       " (8270, 0),\n",
       " (53028, 1),\n",
       " (39368, 0),\n",
       " (31845, 1),\n",
       " (3479, 0),\n",
       " (39570, 1),\n",
       " (29309, 0),\n",
       " (42774, 1),\n",
       " (38240, 0),\n",
       " (55905, 1),\n",
       " (14, 0),\n",
       " (24450, 1),\n",
       " (9178, 0),\n",
       " (7419, 1),\n",
       " (9166, 0),\n",
       " (22749, 1),\n",
       " (4681, 0),\n",
       " (39123, 1),\n",
       " (5848, 0),\n",
       " (47754, 1),\n",
       " (30163, 0),\n",
       " (45411, 1),\n",
       " (25121, 0),\n",
       " (49770, 1),\n",
       " (10210, 0),\n",
       " (22131, 1),\n",
       " (20753, 0),\n",
       " (4527, 1),\n",
       " (37496, 0),\n",
       " (9309, 1),\n",
       " (6217, 0),\n",
       " (25737, 1),\n",
       " (39529, 0),\n",
       " (27339, 1),\n",
       " (33439, 0),\n",
       " (17265, 1),\n",
       " (45097, 0),\n",
       " (45186, 1),\n",
       " (30403, 0),\n",
       " (6198, 1),\n",
       " (52450, 0),\n",
       " (1992, 1),\n",
       " (4442, 0),\n",
       " (27147, 1),\n",
       " (18478, 0),\n",
       " (53766, 1),\n",
       " (34405, 0),\n",
       " (27258, 1),\n",
       " (25175, 0),\n",
       " (27414, 1),\n",
       " (11689, 0),\n",
       " (4755, 1),\n",
       " (11290, 0),\n",
       " (30741, 1),\n",
       " (56900, 0),\n",
       " (11829, 1),\n",
       " (11573, 0),\n",
       " (4473, 1),\n",
       " (40535, 0),\n",
       " (18129, 1),\n",
       " (41503, 0),\n",
       " (45849, 1),\n",
       " (43412, 0),\n",
       " (38613, 1),\n",
       " (19777, 0),\n",
       " (33483, 1),\n",
       " (2839, 0),\n",
       " (18438, 1),\n",
       " (58640, 0),\n",
       " (41121, 1),\n",
       " (5119, 0),\n",
       " (49899, 1),\n",
       " (19889, 0),\n",
       " (51228, 1),\n",
       " (6601, 0),\n",
       " (49656, 1),\n",
       " (7247, 0),\n",
       " (53625, 1),\n",
       " (21256, 0),\n",
       " (30885, 1),\n",
       " (10519, 0),\n",
       " (46554, 1),\n",
       " (25331, 0),\n",
       " (54882, 1),\n",
       " (16732, 0),\n",
       " (47730, 1),\n",
       " (12881, 0),\n",
       " (38424, 1),\n",
       " (58144, 0),\n",
       " (34587, 1),\n",
       " (22043, 0),\n",
       " (11589, 1),\n",
       " (52903, 0),\n",
       " (50751, 1),\n",
       " (21404, 0),\n",
       " (16251, 1),\n",
       " (35302, 0),\n",
       " (3231, 1),\n",
       " (29500, 0),\n",
       " (51492, 1),\n",
       " (19088, 0),\n",
       " (42069, 1),\n",
       " (38453, 0),\n",
       " (16299, 1),\n",
       " (47239, 0),\n",
       " (27213, 1),\n",
       " (38866, 0),\n",
       " (18021, 1),\n",
       " (40307, 0),\n",
       " (42294, 1),\n",
       " (38858, 0),\n",
       " (6861, 1),\n",
       " (7612, 0),\n",
       " (10449, 1),\n",
       " (27851, 0),\n",
       " (4251, 1),\n",
       " (13243, 0),\n",
       " (43464, 1),\n",
       " (15829, 0),\n",
       " (39858, 1),\n",
       " (1901, 0),\n",
       " (51891, 1),\n",
       " (32408, 0),\n",
       " (28533, 1),\n",
       " (1987, 0),\n",
       " (927, 1),\n",
       " (56773, 0),\n",
       " (40881, 1),\n",
       " (47527, 0),\n",
       " (38568, 1),\n",
       " (52321, 0),\n",
       " (25056, 1),\n",
       " (47516, 0),\n",
       " (27948, 1),\n",
       " (1747, 0),\n",
       " (32250, 1),\n",
       " (10774, 0),\n",
       " (44463, 1),\n",
       " (13645, 0),\n",
       " (59940, 1),\n",
       " (5266, 0),\n",
       " (48888, 1),\n",
       " (4571, 0),\n",
       " (14295, 1),\n",
       " (15617, 0),\n",
       " (32709, 1),\n",
       " (6803, 0),\n",
       " (49653, 1),\n",
       " (25606, 0),\n",
       " (34434, 1),\n",
       " (20033, 0),\n",
       " (44094, 1),\n",
       " (10361, 0),\n",
       " (48759, 1),\n",
       " (16624, 0),\n",
       " (50562, 1),\n",
       " (32872, 0),\n",
       " (14142, 1),\n",
       " (30158, 0),\n",
       " (49374, 1),\n",
       " (6448, 0),\n",
       " (39144, 1),\n",
       " (51293, 0),\n",
       " (11523, 1),\n",
       " (24278, 0),\n",
       " (32604, 1),\n",
       " (24151, 0),\n",
       " (21249, 1),\n",
       " (24193, 0),\n",
       " (2592, 1),\n",
       " (6749, 0),\n",
       " (59010, 1),\n",
       " (31621, 0),\n",
       " (49587, 1),\n",
       " (44635, 0),\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array([number_to_input_example(d[0]) for d in data])\n",
    "y = [d[1] for d in data]\n",
    "n = len(X)\n",
    "\n",
    "train_len = int(TRAIN_TEST_SPLIT * n)\n",
    "train_inputs = X[:train_len]\n",
    "train_labels = y[:train_len]\n",
    "validation_inputs = X[train_len:]\n",
    "validation_labels = y[train_len:]\n",
    "\n",
    "\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 1, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 1]],\n",
       "\n",
       "       [[1, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 1, 0]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 1, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 1, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 1],\n",
       "        [1, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 1, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [1, 0, 0, ..., 0, 0, 0]]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "1400/1400 [==============================] - 11s 8ms/step - loss: 8.2732 - acc: 0.0057 - val_loss: 7.9712 - val_acc: 0.0000e+00\n",
      "Epoch 2/10\n",
      "  18/1400 [..............................] - ETA: 8s - loss: 6.2193 - acc: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "model.fit(train_inputs, train_labels, validation_data=(validation_inputs, validation_labels), \n",
    "          batch_size=1, epochs=NUM_EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 5, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "600"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(validation_inputs, validation_labels, batch_size=1,verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
