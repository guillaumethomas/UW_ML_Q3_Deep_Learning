{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Lesson 5b: Recurrent Neural Networks: Categorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Recurrent Neural Networks (RNNs) can be used in many different ways, such as classification, single-step prediction, and the generation of an entire sequence. \n",
    "\n",
    "* **Classification**: the input is a sequence, and the output is a single category - this is the focus of this assignment. (Alternatively, a sequence of categories could be generated, one for each partial sequence as it is processed).\n",
    "\n",
    "* **Prediction**: the input is a sequence, and the output is a prediction for the next element in the sequence. You will explore this in lesson 5b.\n",
    "\n",
    "* **Sequence Generation** (Seq-to-Seq): both the input and the output are entire sequences. For example, RNN-based language translation may take in an input sequence (of characters or word tokens) in English, and generate as output a sequence (of characters or word tokens) in French.\n",
    "\n",
    "RNNs can be used to process inputs that occur naturally in time (such as an audio recording of speech or music represented as a stream of timestamped MIDI messages), but they can also be applied to material that has an order to it, even if it's not necessarly temporal in natures, such as written text (which can be read one character or one word at a time) or even written numbers or math equations (which can be read one digit or symbol at a time, from left to right, for instance.)  This is the problem we investigate today: looking at numbers such as \"1423\" as a sequence of digits ['1', '4', '2', '3'].\n",
    "\n",
    "Our problem comes curtosy of Distinguished Professor Douglas R. Hofstadter of Indiana University, author of books such as _Gödel, Escher, Bach: an Eternal Golden Braid_. Hofstadter writes [private communication, shared with permission)]:\n",
    "\n",
    "---\n",
    "\n",
    "_Lately, I have been musing about the seeming power of deep neural nets.  They learn to recognize members of all sorts of categories, when those members (and non-members) are fed to them as patterns of symbols or of pixels.  So, how about the following challenges involving the natural numbers?_\n",
    "\n",
    "* To recognize the even numbers, expressed in base 3.\n",
    "     (Specifically, 0, 2, 11, 20, 22, 101, 110, 112,...)\n",
    "* To recognize the multiples of 3, expressed in base 10.\n",
    "* To recognize the multiples of 9, expressed in base 10.\n",
    "* To recognize the multiples of 7, expressed in base 10.\n",
    "* To recognize the multiples of 29, expressed in base 10.\n",
    "\n",
    "_(I suppose that if a net can learn any particular one of the above list, it can learn all of them.  Just a guess...)_\n",
    "\n",
    " _Moving right along, how about the following somewhat harder challenges?_\n",
    "\n",
    "* To recognize the correct integer additions, expressed either in base 2 or in base 10.  (For example, the string “12+29=41”.)\n",
    "* To recognize the correct integer multiplications, expressed either in base 2 or in base 10.  (For example, the string “12x29=348”.)\n",
    "\n",
    "_(The latter of this pair seems significantly harder than the former.)_\n",
    "     \n",
    "_And then, of course, the canonical challenge of this sort:_\n",
    "\n",
    "* To recognize the prime numbers, expressed either in base 2 or in base 10.\n",
    "\n",
    "_Each of the above challenges involves a number-theoretical category that can easily be described in purely syntactic terms (i.e., as a rule-based pattern of symbols).  It would be trivial to generate millions of examples of such categories mechanically, and then you just feed them to the neural net.  You can also feed the network lots of counterexamples -- marking them, of course, as non-members of the category.  Can a deep neural network learn any of these categories?  All of them?  Some of them?_\n",
    "\n",
    "---\n",
    "\n",
    "In this assignment, you will use an RNN to try to solve the divisibility-by-3 problem (the rest are challenges you might want to try in your free time!): \n",
    "\n",
    "* **\"To recognize the multiples of 3, expressed in base 10.\"**  Specifically, you must:\n",
    "    * Design an RNN that takes a sequence of digits as input. \n",
    "    * Represent digits in base 10 by using a categorical, one-hot encoding, with one node for each digit from 0 through 9.\n",
    "    * Train the RNN to categorize a number as True if it is evenly divisible by 3, False otherwise.\n",
    "    * Test the RNN on a set of previously-unseen numbers, including numbers that are 4 digits long, such as 2225 and 3333.\n",
    "    * Acheive an accuracy of at least 95% on the test set (report the accuracy in the cell marked below).\n",
    "    * Answer the questions at the end of this notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [One hot encoding](https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/) \n",
    "- [Sequence Prediction](https://machinelearningmastery.com/sequence-prediction-problems-learning-lstm-recurrent-neural-networks/)\n",
    "- [LSTM Tutorial Keras](https://adventuresinmachinelearning.com/keras-lstm-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Setup\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Keras - Guide to the sequential model](https://keras.io/getting-started/sequential-model-guide/)\n",
    "- [Input](https://keras.io/layers/core/)\n",
    "- [GRU](https://keras.io/layers/recurrent/)\n",
    "- [LSTM](https://keras.io/layers/recurrent/)\n",
    "- [Dense](https://keras.io/layers/core/#Dense)\n",
    "- [Masking](https://keras.io/layers/core/)\n",
    "- [Dropout](https://keras.io/layers/core/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Keras - fit](https://keras.io/models/sequential/)\n",
    "- [Recurrent Neural Networks by Example in Python](https://towardsdatascience.com/recurrent-neural-networks-by-example-in-python-ffd204f99470)\n",
    "- [Sequence Classification with LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Report your final accuracy on the validation dataset below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Examine model outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Further Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "1) What happens if you give a 5-digit number or a 6-digit number to the trained model, after training on 1-, 2-, 3-, and 4- digit numbers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "2) Pick another number from Hofstadter's list above, such as 9, 7, or 29. Train a model, and report the accuracy of your results. Did it work or not? Why or why not (your best guess)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "3) Record any other comments/insights from your model training process. What worked well? What caused trouble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "4) If you didn't have a training algorithm, how would you design a RNN-style system to recognize divisibility by 3?\n",
    "Ignoring the details of the weights, what kind of state must be carried over from step to step as each digit is read in\n",
    "successively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "5) BONUS (hard): Explain how the neural net you trained above works, with evidence from examining the node activations as the net runs. Does it do anything similar to what you would have designed as a human?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Jose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras.layers import Input, GRU, LSTM, Dense, Masking, Dropout, InputLayer, TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import to_categorical\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up params for dataset.\n",
    "DIVISIBILITY_NUMBER = 3         # We want to test for divisilibity by 3.\n",
    "TRAIN_TEST_SPLIT = 0.8          # Percentage of data in training set\n",
    "NUM_EXAMPLES_PER_CLASS = 1000   # Generate the first 1000 multiples of 3 for training/testing\n",
    "                                # Also generate 1000 non-multiples of 3.\n",
    "NUM_CATEGORIES = 10             # 10 digits\n",
    "MAX_DIGITS = 5                  # Number of digits allowed in input strings\n",
    "\n",
    "# Neural net hyperparameters-- just an example. Adjust these as needed.\n",
    "BATCH_SIZE = 32\n",
    "NUM_LSTM_NODES = 10             \n",
    "DROPOUT = 0.5\n",
    "LEARING_RATE = 0.002\n",
    "NUM_EPOCHS = 60\n",
    "\n",
    "# TODO: add/modify constants as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "def generate_example_numbers(base_number=DIVISIBILITY_NUMBER, num_examples_per_class=NUM_EXAMPLES_PER_CLASS):\n",
    "    \"\"\"Return a tuple of two lists: (list_of_multiples, list_of_nonmultiples).\n",
    "    \n",
    "    For example, ([0, 3, 6, 9, 12, ...2997], [1, 4, 5, 8, 11, 13, 14,...,2999]).\n",
    "    Each list contains num_examples_per_class elements.\n",
    "    \"\"\"\n",
    "    #a = sample(range(0,num_examples_per_class * 60), num_examples_per_class * base_number * 2)\n",
    "    # based on the density of the multiples within a set of integers\n",
    "    #multiples = []\n",
    "    #non_multiples = []\n",
    "    #for i in a:\n",
    "    #    if i % base_number == 0:\n",
    "    #        multiples.append(i)\n",
    "    #    else:\n",
    "    #        non_multiples.append(i)\n",
    "    \n",
    "    #multiples = sample(multiples, num_examples_per_class)\n",
    "    #non_multiples = sample(non_multiples, num_examples_per_class)\n",
    "    \n",
    "    #return (multiples, non_multiples)\n",
    "    multiples = [n * base_number for n in range(num_examples_per_class)]\n",
    "    non_multiples = [n * (base_number+1) for n in range(num_examples_per_class)]\n",
    "    return (multiples, non_multiples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digit_to_vector(digit):\n",
    "    \"\"\"Given a digit from 0-9, return a numpy array representing the digit using a 1-hot encoding.\n",
    "    keras.utils.to_categorical may be useful.\n",
    "    \"\"\"\n",
    "    tmp = to_categorical(digit, dtype='int',num_classes=NUM_CATEGORIES)\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_input_example(number, max_digits=MAX_DIGITS):\n",
    "    \"\"\"Given an integer number, return a numpy float array of 0.0s and 1.0s, of the correct shape to feed into the \n",
    "    neural net.\n",
    "    \n",
    "    For example, if you have a max of 5 digits then you should have a 2D numpy matrix: 5 rows (one for each\n",
    "    sequence index), and 10 columns (1 for each digit).\n",
    "    \n",
    "    In order to train in \"batch\" mode, the RNN expects every example to have the same shape. So if you have a 2-digit\n",
    "    number such as \"42\", you need to pad the example with a \"padding\" token somehow; for example, \"???42\", and then\n",
    "    use keras.layers.Masking to ignore the leading digits. Or just pad with 0s, as in \"00042\". \n",
    "    keras.preprocessing.sequence.pad_sequences can help with this.\n",
    "    \"\"\"\n",
    "    #a = pad_sequences([[int(i) for i in str(number)]], maxlen=max_digits, padding='pre')[0]\n",
    "    #a = [digit_to_vector(i) for i in a]\n",
    "    \n",
    "    #return np.array(a)\n",
    "    padded_number = pad_sequences([[int(k) for k in str(number)]], maxlen=max_digits)\n",
    "    result = [np.zeros(10) for _ in range(max_digits)]\n",
    "    for i, k in zip(range(max_digits), padded_number[0]):\n",
    "      result[i][k] = 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_to_input_example(42, max_digits=MAX_DIGITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(divisibility_number=DIVISIBILITY_NUMBER, \n",
    "                     train_test_split=TRAIN_TEST_SPLIT, \n",
    "                     num_examples_per_class=NUM_EXAMPLES_PER_CLASS):\n",
    "    \"\"\"Generate a dataset ready for training. Returns a list of tuples. Each tuple is of the form\n",
    "    (input_array, label). The dataset should be shuffled either here or during the training process to\n",
    "    mix divisile-by-DIVISIBILITY_NUMBER and not-divisible-by-DIVISIBILITY_NUMBER examples.\n",
    "    The dataset should consist of NUM_EXAMPLES_PER_CLASS positive examples (e.g., 1000 examples of divisible-by-3), and\n",
    "    also NUM_EXAMPLES_PER_CLASS negative examples (e.g., 1000 examples of not-divisible-by-3).\n",
    "    \"\"\"\n",
    "    example_numbers = generate_example_numbers(divisibility_number, num_examples_per_class)\n",
    "    divisible = example_numbers[0]\n",
    "    non_divisible = example_numbers[1]\n",
    "    np.random.shuffle(divisible)\n",
    "    np.random.shuffle(non_divisible)\n",
    "    \n",
    "    result = []\n",
    "    for k, m in zip(divisible, non_divisible):\n",
    "        result.append((k, 1))\n",
    "        result.append((m, 0))\n",
    "        \n",
    "    \n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (1, 5, 10)                840       \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (1, 5, 10)                110       \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (1, 10)                   840       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (1, 1)                    11        \n",
      "=================================================================\n",
      "Total params: 1,801\n",
      "Trainable params: 1,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Build RNN model.\n",
    "def build_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(NUM_LSTM_NODES, dropout=DROPOUT, recurrent_dropout=DROPOUT, batch_input_shape=(1,MAX_DIGITS,10), return_sequences=True))  # Use return_sequences=True for multiple hidden layers\n",
    "    model.add(Dense(NUM_LSTM_NODES))\n",
    "    model.add(LSTM(NUM_LSTM_NODES, dropout=DROPOUT, recurrent_dropout=DROPOUT, batch_input_shape=(1,MAX_DIGITS,10), return_sequences=False))  # Use return_sequences=True for multiple hidden layers\n",
    "    model.add(Dense(1))  # Model should return 1 or 0 for divisible/not-divisible\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Print the model configuration.\n",
    "model = build_model()\n",
    "\n",
    "# Compile model\n",
    "adam = Adam(lr=LEARING_RATE)   # Modify learning algorithm as needed\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "\n",
    "# Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(486, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "162.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0] / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = np.array([number_to_input_example(d[0]) for d in data])\n",
    "y = [d[1] for d in data]\n",
    "n = len(X)\n",
    "\n",
    "train_len = int(TRAIN_TEST_SPLIT * n)\n",
    "train_inputs = X[:train_len]\n",
    "train_labels = y[:train_len]\n",
    "validation_inputs = X[train_len:]\n",
    "validation_labels = y[train_len:]\n",
    "\n",
    "\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1600 samples, validate on 400 samples\n",
      "Epoch 1/60\n",
      "1600/1600 [==============================] - 12s 7ms/step - loss: 0.7182 - acc: 0.5350 - val_loss: 0.6537 - val_acc: 0.6850\n",
      "Epoch 2/60\n",
      "1600/1600 [==============================] - 12s 7ms/step - loss: 0.6633 - acc: 0.5925 - val_loss: 0.5657 - val_acc: 0.7725\n",
      "Epoch 3/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.5780 - acc: 0.6700 - val_loss: 0.4335 - val_acc: 0.7800\n",
      "Epoch 4/60\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 0.5521 - acc: 0.6637 - val_loss: 0.4319 - val_acc: 0.7875\n",
      "Epoch 5/60\n",
      "1600/1600 [==============================] - 10s 7ms/step - loss: 0.5344 - acc: 0.6550 - val_loss: 0.4056 - val_acc: 0.6175\n",
      "Epoch 6/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4949 - acc: 0.6575 - val_loss: 0.3973 - val_acc: 0.5775\n",
      "Epoch 7/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.5204 - acc: 0.6244 - val_loss: 0.4228 - val_acc: 0.6425\n",
      "Epoch 8/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.5150 - acc: 0.6556 - val_loss: 0.3978 - val_acc: 0.5300\n",
      "Epoch 9/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.5168 - acc: 0.6488 - val_loss: 0.3953 - val_acc: 0.6350\n",
      "Epoch 10/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4841 - acc: 0.6819 - val_loss: 0.4089 - val_acc: 0.6750\n",
      "Epoch 11/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4877 - acc: 0.6519 - val_loss: 0.3629 - val_acc: 0.6100\n",
      "Epoch 12/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4612 - acc: 0.6631 - val_loss: 0.3634 - val_acc: 0.6275\n",
      "Epoch 13/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4795 - acc: 0.6562 - val_loss: 0.3475 - val_acc: 0.5875\n",
      "Epoch 14/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4735 - acc: 0.6256 - val_loss: 0.3540 - val_acc: 0.6150\n",
      "Epoch 15/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4657 - acc: 0.6488 - val_loss: 0.4127 - val_acc: 0.6275\n",
      "Epoch 16/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4413 - acc: 0.6512 - val_loss: 0.3344 - val_acc: 0.5925\n",
      "Epoch 17/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4247 - acc: 0.6419 - val_loss: 0.3135 - val_acc: 0.5975\n",
      "Epoch 18/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4272 - acc: 0.6206 - val_loss: 0.3310 - val_acc: 0.5900\n",
      "Epoch 19/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4424 - acc: 0.6431 - val_loss: 0.3261 - val_acc: 0.5850\n",
      "Epoch 20/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4321 - acc: 0.6494 - val_loss: 0.3303 - val_acc: 0.6000\n",
      "Epoch 21/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4182 - acc: 0.6138 - val_loss: 0.3096 - val_acc: 0.6100\n",
      "Epoch 22/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.5007 - acc: 0.6444 - val_loss: 0.3949 - val_acc: 0.5925\n",
      "Epoch 23/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4456 - acc: 0.6312 - val_loss: 0.3213 - val_acc: 0.6075\n",
      "Epoch 24/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4512 - acc: 0.6175 - val_loss: 0.3189 - val_acc: 0.6025\n",
      "Epoch 25/60\n",
      "1600/1600 [==============================] - 12s 7ms/step - loss: 0.4514 - acc: 0.6138 - val_loss: 0.3053 - val_acc: 0.6050\n",
      "Epoch 26/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4536 - acc: 0.6075 - val_loss: 0.3210 - val_acc: 0.6000\n",
      "Epoch 27/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4346 - acc: 0.6094 - val_loss: 0.3068 - val_acc: 0.6125\n",
      "Epoch 28/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4372 - acc: 0.5894 - val_loss: 0.3077 - val_acc: 0.6100\n",
      "Epoch 29/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4391 - acc: 0.6306 - val_loss: 0.3041 - val_acc: 0.6100\n",
      "Epoch 30/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4098 - acc: 0.6212 - val_loss: 0.2982 - val_acc: 0.6050\n",
      "Epoch 31/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4360 - acc: 0.6456 - val_loss: 0.2986 - val_acc: 0.6150\n",
      "Epoch 32/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4018 - acc: 0.6025 - val_loss: 0.3001 - val_acc: 0.6100\n",
      "Epoch 33/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4387 - acc: 0.6050 - val_loss: 0.3018 - val_acc: 0.5875\n",
      "Epoch 34/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4793 - acc: 0.6088 - val_loss: 0.2979 - val_acc: 0.6125\n",
      "Epoch 35/60\n",
      "1600/1600 [==============================] - 14s 9ms/step - loss: 0.4405 - acc: 0.6056 - val_loss: 0.3133 - val_acc: 0.6150\n",
      "Epoch 36/60\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 0.4094 - acc: 0.6169 - val_loss: 0.2905 - val_acc: 0.6100\n",
      "Epoch 37/60\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 0.4245 - acc: 0.6250 - val_loss: 0.2871 - val_acc: 0.6125\n",
      "Epoch 38/60\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 0.3982 - acc: 0.6144 - val_loss: 0.2822 - val_acc: 0.6150\n",
      "Epoch 39/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4156 - acc: 0.6494 - val_loss: 0.2970 - val_acc: 0.6300\n",
      "Epoch 40/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4106 - acc: 0.6744 - val_loss: 0.2791 - val_acc: 0.6225\n",
      "Epoch 41/60\n",
      "1600/1600 [==============================] - 13s 8ms/step - loss: 0.4106 - acc: 0.6387 - val_loss: 0.2860 - val_acc: 0.6300\n",
      "Epoch 42/60\n",
      "1600/1600 [==============================] - 10s 7ms/step - loss: 0.4063 - acc: 0.6713 - val_loss: 0.2820 - val_acc: 0.6250\n",
      "Epoch 43/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4152 - acc: 0.6437 - val_loss: 0.2809 - val_acc: 0.6200\n",
      "Epoch 44/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.3883 - acc: 0.6350 - val_loss: 0.2928 - val_acc: 0.6225\n",
      "Epoch 45/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4029 - acc: 0.6425 - val_loss: 0.2735 - val_acc: 0.6225\n",
      "Epoch 46/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4411 - acc: 0.6088 - val_loss: 0.2827 - val_acc: 0.6200\n",
      "Epoch 47/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4043 - acc: 0.6194 - val_loss: 0.2820 - val_acc: 0.6225\n",
      "Epoch 48/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.3906 - acc: 0.6075 - val_loss: 0.2858 - val_acc: 0.6225\n",
      "Epoch 49/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.3974 - acc: 0.6219 - val_loss: 0.2869 - val_acc: 0.6200\n",
      "Epoch 50/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4025 - acc: 0.6394 - val_loss: 0.2792 - val_acc: 0.6350\n",
      "Epoch 51/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.3783 - acc: 0.6719 - val_loss: 0.2702 - val_acc: 0.6375\n",
      "Epoch 52/60\n",
      "1600/1600 [==============================] - 12s 8ms/step - loss: 0.3867 - acc: 0.6831 - val_loss: 0.2728 - val_acc: 0.6800\n",
      "Epoch 53/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4159 - acc: 0.6663 - val_loss: 0.2658 - val_acc: 0.6375\n",
      "Epoch 54/60\n",
      "1600/1600 [==============================] - 12s 8ms/step - loss: 0.3895 - acc: 0.6606 - val_loss: 0.2715 - val_acc: 0.6275\n",
      "Epoch 55/60\n",
      "1600/1600 [==============================] - 12s 7ms/step - loss: 0.3852 - acc: 0.6462 - val_loss: 0.2662 - val_acc: 0.6250\n",
      "Epoch 56/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4054 - acc: 0.6256 - val_loss: 0.2755 - val_acc: 0.6250\n",
      "Epoch 57/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.4016 - acc: 0.6319 - val_loss: 0.2690 - val_acc: 0.6250\n",
      "Epoch 58/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.4191 - acc: 0.6337 - val_loss: 0.2639 - val_acc: 0.6250\n",
      "Epoch 59/60\n",
      "1600/1600 [==============================] - 11s 7ms/step - loss: 0.3930 - acc: 0.6456 - val_loss: 0.2922 - val_acc: 0.6250\n",
      "Epoch 60/60\n",
      "1600/1600 [==============================] - 10s 6ms/step - loss: 0.3885 - acc: 0.6481 - val_loss: 0.2716 - val_acc: 0.6250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a30ad8ef0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_inputs, train_labels, validation_data=(validation_inputs, validation_labels), \n",
    "          batch_size=1, epochs=NUM_EPOCHS, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 5, 10)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 62.50%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate(validation_inputs, validation_labels, batch_size=1,verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
